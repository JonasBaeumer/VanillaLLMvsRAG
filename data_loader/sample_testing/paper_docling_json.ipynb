{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7abfcd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c76c1c",
   "metadata": {},
   "source": [
    "## Load files and dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ffded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_if_exists(filepath: Path) -> Optional[dict]:\n",
    "    if filepath.exists():\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f944a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_v1_data(v1_path: Path) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Loads parsed paper and reviews from a given v1 folder.\n",
    "    Looks for paper.docling.json first, falls back to .itg or .tei if needed.\n",
    "    \"\"\"\n",
    "    parsed_filenames = [\"paper.docling.json\", \"paper.itg.json\", \"paper.tei.json\"]\n",
    "    paper_content = None\n",
    "\n",
    "    for filename in parsed_filenames:\n",
    "        paper_path = v1_path / filename\n",
    "        if paper_path.exists():\n",
    "            paper_content = load_json_if_exists(paper_path)\n",
    "            if paper_content:\n",
    "                break\n",
    "\n",
    "    if not paper_content:\n",
    "        print(f\"[!] No parsed paper found in {v1_path}\")\n",
    "        return None\n",
    "\n",
    "    reviews = load_json_if_exists(v1_path / \"reviews.json\") or []\n",
    "\n",
    "    return {\n",
    "        \"paper_path\": v1_path,\n",
    "        \"paper_content\": paper_content,\n",
    "        \"reviews\": reviews\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a429eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arr_emnlp_dataset(root_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Loads v1 content and reviews for all papers under the ARR-EMNLP structure.\n",
    "\n",
    "    Args:\n",
    "        root_dir: Path to the ARR-EMNLP root (e.g., \"./data/ARR-EMNLP\")\n",
    "\n",
    "    Returns:\n",
    "        List of dicts, each with paper_id, parsed content, and reviews.\n",
    "    \"\"\"\n",
    "    root = Path(root_dir)\n",
    "    dataset = []\n",
    "\n",
    "    for paper_dir in root.iterdir():\n",
    "        if not paper_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        v1_dir = paper_dir / \"v1\"\n",
    "        if not v1_dir.exists():\n",
    "            print(f\"[!] Skipping {paper_dir.name}, no v1 folder found.\")\n",
    "            continue\n",
    "\n",
    "        paper_entry = extract_v1_data(v1_dir)\n",
    "        if paper_entry:\n",
    "            paper_entry[\"paper_id\"] = paper_dir.name\n",
    "            dataset.append(paper_entry)\n",
    "\n",
    "    print(f\"âœ… Loaded {len(dataset)} paper(s) with v1 submissions.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dea19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../../data/ARR-EMNLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a1dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1 paper(s) with v1 submissions.\n",
      "ðŸ“‚ Paper folder: 1\n",
      "ðŸ“ v1 files: ['paper.pdf', 'reviews.json', 'paper.itg', 'paper.docling.json', 'paper.tei', 'meta.json']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_arr_emnlp_dataset(dataset_path)\n",
    "\n",
    "# Find all paper folders (e.g., 123ABC456)\n",
    "paper_dirs = [p for p in dataset_path.iterdir() if p.is_dir()]\n",
    "first_paper_path = paper_dirs[0]\n",
    "v1_path = first_paper_path / \"v1\"\n",
    "\n",
    "# List the files inside the v1 directory\n",
    "v1_files = {file.name: file for file in v1_path.iterdir() if file.is_file()}\n",
    "v1_files_list = list(v1_files.keys())\n",
    "\n",
    "print(\"ðŸ“‚ Paper folder:\", first_paper_path.name)\n",
    "print(\"ðŸ“ v1 files:\", v1_files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5b255",
   "metadata": {},
   "source": [
    "## Meta.json overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79f2c8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‚ï¸ Version Meta:\n",
      "  title: Impact of Co-occurrence on Factual Knowledge of Large Language Models\n",
      "  authors: ['Cheongwoong Kang', 'Jaesik Choi']\n",
      "  abstract: Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at [https://github.com/CheongWoong/impact\\_of\\_cooccurrence](https://github.com/CheongWoong/impact\\_of\\_cooccurrence).\n",
      "  license: Copyright Â© 2021 administered by the Association for Computational Linguistics (ACL) on behalf of the authors and content contributors. Content displayed on this webpage is made available under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
      "  year: 2023\n",
      "  keywords: ['Factual Knowledge', 'Large Language Models', 'Co-occurrence', 'Term Frequency', 'Data Statistics']\n",
      "  doc_id: 2\n",
      "  decision: Accept-Findings\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load version-level metadata\n",
    "meta_path = v1_files.get(\"meta.json\")\n",
    "if meta_path and meta_path.exists():\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        meta_data = json.load(f)\n",
    "    print(\"ðŸ—‚ï¸ Version Meta:\")\n",
    "    for k, v in meta_data.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"meta.json not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e8f5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Reviews:\n",
      "\n",
      "ðŸ”¸ Review #1:\n",
      "Reviewer ID: rfX6ne8ne4\n",
      "\n",
      "ðŸ“Œ Paper topic and main contributions:\n",
      "The paper investigates the effect of co-occurrence statistics on the ability of large language models to correctly answer simple factual questions (of the subject-relation-object form). The paper specifically checks whether simple co-occurrences between the subject and the object in the pretraining data, can lead the models to incorrectly answer factual questions where the co-occurrence diverges from the correct answer. As it is difficult to check this causal relation by direct manipulation (given the enormous costs of pretraining large language models), a correlation study was conducted. Results show correlation between the co-occurrence statistics of a triplet and the ability of the model to answer correctly questions on it. \n",
      "The paper further explores mitigation strategies to combat this bias. Two approaches are explored: debiased finetuning and knowledge editing. The former approach presents limited gains. Knowledge editing does show promise, but may be restricted as it requires weeding out the incorrect facts one by one (if I understand correctly). It may also cause unintended changes to unrelated facts.\n",
      "\n",
      "ðŸ“Œ Reasons to accept:\n",
      "-- The paper addresses an important and timely topic, namely the ability of LLMs to act as knowledge bases.\n",
      "-- The related work section is very elaborate and provides insight into the field at hand.\n",
      "-- The arguments of the paper are well-presented, and the writing is generally clear.\n",
      "\n",
      "ðŸ“Œ Reasons to reject:\n",
      "-- I am somewhat confused as to the exact claim the paper is making. While the results clearly show a correlation between the co-occurrence statistics of a triplet and the performance of the model on it, it is not clear to me whether this in fact proves that there is a bias where such simple surface statistics push the language model astray from making the right prediction. What it does show is that questions where the degree of co-occurrence is smaller are more difficult for the model. The introduction reads â€œin which frequently co-occurred words are preferred over the correct answer.â€ I could not see how the experiments directly make this point. Simple correlation seems to me insufficient in this case, since making mistakes with little co-occurrence doesnâ€™t mean that there is a different option with higher co-occurrence.\n",
      "In order to show that the behavior is biased, I would expect the paper to shows that the surface statistics interfere in some sense with the prediction of the model, in a way that would make it predict such answer even when it is not true. For example, I would have expected the paper to examine questions which we would expect (based on their prevalence in the training data) the model to answer correctly, and show that in these cases it tends to make more errors where there is a strong collocation and that the mistakes is towards the collocating words. If the paper indeed makes this kind of more subtle claim and I have missed it, I would welcome a response from the authors on this matter. Thank you. \n",
      "Following rebuttal: the results you have posted are helpful and address this comment. Please include them in the next version.\n",
      "-- The results of the attempts to mitigate the bias are not very strong. I should say that I do not see it in itself as grounds for rejection.\n",
      "-- Some important presentational details are not sufficiently clear (see below).\n",
      "\n",
      "ðŸ“Œ Questions for the authors:\n",
      "-- I would appreciate your response to my first point above.\n",
      "-- Was there any attempt to look at stronger, proprietary models? I'm asking because they are much stronger and may exhibit diff behavior.\n",
      "-- The point about memorization is unclear to me. Why does co-occurrence imply memorization? on the contrary, if the model \"saw\" the correct answer and was overridden by co-occurrence this is the opposite of memorization.\n",
      "-- Why is knowledge editing a mitigation strategy. Obviously wrong knowledge should be corrected, but this is independent of the cause of the mistake, which is the subject of this paper, isn't it?\n",
      "\n",
      "ðŸ“Œ Missing references:\n",
      "--\n",
      "\n",
      "ðŸ“Œ Typos grammar style and presentation improvements:\n",
      "Local comments: -- What was claimed exactly in previous work with respect to the discussed bias and how does that differ from the work here?\n",
      "-- l. 136: you write \"Our work is the 136 first to investigate the effects of finetuning on the 137 correlation between term frequency statistics and 138 factual knowledge of LLMs.\" This was not sufficiently clear to me:  I thought your main claim is about the relation between these stats in pretraining and in model behavior. This should be made clearer. Otherwise, a well written previous work section.\n",
      "-- Section 4.2: Why not use the more standard names then like marginal probability, joint probability and PMI?  -- Figures 2b, 3: The graphs do not show much in my opinion. Could they be explained in a sentence in the text? what does the figure here contribute?\n",
      "-- Figure 5 (and in other places in the paper): can you also compute correlation w/o binning? what does that turn out to be?\n",
      "-- Section 6.1: a more formal definition of the filtering method should be given.\n",
      "Grammar: -- l. 442: performances s.b. performance -- l. 501: changes s.b. change\n",
      "\n",
      "ðŸ“Œ Ethical concerns:\n",
      "No\n",
      "\n",
      "ðŸ§ª Scores:\n",
      "  - Soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n",
      "  - Excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n",
      "  - Reproducibility: 5: Could easily reproduce the results.\n",
      "\n",
      "ðŸ’¬ Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.\n",
      "\n",
      "ðŸ”¸ Review #2:\n",
      "Reviewer ID: EIKFZmuV6r\n",
      "\n",
      "ðŸ“Œ Paper topic and main contributions:\n",
      "The paper argues that large language models suffer from the so-called 'co-occurrence bias', i.e. when asked a factual question they tend to assign higher probabilities to words that have higher co-occurrence statistics with the query instead of the correct answer. The authors show that this bias persists in models of different sizes and cannot be effectively removed using fine-tuning, but can be partially remedied with knowledge editing. In order to prove their point, the authors introduce a series of term-frequency baselines, two of which are co-occurrence statistics. They show that these statistics can be used as a shortcut feature (in some settings, selecting the most frequently co-occurrent word results in 60% accuracy on the test set) and explain the actual behaviour of the models to a large extent. The paper explores two mitigation strategies: fine-tuning on a debiased version of the training dataset and rank-one model editing (ROME). While fine-tuning is not particularly effective, ROME shows promising results on frequent relation-like facts but does not really help with rarer ones.\n",
      "\n",
      "ðŸ“Œ Reasons to accept:\n",
      "In my opinion, the most interesting aspect of the paper are the frequency baselines introduced to analyse the structure of the training dataset and to explain the behaviour of the models. They bring the co-occurrence/frequency bias influencing the behaviour of the models to fore, and the analysis is convincing.\n",
      "\n",
      "ðŸ“Œ Reasons to reject:\n",
      "The main point of the paper -- that LLMs suffer from co-occurrency bias -- is not particularly new. Other papers investigating this issue are mentioned in the Related Work section. The authors claim that their work is the fist \"to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs\" (ll. 136--139), but there is no discussion of why fine-tuning should help at all, and in the end it does not, which amounts to a weak negative result. The section on mitigating occupies less than 1.5 pages and does not contain any methodological insights.\n",
      "\n",
      "ðŸ“Œ Questions for the authors:\n",
      "In the discussion of fine-tuning in lines 347--348, it is said that \"the models may learn appropriate cadidate sets during finetuning.\" Does this mean that there is some amount of knowledge leakage in fine-tuning and that the testing set-up is slightly different from the zero-shot setting?\n",
      "In Figure 6, we see that the accuracy of the models' outputs _improves_ when the conditional probability of the object given the subject goes down from around 1/64 to 0. This goes against the general trend of the positive association between conditional probability and accuracy. Is there any interpretation for this?\n",
      "\n",
      "ðŸ“Œ Typos grammar style and presentation improvements:\n",
      "Some of the statements in the paper look confusing. In lines 87--89, the authors point out that \"relying heavily on co-occurrence is not appropriate for understanding the accurate meaning behind words.\" This is probably true, but this is all we have when training LLMs, and the paper does not propose a way out. Similarly, in lines 173--174, the authors state that \"relying on co-occurrence implies simple memorization.\" Again, the relationship between co-occurrence and memorization is more complicated, and equating one with the other seems erroneous.\n",
      "\n",
      "ðŸ“Œ Ethical concerns:\n",
      "No\n",
      "\n",
      "ðŸ§ª Scores:\n",
      "  - Soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.\n",
      "  - Excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.\n",
      "  - Reproducibility: 3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.\n",
      "\n",
      "ðŸ’¬ Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.\n",
      "\n",
      "ðŸ”¸ Review #3:\n",
      "Reviewer ID: 6J9oy63MjV\n",
      "\n",
      "ðŸ“Œ Paper topic and main contributions:\n",
      "The paper proposes a framework to probe the shortcuts in LLMs. The framework firsts test the accuracy of LLM on factual knowledge dataset like LAMA. Then the framework computes the correlation of the subject and the object in the pretraining corpus. Results on GPT models (from 125M to 6B) show that, as the co-occurrence of subject and object decreases, the accuracy also decreases. Larger models (GPT-3.5 of 175B) also suffer from the problem. The authors also find that simple baseline based on co-occurrence is sufficient to surpass the performance of GPT-J 6B. Finally, the authors also give solution to mitigate the shortcut problem.\n",
      "\n",
      "ðŸ“Œ Reasons to accept:\n",
      "-\tThe paper contains detailed analysis of shortcut problem regarding token co-occurrence. \n",
      "-\tThe paper investigates an important area of verifying the factual knowledge of LLMs. \n",
      "-\tThe paper is well-written, and is free of significant presentation issues. \n",
      "-\tAlong with the identification of the problem, the paper also proposes to mitigate the problem.\n",
      "\n",
      "ðŸ“Œ Reasons to reject:\n",
      "-\tOne more experiment should be done to verify the claim that \"answers with higher co-occurrence are more likely to be generated\": The authors should count in each question, whether the model's generated answer has a high count in the pretraining corpus. Currently there is only a table (Table 1) showing similar results, i.e., the wrong answers have a relatively lower count in the pretraining corpus. However, quantitative results over the whole dataset should be given. \n",
      "-\tThere are existing work probing the shortcut learning problem of language models. The authors should elaborate more on the work to claim that they are the first to investigate the effects of finetuning on the correlation between term frequency statistics and factual knowledge of LLMs. For example: [2].\n",
      "\n",
      "ðŸ“Œ Questions for the authors:\n",
      "-\tThe paper focuses only on token-level shortcuts. Can the findings be generalized to other kinds of shortcuts, such as word-overlap (Right for the wrong reasons: Diagnosing syntactic heuris- tics in natural language inference) or length (Annotation artifacts in natural language inference data) shortcuts? \n",
      "-\tThe proposed mitigation methods include balancing the data and knowledge editing. However, I am curious about whether demonstrations or Chain-of-Thoughts [1] can also solve the issue. \n",
      "-\tFig. 3 shows that smaller model cannot memorize the data. Then why it also has the shortcut problem like other larger models who memorize the biases in the pretraining data?\n",
      "\n",
      "ðŸ“Œ Missing references:\n",
      "[1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [2] Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models (NAACL Findings 2022)\n",
      "\n",
      "ðŸ“Œ Ethical concerns:\n",
      "No\n",
      "\n",
      "ðŸ§ª Scores:\n",
      "  - Soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments. \n",
      "  - Excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\n",
      "  - Reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.\n",
      "\n",
      "ðŸ’¬ Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.\n"
     ]
    }
   ],
   "source": [
    "# Load review data\n",
    "reviews_path = v1_files.get(\"reviews.json\")\n",
    "if reviews_path and reviews_path.exists():\n",
    "    with open(reviews_path, \"r\") as f:\n",
    "        reviews_data = json.load(f)\n",
    "\n",
    "    print(\"\\nðŸ“ Reviews:\")\n",
    "    if reviews_data:\n",
    "        for i, review in enumerate(reviews_data):\n",
    "            print(f\"\\nðŸ”¸ Review #{i + 1}:\")\n",
    "            print(\"Reviewer ID:\", review.get(\"rid\", \"[unknown]\"))\n",
    "\n",
    "            # Extract report sections if available\n",
    "            report = review.get(\"report\", {})\n",
    "            if report:\n",
    "                for section, text in report.items():\n",
    "                    print(f\"\\nðŸ“Œ {section.replace('_', ' ').capitalize()}:\")\n",
    "                    print(text.strip() if text else \"[empty]\")\n",
    "            else:\n",
    "                print(\"No report found.\")\n",
    "\n",
    "            # Optional: print scores\n",
    "            scores = review.get(\"scores\", {})\n",
    "            if scores:\n",
    "                print(\"\\nðŸ§ª Scores:\")\n",
    "                for score_cat, score_val in scores.items():\n",
    "                    print(f\"  - {score_cat.capitalize()}: {score_val}\")\n",
    "\n",
    "            # Optional: reviewer confidence\n",
    "            confidence = review.get(\"meta\", {}).get(\"reviewer_confidence\")\n",
    "            if confidence:\n",
    "                print(\"\\nðŸ’¬ Reviewer Confidence:\", confidence)\n",
    "    else:\n",
    "        print(\"No reviews found in file.\")\n",
    "else:\n",
    "    print(\"reviews.json not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc9568a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Top-level keys in docling JSON:\n",
      "dict_keys(['schema_name', 'version', 'name', 'origin', 'furniture', 'body', 'groups', 'texts', 'pictures', 'tables', 'key_value_items', 'pages'])\n",
      "\n",
      "ðŸ” Preview of the 'content' field (if exists):\n",
      "No 'content' field found.\n",
      "\n",
      "ðŸ” Raw document preview:\n",
      "{'body': {...},\n",
      " 'furniture': {...},\n",
      " 'groups': [],\n",
      " 'key_value_items': [],\n",
      " 'name': 'paper',\n",
      " 'origin': {...},\n",
      " 'pages': {...},\n",
      " 'pictures': [...],\n",
      " 'schema_name': 'DoclingDocument',\n",
      " 'tables': [...],\n",
      " 'texts': [...],\n",
      " 'version': '1.0.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Diagnostic: Explore the actual docling file structure\n",
    "docling_path = v1_files.get(\"paper.docling.json\")\n",
    "if docling_path and docling_path.exists():\n",
    "    with open(docling_path, \"r\") as f:\n",
    "        docling_data = json.load(f)\n",
    "\n",
    "    print(\"ðŸ” Top-level keys in docling JSON:\")\n",
    "    pprint(docling_data.keys())\n",
    "\n",
    "    print(\"\\nðŸ” Preview of the 'content' field (if exists):\")\n",
    "    if \"content\" in docling_data:\n",
    "        pprint(docling_data[\"content\"], depth=2)\n",
    "    else:\n",
    "        print(\"No 'content' field found.\")\n",
    "\n",
    "    print(\"\\nðŸ” Raw document preview:\")\n",
    "    pprint(docling_data, depth=1)\n",
    "else:\n",
    "    print(\"paper.docling.json not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66a13146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Parsed Paper (docling):\n",
      "Document Name: paper\n",
      "Schema Version: 1.0.0\n",
      "\n",
      "ðŸ§¾ First Text Chunks:\n",
      "\n",
      "ðŸ”¹ Text #1:\n",
      "Impact of Co-occurrence on Factual Knowledge of Large Language Models\n",
      "\n",
      "ðŸ”¹ Text #2:\n",
      "Cheongwoong Kang\n",
      "\n",
      "ðŸ”¹ Text #3:\n",
      "KAIST\n",
      "\n",
      "ðŸ”¹ Text #4:\n",
      "cw.kang@kaist.ac.kr\n",
      "\n",
      "ðŸ”¹ Text #5:\n",
      "Abstract\n",
      "\n",
      "ðŸ§± Structure Overview (body keys):\n",
      "dict_keys(['self_ref', 'children', 'name', 'label'])\n"
     ]
    }
   ],
   "source": [
    "docling_path = v1_files.get(\"paper.docling.json\")\n",
    "if docling_path and docling_path.exists():\n",
    "    with open(docling_path, \"r\") as f:\n",
    "        docling_data = json.load(f)\n",
    "\n",
    "    print(\"\\nðŸ“„ Parsed Paper (docling):\")\n",
    "\n",
    "    # Show top-level metadata if available\n",
    "    print(\"Document Name:\", docling_data.get(\"name\", \"[no name]\"))\n",
    "    print(\"Schema Version:\", docling_data.get(\"version\", \"[no version]\"))\n",
    "\n",
    "    # Display first few text segments\n",
    "    texts = docling_data.get(\"texts\", [])\n",
    "    if texts:\n",
    "        print(\"\\nðŸ§¾ First Text Chunks:\")\n",
    "        for i, item in enumerate(texts[:5]):\n",
    "            content = item.get(\"text\", \"[no text]\")\n",
    "            print(f\"\\nðŸ”¹ Text #{i+1}:\")\n",
    "            print(content[:500] + \"...\" if len(content) > 500 else content)\n",
    "    else:\n",
    "        print(\"No texts found in document.\")\n",
    "\n",
    "    # Optionally preview body or furniture\n",
    "    print(\"\\nðŸ§± Structure Overview (body keys):\")\n",
    "    pprint(docling_data.get(\"body\", {}).keys())\n",
    "else:\n",
    "    print(\"paper.docling.json not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "504f2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_full_paper(docling_data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Reconstruct the full paper text from the docling-formatted JSON.\n",
    "\n",
    "    Args:\n",
    "        docling_data (dict): Parsed JSON data from paper.docling.json.\n",
    "\n",
    "    Returns:\n",
    "        str: The full concatenated text of the paper.\n",
    "    \"\"\"\n",
    "    texts = docling_data.get(\"texts\", [])\n",
    "    full_text = \"\\n\\n\".join(t.get(\"text\", \"\") for t in texts if \"text\" in t and t[\"text\"].strip())\n",
    "    return full_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64d491c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Full paper preview (first 10000 characters):\n",
      "Impact of Co-occurrence on Factual Knowledge of Large Language Models\n",
      "\n",
      "Cheongwoong Kang\n",
      "\n",
      "KAIST\n",
      "\n",
      "cw.kang@kaist.ac.kr\n",
      "\n",
      "Abstract\n",
      "\n",
      "Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple cooccurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at https://github.com/CheongWoong/ impact_of_cooccurrence .\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "Natural language processing has seen significant progress in recent years with the advent of large language models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020). Factual knowledge probing benchmarks like LAMA have demonstrated that LLMs have a high capacity to recall factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Roberts et al., 2020; Shin et al., 2020; Zhong et al., 2021). However, factual knowledge stored in LLMs may not always be correct (Elazar et al., 2021; Cao et al., 2021). Understanding the reasons behind such inaccuracies is critical for developing more accurate and reliable language models. Recent studies point out that LLMs often learn short-\n",
      "\n",
      "Jaesik Choi KAIST\n",
      "\n",
      "jaesik.choi@kaist.ac.kr\n",
      "\n",
      "Figure 1: This figure shows an overall framework of our correlation analysis between co-occurrence counts and factual knowledge of LLMs. We assume that if the target model heavily relies on subject-object co-occurrence, it is more likely to recall the most co-occurring word without accurate semantic understanding. For instance, in this hypothetical example, the model fails to answer the question about the capital of Canada by generating the most frequently co-occurring word 'Toronto', while the correct answer is 'Ottawa'. This indicates that relying heavily on co-occurrence statistics may have potential errors.\n",
      "\n",
      "The capital of\n",
      "\n",
      "Canada\n",
      "\n",
      "is [MASK]\n",
      "\n",
      "Language Model\n",
      "\n",
      "Pre-training data\n",
      "\n",
      "Toronto\n",
      "\n",
      "Ottawa\n",
      "\n",
      "Subject\n",
      "\n",
      "Object\n",
      "\n",
      "Count\n",
      "\n",
      "Canada\n",
      "\n",
      "Toronto\n",
      "\n",
      "246\n",
      "\n",
      "Canada\n",
      "\n",
      "Ottawa\n",
      "\n",
      "19\n",
      "\n",
      "â€¦\n",
      "\n",
      "â€¦\n",
      "\n",
      "â€¦\n",
      "\n",
      "Canada\n",
      "\n",
      "London\n",
      "\n",
      "8\n",
      "\n",
      "cuts relying on spurious features rather than understanding language (Wallace et al., 2019; McCoy et al., 2019; Poerner et al., 2020; Ettinger, 2020; Kassner and SchÃ¼tze, 2020; Cao et al., 2021; Elazar et al., 2021; Bender et al., 2021). We suspect that relying on co-occurrence statistics of the pre-training corpora is one of the main factors that cause such behaviors (Razeghi et al., 2022; Li et al., 2022; Elazar et al., 2022; Kandpal et al., 2023; Kazemi et al., 2023).\n",
      "\n",
      "In this work, we investigate the effects of cooccurrence statistics of the pre-training data on factual knowledge in LLMs. First, we adopt the LAMA dataset (Petroni et al., 2019) to probe factual knowledge, represented as a subject-relationobject triple. Then, we analyze the correlation between co-occurrence statistics and performance on\n",
      "\n",
      "factual knowledge probing. Specifically, we count co-occurrences of word pairs in the pre-training corpora. We focus on subject-object co-occurrence, motivated by the concept of distant supervision, which shows that a sentence often contains the triple if it contains a subject and an object of a triple (Mintz et al., 2009). Figure 1 illustrates an overall framework of our correlation analysis between cooccurrence counts and factual knowledge of LLMs. We hypothesize that the target model would generate the most frequently co-occurring word if it heavily relies on co-occurrence. In this simulated example, given the fact 'Canada'-'capital'-'Ottawa', the target model generates the most frequently cooccurring word 'Toronto', which is not the correct answer.\n",
      "\n",
      "We test our hypothesis with GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), which are open-source versions of GPT-3 (Brown et al., 2020). We compute co-occurrence statistics of the Pile dataset (Gao et al., 2020), on which the target models are pre-trained. We show that the factual probing accuracy of LLMs highly correlates with subject-object co-occurrence, leading to failures in recalling rare facts. Although scaling up model sizes or finetuning boosts the overall performance on factual knowledge probing, they do not resolve co-occurrence bias, in which frequently co-occurred words are preferred over the correct answer. Besides, we find that a significant portion of facts in the LAMA dataset can be recalled by simply generating the object with the highest co-occurrence count. Although cooccurrence is necessary to recall factual knowledge, it is not sufficient. Therefore, relying heavily on co-occurrence is inappropriate for understanding the accurate meaning behind words.\n",
      "\n",
      "Relying heavily on the co-occurrence statistics may lead to hallucinations (Fish, 2009; Maynez et al., 2020; Ji et al., 2023) if the co-occurrence statistics reflect factually incorrect information. Therefore, we suggest finetuning LLMs on the debiased LAMA, constructed by filtering out biased samples whose subject-object co-occurrence count is high. Although finetuning on the debiased dataset allows LLMs to learn rare facts that appear in the training set, it is not generalizable to test cases.\n",
      "\n",
      "In summary, we show that factual knowledge probing accuracy correlates with subject-object co-occurrence. In addition, we present novel ev-\n",
      "\n",
      "idence and insights by providing a more detailed picture. Specifically, we demonstrate that LLMs prefer frequently co-occurring words, which often override the correct answer, especially when the correct answer rarely co-occurs with the subject. While existing studies only show that the performance of LLMs correlates with co-occurrence, our results provide evidence and reasons for that. We hope our results spur future work on mitigating co-occurrence bias to build accurate and reliable language models.\n",
      "\n",
      "2 Related Work\n",
      "\n",
      "2.1 Prompt Tuning and Finetuning\n",
      "\n",
      "There have been recent attempts to tune input prompts to improve the performance of LLMs further (Liu et al., 2023b; Lester et al., 2021; Li and Liang, 2021; Qin and Eisner, 2021; Liu et al., 2022, 2023a). However, directly optimizing prompts is not trivial since changes in the input space may cause non-monotonic performance changes (Hu et al., 2022). Especially, Fichtel et al. (2021) demonstrate that finetuned LMs outperform prompt-tuned LMs on factual knowledge probing tasks. Although LLMs, such as GPT-3 and T0, were primitively designed to perform well on various tasks without finetuning (Brown et al., 2020; Sanh et al., 2022), recent work shows that finetuning improves the linguistic capabilities of LLMs substantially (Ouyang et al., 2022; Wei et al., 2022). Therefore, we consider finetuned LMs for analysis.\n",
      "\n",
      "2.2 Term Frequency and Model Behaviors\n",
      "\n",
      "There have been several approaches to understanding the effects of training data on model behaviors. Specifically, recent studies observe a correlation between pre-training term frequency and model behaviors (Kassner et al., 2020; Wei et al., 2021; Li et al., 2022; Razeghi et al., 2022; Kandpal et al., 2023; Elazar et al., 2022). Our work offers unique contributions by providing additional evidence and insights with in-depth analysis. Specifically, we verify that (1) LLMs learn co-occurrence bias from the pre-training data, preferring frequently co-occurred words over the correct answer, which is especially problematic when recalling rare facts, and (2) co-occurrence bias is not overcome either by scaling up model sizes or finetuning.\n",
      "\n",
      "2.3 Spurious Features\n",
      "\n",
      "A spurious correlation refers to a relationship in which variables are correlated but does not imply causation due to a coincidence or a confounding factor (Simon, 1954). LMs often learn shortcuts relying on spurious features, such as word overlap, type matching, misprimes, and surface form, which mostly come from dataset bias (Gururangan et al., 2018; McCoy et al., 2019; Wallace et al., 2019; Kassner and SchÃ¼tze, 2020; Poerner et al., 2020; Wang et al., 2022). For example, if a heuristic (e.g. word overlap, surface form) frequently cooccurs with specific labels, the models may learn the shortcut relying on the heuristic to make decisions. Although spurious features may be helpful in generating plausible responses, it is not appropriate for accurate semantic understanding. Our work suggests that co-occurrence statistics of the pre-training data may work as spurious features, causing hallucinations (Fish, 2009; Maynez et al., 2020; Ji et al., 2023) or biased responses (Bolukbasi et al., 2016; Caliskan et al., 2017; Bommasani et al., 2021).\n",
      "\n",
      "2.4 Memorization\n",
      "\n",
      "LLMs have been shown to memorize information in training data and generate it verbatim at test time (Emami et al., 2020; Feldman and Zhang, 2020; McCoy et al., 2023; Zhang et al., 2021; Lee et al., 2022; Akyurek et al., 2022; Magar and Schwartz, 2022; Carlini et al., 2023). Memorization implies that LLMs recall memorized information rather than generalizing to new inputs based on learned knowledge. Although recent studies indicate that memorization poses privacy risks (Song and Shmatikov, 2019; Carlini et al., 2019, 2021; Kandpal et al., 2022), it is necessary for nearoptimal generalization when learning from a longtail distribu...\n"
     ]
    }
   ],
   "source": [
    "# Assuming docling_data was loaded already\n",
    "full_paper_text = reconstruct_full_paper(docling_data)\n",
    "\n",
    "print(\"ðŸ§¾ Full paper preview (first 10000 characters):\")\n",
    "print(full_paper_text[:10000] + \"...\" if len(full_paper_text) > 10000 else full_paper_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
