{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64b8ced",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "All later components (baseline, RAG, metrics) need the same corpus and review “ground truth.” Cleaning and structuring it early prevents painful re‑work.\n",
    "\n",
    "• Pick a public peer‑review dataset (e.g., PeerRead, ACL Blind Submission). <br>\n",
    "• Write a loader that yields (paper_text, meta, gold_review) triples. <br>\n",
    "• Basic preprocessing: strip markup, split into sections, tokenize. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a96e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872c3f3",
   "metadata": {},
   "source": [
    "Each json file contains a list of dictionaries with the respective entries:\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"...\",\n",
    "    \"input\": \"...\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"...\",\n",
    "    \"input\": \"...\"\n",
    "  },\n",
    "  ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d76bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: You are a decision maker. Please review all responses from the author and comments from all reviewers to provide the meta-review and determine the final decision. Explicitly state 'Accept' or 'Reject' at the end of your output.\n",
      "Input: Title: Characterizing and Measuring the Similarity of Neural Networks with Persistent Homology . Abstract: Characterizing the structural properties of neural networks is crucial yet poorly understood, and there are no well-established similarity measures between networks. In this work, we observe that neural networks can be represented as abstract simplicial complex and analyzed using their topological 'fingerprints' via Persistent Homology (PH). We then describe a PH-based representation proposed for characterizing and measuring similarity of neural networks. We empirically show the effectiveness of this representation as a descriptor of different architectures in several datasets. This approach based on Topological Data Analysis is a step towards better understanding neural networks and a useful similarity measure. Main Text: Introduction\n",
      "\n",
      "Machine learning practitioners can train different neural networks for the same task. Even for the same neural architecture, there are many hyperparameters, such as the number of neurons per layer or the number of layers. Moreover, the final weights for the same architecture and hyperparameters can vary depending on the initialization and the optimization process itself, which is stochastic. Thus, there is no direct way of comparing neural networks accounting for the fact that neural networks solving the same task should be measured as being similar, regardless of the specific weights. This also prevents one from finding and comparing modules inside neural networks (e.g., determining if a given sub-network does the same function as other sub-network in another model). Moreover, there are no well-known methods for effectively characterizing neural networks.\n",
      "\n",
      "This work aims to characterize neural networks such that they can be measured to be similar once trained for the same task, with independence of the particular architecture, initialization, or optimization process. To characterize neural networks and measuring their similarity, we assume two different similarities that should be related if the characterization is correct, namely, 1. similarity between the tasks a model is trained on, and 2. similarity between neural networks themselves considered as mathematical objects. We do not provide a mathematical definition for the first one, rather an intuitive one, and understand is as qualitative similarity between semantically close tasks.\n",
      "\n",
      "For instance, we think that MNIST (10 labels) is closer to MNIST restricted to 2 labels than to language identification, and that MNIST is closer to MNIST restricted to 8 labels than to MNIST\n",
      "restricted to 2 labels. For the second one, in this work we propose a similarity measure.\n",
      "\n",
      "Ideally, this similarity measure should then capture 1. the semantic similarity between the underlying tasks neural networks have been trained on (understanding that networks performing similar tasks should be similar, regardless of their specific weights and architectures), and 2. intrinsic properties of the neural network should also be captured to some degree. Thus, we consider two scenarios, that is, comparing NNs trained for similar tasks, using the mentioned task similarity, and comparing neural networks trained on identical tasks but with different architectures.\n",
      "\n",
      "We focus on Multi-Layer Perceptrons (MLPs) for the sake of simplicity. We start by observing that we can represent a neural network as a directed weighted graph to which we can associate certain topological concepts.1 Considering it as a simplicial complex, we obtain its associated Persistent Diagram. Then, we can compute distances between Persistent Diagrams of different neural networks.\n",
      "\n",
      "1See Jonsson (2007) for a complete reference on graph topology.\n",
      "\n",
      "1 The proposed experiments aim to show that the selected structural feature, Persistent Homology, serves to relate neural networks trained for similar problems and that such a comparison can be performed by means of a predefined measure between the associated Persistent Homology diagrams.\n",
      "\n",
      "To test the hypothesis, we study different classical problems (MNIST, Fashion MNIST, CIFAR-10, and language identification and text classification datasets), different architectures (number and size of layers) as well as a control experiment (input order, to which neural network similarity should be invariant). The code and results are fully open source in the Supplementary Material under a MIT\n",
      "license.\n",
      "\n",
      "In summary, the main contributions of this work are the following: 1. We propose an effective graph characterization strategy of neural networks based on Persistent Homology. 2. Based on this characterization, we suggest a similarity measure of neural networks. 3. We provide empirical evidence that this Persistent Homology framework captures valuable information from neural networks and that the proposed similarity measure is meaningful.\n",
      "\n",
      "The remainder of this paper is organized as follows. In Section 2, we go through the related work.\n",
      "\n",
      "Then, in Section 3 we describe our proposal and the experimental framework to validate it. Finally, in sections 4 and 5 we report and discuss the results and arrive to conclusions, respectively.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "One of the fundamental papers of Topological Data Analysis (TDA) is presented in Carlsson (2009) and suggests the use of Algebraic Topology to obtain qualitative information and deal with metrics for large amounts of data. For an extensive overview of simplicial topology on graphs, see Giblin\n",
      "(1977); Jonsson (2007). Aktas et al. (2019) provide a thorough analysis of PH methods.\n",
      "\n",
      "More recently, a number of publications have dealt with the study of the capacity of neural networks using PH. Guss & Salakhutdinov (2018) characterize learnability of different neural architectures by computable measures of data complexity. Donier (2019) propose the concept of spatial capacity allocation analysis. Konuk & Smith (2019) propose an empirical study of how NNs handle changes in topological complexity of the input data.\n",
      "\n",
      "Rieck et al. (2019b) introduce the *neural persistence* metric, a complexity measure based on TDA on weighted stratified graphs. This work suggests a representation of the neural network as a multipartite graph. They perform the filtering of the Persistent Homology diagrams independently for each layer. As the filtration contains at most 1-simplices (edges), they only capture zero-dimensional topological information, i.e. connectivity information. When consecutive layer analysis is used, the global topology of the network is not taken into account making the strong assumption that the NNs encode the learned information layer pairwise exclusively. Additionally, there are trivial global transformations of a NN that are not captured by analyzing pairs of layers:\n",
      "- Superfluous cycle insertions: for example, add two neurons and connect their input to a single neuron in a lower layer and their two outputs to a single output neuron in an upper layer with opposite weights.\n",
      "\n",
      "- Identity layer insertions: for instance, insert an intermediate identity layer with neurons and trivially connect to the next layer.\n",
      "\n",
      "- Non-planar neural networks analysis: the analysis of neural networks that use multiple connections between non-consecutive layers require higher order topological analysis.\n",
      "\n",
      "In terms of pure neural network analysis, there are relevant works, like Hofer et al. (2020), that study topological regularization. Clough et al. (2020) introduce a method for training neural networks for image segmentation with prior topology knowledge, specifically via Betti numbers. Corneanu et al.\n",
      "\n",
      "(2020) try to estimate (with limited success) the performance gap between training and testing via neuron activations and linear regression of the Betti numbers. This type of representation depend on the input data and not only on the NN function under study. Instead, we are interested in characterising and comparing NNs as functions, independently of the data to which they are applied.\n",
      "\n",
      "On the other hand, topological analysis of decision boundaries has been a very prolific area. Ramamurthy et al. (2019) propose a labeled Vietoris-Rips complex to perform PH inference of decision boundaries for quantification of the complexity of neural networks. Naitzat et al. (2020) experiment on the PH of a wide range of point cloud input datasets for a binary classification problems to see that NNs transform a topologically rich dataset (in terms of Betti numbers) into a topologically simpler one as it passes through the layers. They also verify that the reduction in Betti numbers is significantly faster for ReLU activations than hyperbolic tangent activations.\n",
      "\n",
      "Regarding neural network representations, one of the most related works to ours, Gebhart et al.\n",
      "\n",
      "(2019), focuses on topological representations of neural networks. They introduce a method for computing PH over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input.\n",
      "\n",
      "Interestingly, in Watanabe & Yamana (2020), authors work on neural network representations through simplicial complexes based on deep Taylor decomposition and they calculate the PH of neural networks in this representation. In Chowdhury et al. (2019), they use directed homology to represent MLPs. They show that the path homology of these networks is non-trivial in higher dimensions and depends on the number and size of the network layers. They investigate homological differences between distinct neural network architectures.\n",
      "\n",
      "As far as neural network similarity measures are concerned, the literature is not especially prolific. In Kornblith et al. (2019), authors examine similarity measures for representations (meaning, outputs of different layers) of neural networks based on canonical correlation analysis. However, note that this method *compares neural network representations (intermediate outputs), not the neural networks* themselves. Remarkably, in Ashmore & Gashler (2015), authors do deal with the intrinsic similarity of neural networks themselves based on Forward Bipartite Alignment. Specifically, they propose an algorithm for aligning the topological structures of two neural networks. Their algorithm finds optimal bipartite matches between the nodes of the two MLPs by solving the well-known graph cutting problem. The alignment enables applications such as visualizations or improving ensembles.\n",
      "\n",
      "However, the methods only works under very restrictive assumptions,2and this line of work does not appear to have been followed up.\n",
      "\n",
      "Finally, we note that there has been a considerable growth of interest in applied topology in the recent years. This popularity increase and the development of new software libraries,3along with the growth of computational capabilities, have empowered new works. Some of the most remarkable libraries are Ripser Tralie et al. (2018); Bauer (2021), and Flagser Lütgehetmann et al. (2019). They are focused on the efficient computation of PH. For GPU-Accelerated computation of Vietoris-Rips PH, Ripser++ Zhang et al. (2020) offers an important speedup. The Python library we are using, Giotto-TDA Tauzin et al. (2020), makes use of both above libraries underneath.\n",
      "\n",
      "We have seen that there is a trend towards the use of algebraic topology methods for having a better understanding of phenomena of neural networks and having more principled deep learning algorithms.\n",
      "\n",
      "Nevertheless, little to no works have proposed neural network characterizations or similarity measures based on intrinsic properties of the networks, which is what we intend to do.\n",
      "\n",
      "## 3 Methodology\n",
      "\n",
      "In this section, we propose our method, which is heavily based on concepts from algebraic topology.\n",
      "\n",
      "We refer the reader to the Supplementary Material for the mathematical definitions. In this section, we also describe the conducted experiments.\n",
      "\n",
      "Intrinsically characterizing and comparing neural networks is a difficult, unsolved problem. First, the network should be represented in an object that captures as much information as possible and then it should be compared with a measure depending on the latent structure. Due to the stochasticity of both the initialization and training procedure, networks are parameterized differently. For the same task, different functions that effectively solve it can be obtained. Being able to compare the trained networks can be helpful to detect similar neural structures.\n",
      "\n",
      "We want to obtain topological characterizations associated to neural networks trained on a given task. For doing so, we use the Persistence Homology (from now on, PH) of the graph associated to a\n",
      "\n",
      "2For example, the two neural networks \"must have the same number of units in each of their corresponding layers\", and the match is done layer by layer.\n",
      "\n",
      "3https://www.math.colostate.edu/~adams/advising/appliedTopologySoftware/\n",
      "3 neural network. We compute the PH for various neural networks learned on different tasks. We then compare all the diagrams for each one of the task.\n",
      "\n",
      "More specifically, for each of the studied tasks (image classification on MNIST, Fashion MNIST\n",
      "and CIFAR-10; language identification, and text classification on the Reuters dataset),4 we proceed as follows: 1. We train several neural network models on the particular problem. 2. We create a directed graph from the weights of the trained neural networks (after changing the direction of the negative edges and normalising the weights of the edges). 3. We consider the directed graph as a simplicial complex and calculate its PH, using the weight of the edges as the filtering parameter, which range from 0 to 1. This way we obtain the so-called Persistence Diagram. 4. We compute the distances between the Persistence Diagrams (prior discretization of the Persistence Diagram so that it can be computed) of the different networks. 5. Finally, we analyze the similarity between different neural networks trained for the same task, for a similar task, and for a completely different task, independently of the concrete architecture, to see whether there is topological similarity.\n",
      "\n",
      "As baselines, we set two standard matrix comparison methods that are the 1-Norm and the Frobenius norm. Having adjacency matrix A and B, we compute the difference as *norm*(A−B). However, these methods only work for matrices of similar size and thus, they are not general enough. We could also have used the Fast Approximate Quadratic assignment algorithm suggested in Vogelstein et al.\n",
      "\n",
      "(2015), but for large networks this method becomes unfeasible to compute.\n",
      "\n",
      "## 3.1 Proposal\n",
      "\n",
      "Our method is as follows. We start by associating to a neural network a weighted directed graph that is analyzed as an abstract simplicial complex consisting on the union of points, edges, triangles, tetrahedrons and larger dimension polytopes (those are the elements referred as simplices). Abstract simplicial complexes are used in opposition to geometric simplicial complexes, generated by a point cloud embedded in the Euclidean space R\n",
      "n.\n",
      "\n",
      "Given a trained neural network, we take the collection of neural network parameters as directed and weighted edges that join neurons, represented by graph nodes. Biases are considered as new vertices that join target neurons with an edge having a given weight. Note that, in this representation, we lose the information about the activation functions, for simplicity and to avoid representing the network as a multiplex network. Bias information could also have been ignored because we want large PH\n",
      "groups that characterize the network, while these connections will not change the homology group dimension of any order.\n",
      "\n",
      "For negative edge weights, we reverse edge directions and maintain the absolute value of the weights.\n",
      "\n",
      "We discard the use of weight absolute value since neural networks are not invariant under weight sign transformations. This representation is consistent with the fact that every neuron can be replaced by a neuron from which two edges with opposite weights emerge and converge again on another neuron with opposite weights. From the point of view of homology, this would be represented as a closed cycle. We then normalize the weights of all the edges as expressed in Equation 1 where w is the weight to normalize, W are all the weights and ζ is an smoothing parameter that we set to 0.000001. This smoothing parameter is necessary as we want to avoid normalized weights of edges to be 0. This is because 0 implies a lack of connection.\n",
      "\n",
      "$$m a x(1-{\\frac{|w|}{m a x(|W|)}},\\zeta)$$\n",
      "$$(1)$$\n",
      ",ζ ) (1)\n",
      "Given this weighted directed graph, we then define a directed flag complex associated to it. Topology of this directed flag complex can be studied using homology groups Hn. In this work we calculate homology groups up to degree 3 (H0-H3) due to computational complexity and our neural network representation method's layer connectivity limit.\n",
      "\n",
      "The dimensions of these homology groups are known as Betti numbers. The i-th Betti number is the number of i-dimensional voids in the simplicial complex (β0 gives the number of connected components of the simplicial complex, β1 gives the number of non reducible loops and so on). For a deeper introduction to algebraic topology and computational topology, we refer to Edelsbrunner &\n",
      "Harer (2009); Ghrist (2014).\n",
      "\n",
      "We work with a family of simplicial complexes, Kε , for a range of values of ε ∈ R so that the complex at step εiis embedded in the complex at ε j for i ≤ j, i.e. Kεi ⊆ Kε j\n",
      ". In our case, ε is the minimum weight of included edges of our graph representation of neural networks. Filtration parameter could also be used to select active nodes thought, the library we used does not include this capability.\n",
      "\n",
      "The nested family of simplicial complexes is called a *filtration*. We calculate a sequence of homology groups by varying the ε parameter, obtaining a persistence homology diagram. PH calculations are performed on Z2.\n",
      "\n",
      "This filtration gives a collection of contained directed weighted graph or simplicial complex Kεmin ⊆\n",
      "... ⊆ Kεt ⊆ Kεt+1 ⊆ ... ⊆ Kεmax , where t ∈ [0,1] and εmin = 0, εmax = 1 (recall that edge weights are normalized).\n",
      "\n",
      "Given a filtration, one can look at the birth, when a homology class appears, and death, the time when the homology class disappears. The PH treats the birth and the death of these homological features in Kε for different ε values. Lifespan of each homological feature can be represented as an interval\n",
      "(birth,*death*), of the homological feature. For each filtration, we can record all these intervals by a Persistence Barcode (PB) Carlsson (2009), or in a Persistence Diagram (PD), as a collection of multiset of intervals.\n",
      "\n",
      "As mentioned previously, our interest in this work is to compare PDs from two different simplicial complexes. There are two distances traditionally used to compare PDs, Wasserstein distance and Bottleneck distance. Their stability with respect to perturbations on PDs has been object of different studies Chazal et al. (2012); Cohen-Steiner et al. (2005). As shown in comparative studies such as in Fasy et al. (2020), different distances and different ways of vectorising persistence diagrams have results with different levels of stability and quality. In order to make computations feasible and to obviate noisy intervals, we filter the PDs by limiting the minimum PD interval size. We do so by setting a minimum threshold η = 0.01. Intervals with a lifespan under this value are not considered. Additionally, for computing distances, we need to remove infinity values. As we are only interested in the deaths until the maximum weight value, we replace all the infinity values by 1.0.\n",
      "\n",
      "Wasserstein distance calculations are computationally hard for large PDs (each PD of our NN models has a million persistence intervals per diagram). Therefore we use a vectorized version of PDs instead, also called PD discretization. This vectorized version summaries have been proposed and used on recent literature Adams et al. (2017); Berry et al. (2020); Bubenik (2015); Lawson et al. (2019); Rieck et al. (2019a). For the persistence diagram distance calculation, we use the Giotto-TDA library Tauzin et al. (2020) and compute the following supported vectorized persistence summaries: 1. Persistence landscape. 2. Weighted silhouette. 3. Heat vectorizations.\n",
      "\n",
      "## 3.2 Experimental Framework\n",
      "\n",
      "To determine the topological structural properties of trained NNs, we select different kinds of datasets.\n",
      "\n",
      "We opt for four well-known benchmarks in the machine learning community and one regarding language identification: (1) the MNIST5 dataset for classifying handwritten digit images, (2) the Fashion MNIST Xiao et al. (2017) dataset for classifying clothing images into 10 categories, (3) the CIFAR-106(CIFAR) dataset for classifying 10 different objects, (4) the Reuters dataset for classifying news into 46 topics, and (5) the Language Identification Wikipedia dataset7for identifying 7 different languages. For CIFAR-10 and Fashion MNIST, we pre-train a Convolutional NN (CNN), and the convolutional layers are shared between all the models of the same dataset as a feature extractor.\n",
      "\n",
      "Recall that we are focusing on MLPs, so we do not consider that convolutional weights. For MNIST,\n",
      "Reuters and Language Identification, we use an MLP. For Reuters and Language identification datasets, we vectorize the sentences with character frequency.\n",
      "\n",
      "5http://yann.lecun.com/exdb/mnist/\n",
      "6https://www.cs.toronto.edu/~kriz/cifar.html 7https://www.floydhub.com/floydhub/datasets/language-identification/1/\n",
      "data\n",
      "\n",
      "| Number   | Experiment       | Index   |\n",
      "|----------|------------------|---------|\n",
      "| 1        | Layer size       | 1-4     |\n",
      "| 2        | Number of layers | 5-9     |\n",
      "| 3        | Input order      | 10-14   |\n",
      "| 4        | Number of labels | 15-19   |\n",
      "\n",
      "Table 1: Indices of the experiments of the distance matrices.\n",
      "We study the following variables (hyperparameters): 1. Layer width, 2. Number of layers, 3. Input order8 4. Number of labels (number of considered classes). We define the *base* architecture as the one with a layer width of 512, 2 layers, the original features order, and considering all the classes\n",
      "(10 in the case of MNIST, Fashion MNIST and CIFAR, 46 in the case of Reuters and 7 in the case of the language identification task). Then, doing one change at a time, keeping the rest of the base architecture hyperparameters, we experiment with architectures with the following configurations:\n",
      "1. **Layer width**: 128, 256, 512 (*base*) and 1024. 2. **Number of layers**: 2 (*base*), 4, 6, 8 and 10.\n",
      "\n",
      "3. **Input order**: 5 different randomizations (with *base* structure), the control experiment. 4. **Number**\n",
      "of labels (MNIST, Fashion MNIST, CIFAR-10): 2, 4, 6, 8 and 10 (*base*). 5. **Number of labels**\n",
      "(Reuters): 2, 6, 12, 23 and 46 (*base*). 6. **Number of labels** (Language Identification): 2, 3, 4, 6 and 7 (*base*). Note that this is not a grid search over all the combinations. We always modify one hyperparameter at a time, and keep the rest of them as in the base architecture. In other words, we experiment with all the combinations such that only one of the hyperparameters is set to a non-base value at a time. For each dataset, we train 5 times (each with a different random weight initialization)\n",
      "each of these neural network configurations. Then, we compute the topological distances (persistence landscape, weighted silhouette, heat) among the different architectures. In total, we obtain 5×5×3 distance matrices (5 datasets, 5 random initializations, 3 distance measures). Finally, we average the 5 random initializations, such that we get 5×3 matrices, one for each distance on each dataset.\n",
      "\n",
      "All the matrices have dimensions 19×19, since 19 is the number of experiments for each dataset\n",
      "(corresponding to the total the number of architectural configurations mentioned above). Note that the base architecture appears 8 times (1, on the number of neurons per layer, 1 on the number of layers, 1 on the number of labels and the 5 randomizations of weight initializations). All experiments were executed in a machine with 2 NVIDIA V100 of 32GB, 2 Intel(R) Xeon(R) Platinum 8176 CPU\n",
      "@ 2.10GHz, and of 1.5TB RAM, for a total of around 3 days.\n",
      "\n",
      "Note that this work focuses on MLPs, however, the method proposed is also applicable to CNNs, as they have a MLP equivalent.9 The resulting NN is a highly sparse simplicial complex with a large number of parameters which makes PH calculation out of our computation capability for all combination of proposed experiments.\n",
      "\n",
      "## 4 Results & Discussion\n",
      "\n",
      "Results from control experiments can be seen in the third group on Figures 1 and 4. In these figures, groups are separated visually using white dashed lines. Experiments groups are specified in Table 1.\n",
      "\n",
      "Control experiments in all the images appear very dimmed, which means that they are very similar, as expected. Recall that the control experiments consist of 5 (randomizations) × 5 (executions) and that 25 different neural networks have been trained; each one of the network has more than 690,000 parameters that have been randomly initialized. After the training, results show that these networks have very close topological distance, as expected.\n",
      "\n",
      "For Figure 2 we computed both 1-norm and Frobenius norm (the baselines) for graphs' adjacency matrices of control experiments. Note that as we ran the experiment five times, we make the mean for each value of the matrix. In order to show whether the resulting values are positive or negative, we subtract to the maximum difference of each dataset the norm of each cell separately, we take the absolute value and we divide by the maximum difference of each dataset. Therefore, we obtain five values per dataset. Table 2 shows the statistics reflecting that the distance among the experiments are large and, thus, they are not characterizing any similarity but rather an important dissimilarity.\n",
      "\n",
      "8Order of the input features, the control experiment.\n",
      "\n",
      "9https://aul12.me/machinelearning/2019/06/20/cnn-mlp-2.html\n",
      "\n",
      "![6_image_0.png](6_image_0.png)\n",
      "\n",
      "![6_image_1.png](6_image_1.png)\n",
      "\n",
      "(b) Language Identification 0.0 1.0\n",
      "\n",
      "![6_image_2.png](6_image_2.png)\n",
      "\n",
      "Figure 1: Distance matrices using Silhouette discretization.\n",
      "\n",
      "Figure 2: Control experiments using norms.\n",
      "\n",
      "| Norm      | Minimum   | Maximum   | Mean   | Standard deviation   |\n",
      "|-----------|-----------|-----------|--------|----------------------|\n",
      "| 1-Norm    | 0.6683    | 4.9159    | 1.9733 | 1.5693               |\n",
      "| Frobenius | 0.0670    | 0.9886    | 0.4514 | 0.3074               |\n",
      "\n",
      "Table 2: Normalized difference comparison of self-norm against the maximum mean distance of the experiment.\n",
      "\n",
      "![7_image_0.png](7_image_0.png)\n",
      "\n",
      "Figure 3: Control experiment comparison matrix using Silhouette discretization.\n",
      "\n",
      "| Heat distance   |        | Silhouette distance   |        |           |\n",
      "|-----------------|--------|-----------------------|--------|-----------|\n",
      "| Dataset         | Mean   | Deviation             | Mean   | Deviation |\n",
      "| MNIST           | 0.0291 | 0.0100                | 0.1115 | 0.0364    |\n",
      "| F. MNIST        | 0.0308 | 0.0132                | 0.0824 | 0.0353    |\n",
      "| CIFAR-10        | 0.0243 | 0.0068                | 0.0769 | 0.0204    |\n",
      "| Language I.     | 0.0159 | 0.0040                | 0.0699 | 0.0159    |\n",
      "| Reuters         | 0.0166 | 0.0051                | 0.0387 | 0.0112    |\n",
      "\n",
      "Table 3: PH distances across input order (control) experiments, normalized by dataset.\n",
      "In contrast, Figure 3, with our method (Silhouette), shows perfect diagonal of similarity blocks. In the corresponding numeric results, we obtained small distances, as shown in Table 3. We can appreciate that each dataset has its own hub. This confirms the validity of our proposed similarity measure.\n",
      "\n",
      "The method we present also seems to capture some parts of hyperparameter setup. For instance, in Figure 4 we can observe gradual increase of distances in the first group regarding layer size meaning that, as layer size increases, the topological distance increases too. Similarly, for the number of layers\n",
      "(second group) and number of labels (fourth group) the same situation holds. Note that in Fashion MNIST and CIFAR-10, the distances are dimmer because we are not dealing with the weights of the CNNs. Recall that the CNN acts as a frozen extractor and are pretrained for all runs (with the same weights), such that the MLP layers themselves are the only potential source of dissimilarity between runs. Thus, our characterization is sensitive to the architecture (e.g., if we increase the capacity, distances vary), but at the same time, as we saw before, it is not dataset-agnostic, meaning that it also captures whether two neural networks are learning the same problem or not.\n",
      "\n",
      "In Figure 4, Fashion MNIST (Figure 4b) and CIFAR (Figure 4c) dataset results are interestingly different from those of MNIST (Figure 4a) dataset. This is, presumably, because both Fashion MNIST and CIFAR use a pretrained CNN for the problem. Thus, we must analyze the results taking into account this perspective. The first fully connected layer size is important as it can avoid a bottleneck from the previous CNN output. Some works in the literature show that adding multiple fully connected layers does not necessarily enhance the prediction capability of CNNs Basha et al.\n",
      "\n",
      "(2019), which is congruent with our results when adding fully connected layers (experiments 5 to 9)\n",
      "that result in dimmer matrices than the one from. Concerning the experiments on input order, there is slightly more homogeneity than in MNIST, again showing that the order of sample has negligible influence. Moreover, there could have been even more homogeneity taking into account that the fully connected network reduced its variance thanks to the frozen weights of the CNN. This also supports the fact that the CNN is the main feature extractor of the network. As in MNIST results, CIFAR results show that the topological properties are, indeed, a mapping of the practical properties of neural networks. We refer to the Supplementary Material for all distance matrices for all datasets and all distances, as well as for the standard deviations matrices and experiment group statistics.\n",
      "\n",
      "![8_image_0.png](8_image_0.png)\n",
      "\n",
      "Figure 4: Distance matrices using Heat discretization.\n",
      "\n",
      "## 5 Conclusions & Future Work\n",
      "\n",
      "1.0\n",
      "\n",
      "![8_image_1.png](8_image_1.png)\n",
      "\n",
      "0.0\n",
      "\n",
      "Results in five different datasets from computer vision and natural language lead to similar topological properties and are interpretable, which yields to general applicability. The best discretizations found are Heat and Silhouette. They show better separation of experiment groups, and are effectively reflecting changes in a sensitive way (unlike Landscape discretization).\n",
      "\n",
      "The most remarkable conclusion comes from the control experiments. The corresponding neural networks, with different input order but the same architecture, are very close to each other.\n",
      "\n",
      "The PH framework does, indeed, abstract away the specific weight values, and captures latent information from the networks, allowing comparisons to be based on the function they approximate. The selected neural network representation is reliable and complete, and yields coherent and meaningful results. Instead, the baseline measures, the 1-Norm and the Frobenius norm, implied an important dissimilarity between the experiments in the control experiments, meaning that they did not capture the fact that these neural networks were very similar in terms of the solved problem.\n",
      "\n",
      "Our proposed characterization does, indeed, capture meaningful properties. To the best of our knowledge, our proposed similarity measure between neural networks is the first of its kind. As future work, we suggest adapting the method to architectures such as CNNs, RNNs, and Transformers\n",
      "(Vaswani et al., 2017). Finally, we suggest performing more analysis regarding the learning of a neural network from a topological point of view.\n",
      "\n",
      "Figure 5: Language Identification (Landscape).\n",
      "\n",
      "## References\n",
      "\n",
      "H. Adams, Tegan Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, Sofya Chepushtanova, E. Hanson, F. Motta, and Lori Ziegelmeier. Persistence images: A stable vector representation of persistent homology. *J. Mach. Learn. Res.*, 18:8:1–8:35, 2017.\n",
      "\n",
      "M. Aktas, E. Akba¸s, and Ahmed El Fatmaoui. Persistence homology of networks: methods and applications. *Applied Network Science*, 4:1–28, 2019.\n",
      "\n",
      "Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer perceptrons by forward bipartite alignment. In *2015 International Joint Conference on Neural* Networks (IJCNN), pp. 1–7, 2015. doi: 10.1109/IJCNN.2015.7280769.\n",
      "\n",
      "S. H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, and Snehasis Mukherjee. Impact of fully connected layers on performance of convolutional neural networks for image classification.\n",
      "\n",
      "CoRR, abs/1902.02771, 2019. URL http://arxiv.org/abs/1902.02771.\n",
      "\n",
      "Ulrich Bauer. Ripser: efficient computation of vietoris-rips persistence barcodes, 2021.\n",
      "\n",
      "E. Berry, Yen-Chi Chen, J. Cisewski-Kehe, and Brittany Terese Fasy. Functional summaries of persistence diagrams. *Journal of Applied and Computational Topology*, 4:211–262, 2020.\n",
      "\n",
      "Peter Bubenik. Statistical topological data analysis using persistence landscapes. *J. Mach. Learn.*\n",
      "Res., 16:77–102, 2015.\n",
      "\n",
      "G. Carlsson. Topology and data. *Bulletin of the American Mathematical Society*, 46:255–308, 2009.\n",
      "\n",
      "F. Chazal, V. D. Silva, and S. Oudot. Persistence stability for geometric complexes. *Geometriae* Dedicata, 173:193–214, 2012.\n",
      "\n",
      "Samir Chowdhury, T. Gebhart, Steve Huntsman, and Matvey Yutin. Path homologies of deep feedforward networks. *2019 18th IEEE International Conference On Machine Learning And* Applications (ICMLA), pp. 1077–1082, 2019.\n",
      "\n",
      "J. Clough, I. Öksüz, Nicholas Byrne, V. Zimmer, J. A. Schnabel, and A. P. King. A topological loss function for deep-learning based image segmentation using persistent homology. IEEE transactions on pattern analysis and machine intelligence, PP, 2020.\n",
      "\n",
      "D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. *Proceedings of* the twenty-first annual symposium on Computational geometry, 2005.\n",
      "\n",
      "C. Corneanu, M. Madadi, S. Escalera, and A. Martínez. Computing the testing error without a testing set. *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.\n",
      "\n",
      "2674–2682, 2020.\n",
      "\n",
      "Jonathan Donier. Capacity allocation analysis of neural networks: A tool for principled architecture design. *ArXiv*, abs/1902.04485, 2019.\n",
      "\n",
      "H. Edelsbrunner and J. Harer. *Computational Topology - an Introduction*. American Mathematical Society, 2009.\n",
      "\n",
      "Brittany Fasy, Yu Qin, Brian Summa, and Carola Wenk. Comparing distance metrics on vectorized persistence summaries. In *NeurIPS 2020 Workshop on Topological Data Analysis and Beyond*,\n",
      "2020.\n",
      "\n",
      "T. Gebhart, Paul Schrater, and A. Hylton. Characterizing the shape of activation space in deep neural networks. 2019 18th IEEE International Conference On Machine Learning And Applications\n",
      "(ICMLA), pp. 1537–1542, 2019.\n",
      "\n",
      "R. Ghrist. *Elementary Applied Topology*. Self-published, 2014.\n",
      "\n",
      "P. Giblin. *Graphs, surfaces, and homology : an introduction to algebraic topology*. Chapman and Hall, 1977.\n",
      "\n",
      "William H. Guss and R. Salakhutdinov. On characterizing the capacity of neural networks using algebraic topology. *ArXiv*, abs/1802.04443, 2018.\n",
      "\n",
      "C. Hofer, Florian Graf, M. Niethammer, and R. Kwitt. Topologically densified distributions. *ArXiv*,\n",
      "abs/2002.04805, 2020.\n",
      "\n",
      "J. Jonsson. *Simplicial complexes of graphs*. PhD thesis, KTH Royal Institute of Technology, 2007.\n",
      "\n",
      "Emir Konuk and K. Smith. An empirical study of the relation between network architecture and complexity. *2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)*,\n",
      "pp. 4597–4599, 2019.\n",
      "\n",
      "Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. *CoRR*, abs/1905.00414, 2019. URL http://arxiv.org/ abs/1905.00414.\n",
      "\n",
      "P. Lawson, A. Sholl, J. Brown, Brittany Terese Fasy, and C. Wenk. Persistent homology for the quantitative evaluation of architectural features in prostate cancer histology. *Scientific Reports*, 9, 2019.\n",
      "\n",
      "Daniel Lütgehetmann, Dejan Govc, Jason Smith, and R. Levi. Computing persistent homology of directed flag complexes. *arXiv: Algebraic Topology*, 2019.\n",
      "\n",
      "Gregory Naitzat, A. Zhitnikov, and L. Lim. Topology of deep neural networks. *J. Mach. Learn. Res.*,\n",
      "21:184:1–184:40, 2020.\n",
      "\n",
      "K. Ramamurthy, Kush R. Varshney, and Krishnan Mody. Topological data analysis of decision boundaries with application to model selection. *ArXiv*, abs/1805.09949, 2019.\n",
      "\n",
      "Bastian Alexander Rieck, F. Sadlo, and H. Leitte. Topological machine learning with persistence indicator functions. *ArXiv*, abs/1907.13496, 2019a.\n",
      "\n",
      "Bastian Alexander Rieck, Matteo Togninalli, C. Bock, Michael Moor, Max Horn, Thomas Gumbsch, and K. Borgwardt. Neural persistence: A complexity measure for deep neural networks using algebraic topology. *ArXiv*, abs/1812.09764, 2019b.\n",
      "\n",
      "Guillaume Tauzin, Umberto Lupo, Lewis Tunstall, Julian Burella Pérez, Matteo Caorsi, Anibal Medina-Mardones, Alberto Dassatti, and Kathryn Hess. giotto-tda: A topological data analysis toolkit for machine learning and data exploration, 2020.\n",
      "\n",
      "Christopher Tralie, Nathaniel Saul, and Rann Bar-On. Ripser.py: A lean persistent homology library for python. *The Journal of Open Source Software*, 3(29):925, Sep 2018. doi: 10.21105/joss.00925.\n",
      "\n",
      "URL https://doi.org/10.21105/joss.00925.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. *CoRR*, abs/1706.03762, 2017. URL\n",
      "http://arxiv.org/abs/1706.03762.\n",
      "\n",
      "Joshua T. Vogelstein, John M. Conroy, Vince Lyzinski, Louis J. Podrazik, Steven G. Kratzer, Eric T.\n",
      "\n",
      "Harley, Donniell E. Fishkind, R. Jacob Vogelstein, and Carey E. Priebe. Fast approximate quadratic programming for graph matching. *PLOS ONE*, 10(4):1–17, 04 2015. doi: 10.1371/journal.pone.\n",
      "\n",
      "0121002. URL https://doi.org/10.1371/journal.pone.0121002.\n",
      "\n",
      "S. Watanabe and Hayato Yamana. Topological measurement of deep neural networks using persistent homology. In *ISAIM*, 2020.\n",
      "\n",
      "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n",
      "\n",
      "S. Zhang, Mengbai Xiao, and H. Wang. Gpu-accelerated computation of vietoris-rips persistence barcodes. In *Symposium on Computational Geometry*, 2020.\n",
      "Instruction: You are a decision maker. Please review all responses from the author and comments from all reviewers to provide the meta-review and determine the final decision. Explicitly state 'Accept' or 'Reject' at the end of your output.\n",
      "Input: Title: Retrieval-Based Video Language Model for Efficient Long Video Question Answering. Abstract: The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video question answering (VQA) tasks, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens can result in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the VQA process.\n",
      "To address these issues, we introduce a simple yet effective retrieval-based video language model (R-VLM) for efficient and interpretable long video QA. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant $K$ video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance.\n",
      "Our experimental results validate the effectiveness of our framework for comprehending long videos. Furthermore, based on the retrieved chunks, our model is interpretable that provides the justifications on where we get the answers. Main Text: introduces noise to the video QA process. To address these issues, we introduce a simple yet effective retrieval-based video language model (R-VLM) for efficient and interpretable long video QA. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant K video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. Our experimental results validate the effectiveness of our framework for comprehending long videos. Furthermore, based on the retrieved chunks, our model is interpretable that provides the justifications on where we get the answers.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "With the rapid development of the Internet and the widespread use of cameras and smartphones, both individuals and businesses are generating massive amounts of video data every day in various fields such as entertainment, education, and technology. In such era of information explosion, understanding and extracting information from video content has become increasingly important to better meet people's needs and promote social progress. In this context, Video Question Answering (Video QA) emerges as an essential interface for extracting and delivering information from video content. Video QA systems allow users to ask natural language questions about videos and receive answers based on the visual (and auditory) information within the video. There is a growing trend towards leveraging large language models (LLMs) for video QA (Maaz et al., 2023; Wang et al., 2023; Li et al., 2023b; Zhang et al., 2023b). On one hand, LLMs benefit from the vast knowledge acquired through training on enormous text corpora; on the other hand, they provide users with a more natural and intuitive way to interact with video data. Generally, vision tokens extracted from a video snippet are transformed and used as input (prompt) to the LLM, along with the text query. However, it is worth noting that the consumption of abundant vision tokens by a LLM can significantly increase the memory and computational burden, making it unaffordable for low-resource GPU agents. To mitigate this issue, Maaz et al. (2023) perform global spatial and temporal pooling on the video tokens, although this comes at the cost of losing detail due to pooling. Zhang et al. (2023b) aggregate video tokens using a Q-former with cross attention. Most of these methods are designed primarily for short-video QA tasks, where answer-related frames usually spread over the trimmed video snippet. In practical application scenarios, users often pose flexible questions related to long videos (e.g., longer than 1 minute), in which the segments containing pertinent information for answering the questions constitute merely a small fraction of the entire video. The presence of answer-irrelevant tokens are redundancy and may potentially interfere with the video QA process, diminishing its\n",
      "\n",
      "![1_image_0.png](1_image_0.png)\n",
      "\n",
      "Figure 1: Illustration of our proposed retrieval-based video language model for efficient long video question answering. We encode an input long video into a sequence of video chunks, with each chunk represented by a set of spatial and temporal visual tokens. Question-guided retrieval is performed to find the top K relevant video chunks, with their tokens as the context/prompt of the LLM\n",
      "for answer generation. Soft matching (SM) loss is used to regularize the retrieval related learning.\n",
      "effectiveness. Therefore, it is imperative to develop a simple yet efficient framework that can handle long video QA tasks.\n",
      "\n",
      "To address these challenges, we draw inspiration from biology and cognitive science. As we know, human working memory is a cognitive system responsible for temporarily holding and manipulating information necessary for complex tasks such as reasoning, learning, and comprehension (Thompson & Madigan, 2013). Faced with the vast amount of information stored in long-term memory, working memory selectively retrieves relevant information while filtering out irrelevant data for further cognition. Motivated by this, we aim to design a framework that is capable of identifying and concentrating on relevant video segments while filtering out irrelevant information, ensuring accurate and efficient question answering without imposing excessive computational demands.\n",
      "\n",
      "In this paper, we propose a retrieval-based video-language model, R-VLM, for efficient and interpretable long-video question answering. Fig. 1 shows the overall framework. Specifically, given a long video and a question (query), we divide the video into a sequence of non-overlapping video chunks, with each chunk representing a short video segment, e.g., 4 seconds with a sample rate of 1fps. Note that we sample at a low frame rate in considering the memory limitation and video redundancy. To allow a chunk to contain dynamic temporal information, we use 4 seconds that are expected to contain temporal dynamics as the chunk unit. We then aggregate the encoded tokens within a chunk through spatial and temporal pooling to obtain chunk tokens, reducing some redundancy while preserving considerable local details. We perform question-guided retrieval to obtain the top K most relevant chunks, and subsequently use the small set of tokens (after projection) from these chunks as input for the frozen LLM for video QA inference. In this way, we efficiently match and pass the most question-pertinent visual information to the LLM for inference.\n",
      "\n",
      "Our framework demonstrates superior zero-shot generalization performance on several video QA datasets, outperforming the baseline method Video-ChatGPT (Maaz et al., 2023) by 6.8%, 2.8%,\n",
      "4.8%, 6.0% in accuracy on the WildQA, QaEgo4D, lifeQA, and Social-IQ 2.0 dataset, respectively.\n",
      "\n",
      "Notably, the retrieved video chunks provide justification for the model's responses, offering interpretability to the video language model on where matters for answering the question.\n",
      "\n",
      "Our contributions can be summarized below:\n",
      "- We propose a retrieval-based video language model for efficient long video understanding. To the best of our knowledge, we are the first time to validate the feasibility of using retrieval for long video question answering with large video-language model.\n",
      "\n",
      "- Thanks to the retrieval-based context/prompt chunk selection mechanism, our model is more interpretable, where the selected chunks provide insight on based on where and why the model generates the answer.\n",
      "\n",
      "- Our method achieves significant performance improvement over the baseline method. The ablation studies demonstrate the superiority of our retrieval mechanism.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "Large Language Models. LLMs (Radford et al., 2018; Brown et al., 2020; Ouyang et al., 2022) have experienced significant advances and demonstrated remarkable capabilities such as language understanding, reasoning, generation, and in context learning. These abilities enable LLMs to tackle diverse tasks with user prompts in a zero-shot fashion, reflecting their powerful adaptability and generalization. The Instruction tuning strategy as used in ChatGPT (OpenAI, 2023) improves the model's alignment with user intentions and optimizes the quality of generation. At the same time, many open-source instruction tuned models have emerged, such as LLaMA (Touvron et al., 2023),\n",
      "OPT (Zhang et al., 2022) and GML (Zeng et al., 2022), which have greatly promoted technological advancement and made significant contributions to the community. Vision-Language Models with Pre-trained LLMs. Recent advances in LLMs have inspired and promoted the integration of pre-trained LLMs with visual processing components for multimodal understanding (Alayrac et al., 2022; Li et al., 2023a; Zhu et al., 2023; Liu et al., 2023; Huang et al.,\n",
      "2023). Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023a) are seminal works that leverage the pre-trained LLM and CLIP image encoder for zero-shot image-text abilities. BLIP2 (Li et al.,\n",
      "2023a) introduces a Q-Former to map the image tokens from the CLIP encoder (Radford et al.,\n",
      "2021) to the textual embedding space of LLMs. LLaVA (Liu et al., 2023) maps the image spatial tokens to the textual embedding space of LLMs using a linear projector. LLaVA (Liu et al., 2023), MiniGPT4 (Zhu et al., 2023), and mPLUG-owl (Ye et al., 2023) promote instruction following using image-instruction-following datasets. Some works have focused on enabling video language understanding by integrating vision encoder and LLMs (Zhang et al., 2023a; Maaz et al., 2023; Li et al., 2023b; Wang et al., 2023). VideoLLaMA (Zhang et al., 2023a) uses a video Q-former to assemble the features from the pre-trained CLIP image encoder. Video-ChatGPT (Maaz et al., 2023) performs spatial and temporal pooling of the feature maps encoded by the pre-trained CLIP image encoder. The ensembled vision tokens are taken as input to the LLMs. Most of these works globally aggregate video tokens. They may work well for short videos. But for the long video language comprehension, they would be less efficient. The information of the question related video segments may be submerged to the globalwise token representations, making the reasoning difficult. We aim to design a simple yet efficient video language model for long video understanding. Chunk-wise representations and learnable retrieval are introduce to address the challenges.\n",
      "\n",
      "## 3 Proposed Retrieval-Based Video Language Model\n",
      "\n",
      "Given a lengthy video, it is time-consuming to watch the entire content to obtain the desired information. A Video QA system, which can automatically infer answers based on the long video and user's query (question), is in high demand. LLMs possess extensive world knowledge and reasoning capabilities, albeit at the expense of high computational complexity. Some previous works on multi-modal language models utilize a set of projected vision tokens as input (context) to LLM for inference (Li et al., 2023a; Zhu et al., 2023), where the inference cost is proportional to the number of input vision tokens.\n",
      "\n",
      "Leveraging powerful LLMs to understand long videos presents challenges. Firstly, we expect to use only a small set of tokens as input to reduce computational costs. Secondly, as the video length increases, in general, there is a corresponding growth in the number of vision tokens and the overall amount of information. Representing a long video with very few tokens becomes difficult. Furthermore, question-irrelevant information may interfere with answer generation. Motivated by the retrieval mechanism of brain, we introduce a question-guided retrieval mechanism to identify and select a few relevant video chunks as context of LLM.\n",
      "\n",
      "Fig. 1 illustrates the overall framework of our proposed retrieval-based video language model (R -VLM), designed for efficient long video question answering. The framework comprises several components: a frozen LLM, a frozen image encoder, a frozen text encoder, a memory for storing video chunk tokens, a retrieval module for selecting the question-relevant chunks, a learnable MLP\n",
      "block, and a projector. Through end-to-end training with cross-entropy loss and the proposed soft matching (SM) loss, the MLP block is optimized to learn to identify the most relevant K chunks, while the projector is optimized to align the selected vision tokens with the text (question) space. In the following subsections, we provide a detailed explanation of the key components and designs.\n",
      "\n",
      "## 3.1 Video Tokenization\n",
      "\n",
      "Video tokenization involves encoding the raw video into vision tokens, which will be processed and then passed to the LLM for inference.\n",
      "\n",
      "Chunking the video. Given a long video sample Vi ∈ R\n",
      "Ti×H×W×C with Ti frames, C channels, height H, and width W 1, we divide it into small non-overlapped video chunks (see Fig. 1, which are the basic units for question-guided retrieval. We set the duration of a chunk as 4 seconds, i.e.,\n",
      "M = 4 frames when frame rate is 1fps. We have Li = ⌈Ti/M⌉ chunks, where ⌈·⌉ denotes the ceiling function.\n",
      "\n",
      "Vision token extraction of a chunk. The vision tokens of a chunk are obtained by encoding the images and performing spatial-temporal pooling. Following Alayrac et al. (2022) and Maaz et al. (2023), we adapt the pretrained language-image model CLIP (Radford et al., 2021) to extract per frame vision token features, which are suited for capturing high-level semantic information. For the j th video chunk V\n",
      "j i ∈ RM×H×W×C , we obtain M × h × w vision tokens extracted by the CLIP\n",
      "vision encoder, where h = H/p, w = W/p. p denotes the patch size (which is 14 for ViT-L/14).\n",
      "\n",
      "The preliminary token number of a chunk is large, i.e., M×h×w = 4×16×16 = 1024. This would result in a high computational burden and large memory requirement for the LLM even though we only select a few chunks as input to LLM. We found that reducing the spatial resolution by a factor of 4 leads to marginal difference in performance while this can significantly reduce the number of tokens by 75%. Therefore, we perform spatial average pooling with stride 2 to have M × h¯ × w¯ =\n",
      "4 × 8 × 8 = 256 tokens per chunk, where h¯=h/2 and w¯=w/2. This is equivalent to that we take the CLIP features of reduced resolution as the extracted feature. How to further reduce the number of tokens while preserving spatial-temporal features of a chunk? Motivated by Maaz et al. (2023), we perform spatial-temporal pooling on the token tensor. Particularly, global spatial average pooling for each frame is performed and thus we obtain M tokens for the M frames (e.g., 4 tokens). Temporal pooling for each spatial position is performed to have N= h¯ × w¯ = 8 × 8 = 64 tokens. We have N + M = 64 + 4 = 68 tokens F\n",
      "j i ∈ R\n",
      "(N+M)×D, with D dimension for each token. Compared with the original 1024 in a chunk, the number of tokens has been reduced to 6.6%, which brings high calculation efficiency for the later LLM inference.\n",
      "\n",
      "In contrast to Maaz et al. (2023), which performs global spatial and temporal pooling over the entire video, ours with chunks is capable of preserving local details and is better suited for long video QA.\n",
      "\n",
      "## 3.2 Question-Guided Retrieval For Chunk Selection\n",
      "\n",
      "For a long video Vi with abundant video chunks, we retrieve and select the K most relevant chunks based on the question/query and use them as input to the LLM. The top K retrieval aims to efficiently identify the most informative video segments for answering the given question, reducing the memory and computational burden to LLM and excluding the interference from irrelevant content.\n",
      "\n",
      "We encode the question Qiinto a feature vector qi ∈ R\n",
      "D using the frozen CLIP text encoder fθ and a learnable MLP block ψ as qi = ψ(fθ(Qi)), (1)\n",
      "where the MLP block is a two-layer perceptron consisting of two fully connected layers and an ReLU activation function in between. This MLP transformation strengthens the correlation between the question representation and the potential corresponding chunk features for chunk selection. Achieving this is non-trivial, as we do not have any ground-truth locations of the question relevant chunks for supervision.\n",
      "\n",
      "1Note that in considering the abundant redundancy in video, the video is already temporal down sampled to have the frame rate of one frame per second.\n",
      "\n",
      "To identify the question-related chunks, we measure the affinity between the text feature vector and the chunk representation. For simplicity, we obtain the representation feature of the j th chunk for the matching by aggregating its N + M vision tokens by averaging pooling as v j i = *Avgpool*(F\n",
      "j i\n",
      ").\n",
      "\n",
      "This is parameter-free, avoiding the need for large memory in optimization. We store the chunk representation features and chunk tokens in a memory to facilitate retrieval.\n",
      "\n",
      "We compute the similarity scores between the question representation qi and each video chunk representation v j i using cosine similarity metric as\n",
      "\n",
      "$$s_{i}^{j}=\\cos({\\bf q}_{i},{\\bf v}_{i}^{j})=\\frac{{\\bf q}_{i}\\cdot{\\bf v}_{i}^{j}}{||{\\bf q}_{i}||||{\\bf v}_{i}^{j}||}.\\tag{1}$$\n",
      "$$(2)$$\n",
      "\n",
      "We rank the video chunks based on their similarity scores s j i\n",
      "(where j = 1, · · · , Li) and select the top K most relevant chunks. The K ×(N +M) vision tokens of these chunks are input to the LLM\n",
      "after a linear projection (a fully connected layer) on each token.\n",
      "\n",
      "## 3.3 End-To-End Optimization\n",
      "\n",
      "We train the network using an end-to-end optimization approach with the video instruction data.\n",
      "\n",
      "The image encoder, text encoder, and the LLM are frozen during the training. Only the parameters of the MLP block and projector are optimized. For a video-text pair, besides the cross-entropy loss between the generated prediction and groudtruth answer L\n",
      "pred i, we introduce soft matching (SM)\n",
      "loss L\n",
      "SM\n",
      "ito regularize the similarity learning as\n",
      "\n",
      "$${\\mathcal{L}}_{i}^{\\mathrm{SM}}=-\\cos(\\mathbf{q}_{i},{\\bar{\\mathbf{v}}}_{i}),\\quad{\\mathrm{where~}}{\\bar{\\mathbf{v}}}_{i}={\\frac{\\sum_{j=1}^{L_{i}}e^{s_{i}^{j}}\\mathbf{v}_{i}^{j}}{\\sum_{j=1}^{L_{i}}e^{s_{i}^{j}}}}.$$\n",
      ". (3)\n",
      "Here v¯iis a weighted combination of the Li chunk features, with weights determined by the similarities between the query and chunk features (see Eq.(2)). In order to maximize the cosine similarity score between query and the combined feature, the MLP block needs to be optimized to result in higher similarity to those question relevant chunks. Better optimized similarity scores lead to better selection of chunks to the LLM and thus superior performance.\n",
      "\n",
      "The overall loss is as\n",
      "\n",
      "$$({\\mathfrak{I}})$$\n",
      "$${\\mathcal{L}}_{i}={\\mathcal{L}}_{i}^{p r e d}+\\lambda{\\mathcal{L}}_{i}^{\\mathrm{SM}},$$\n",
      "$$(4)$$\n",
      "i, (4)\n",
      "where λ is a hyper-parameter that balances the contribution of the regularization term. We determine the value of λ such that both loss terms are on the same order of magnitude. We set λ = 10 in our experiments.\n",
      "\n",
      "## 4 Experiments 4.1 Implementation Details\n",
      "\n",
      "Following Video-ChatGPT (Maaz et al., 2023), we use the Language-aligned Large Vision Assistant\n",
      "(LLaVA) (Liu et al., 2023) as our base model. We utilize the pre-trained CLIP ViT-L/14 (Radford et al., 2021) as our image encoder and extract the feature from the second-to-last layer as the h × w vision tokens of a frame. We use the fine-tuned Vicuna (7B) from LLaVA as our LLM. For the text encoder, we use the pre-trained CLIP ViT-L/14 text encoder and extract the class token feature of the penultimate layer. The number of neurons for the two fully connected layers of the MLP block is 1024 and 1024, respectively. The number of neurons for the projector is 4096.\n",
      "\n",
      "We only fine-tune the MLP block and the projector while keeping the image encoder, the text encoder and the LLM frozen. We fine-tune the model for 3 epochs using video instruction data, with a learning rate of 2e-5 and a batch size of 40. Training our model takes about 24h on an A100 80GB\n",
      "GPU. We set K to 5, resulting in 5 × (64 + 4) = 340 vision tokens as the input to the LLM. This is comparable to the number of vision tokens used in Video-ChatGPT (356 vision tokens).\n",
      "\n",
      "| Dataset       | #Vid   | #QA   | Duration (sec.)   |\n",
      "|---------------|--------|-------|-------------------|\n",
      "| WildQA        | 261    | 652   | 71.2              |\n",
      "| QaEgo4D       | 166    | 1854  | 495.1             |\n",
      "| lifeQA        | 49     | 372   | 74.0              |\n",
      "| Social-IQ 2.0 | 144    | 876   | 60.0              |\n",
      "\n",
      "Table 1: Information of the evaluation datasets, including the number of videos, the number of QA\n",
      "pairs, and the average video duration.\n",
      "\n",
      "## 4.2 Datasets\n",
      "\n",
      "We use Video Instruction Data collected by Maaz et al. (2023) for video instruction tuning. This dataset contains about 100k question and answer pairs based on the Activitynet dataset (average duration 180 seconds). The 100k QA pairs include various types of questions, including but not limited to describing the general content of the video, questions related to appearance, movement, trajectory, reasoning, and some tasks that require imagination.\n",
      "\n",
      "We evaluate the generalization performance of our framework on four video datasets: WildQA\n",
      "(Castro et al., 2022), QaEgo4D (Barmann & Waibel, 2022), lifeQA (Castro et al., 2020), and Social- ¨\n",
      "IQ 2.0 (Wilf et al., 2023). The average duration of these datasets is larger than one minute. The average duration of QaEgo4D is more than eight minutes. Please see Table 1 for specific information about each dataset. Note that long video datasets are rare. Many popular video QA datasets are not suited for our study, where the average duration of videos are very short, e.g., 15 seconds for MSRVTT-QA (Xu et al., 2017), 10 seconds for MSVD-QA (Xu et al., 2017), and 3 seconds for TGIF-QA (Jang et al., 2019). The collection and annotation of long video dataset is highly desired.\n",
      "\n",
      "We hope the research and industry committee work together to contribute large datasets of long videos to inspire more investigation in future.\n",
      "\n",
      "## 4.3 Evaluation Metrics\n",
      "\n",
      "In this paper, we follow the metrics of accuracy and average score as proposed by Maaz et al. (2023) for performance evaluation, where we use ChatGPT (chatgpt35-turbo) to assist in judging the correctness of model predictions. ChatGPT accepts questions, groundtruth answers, and the model predictions as input. For each question answer pair, ChatGPT gives a binary judgment of\n",
      "\"yes\" or \"no\" to identify whether the predicted answer is correct or not, for accuracy evaluation.\n",
      "\n",
      "Moreover, a score of 0-5 is also given by ChatGPT to indicate how similar the prediction is to the answer. 0 represents the lowest score and 5 represents the highest score.\n",
      "\n",
      "## 4.4 Comparison With Other Video Language Models\n",
      "\n",
      "| Model                             | WildQA     | QaEgo4D    | lifeQA     | Social-IQ 2.0   |\n",
      "|-----------------------------------|------------|------------|------------|-----------------|\n",
      "| Video-LLaMA (Zhang et al., 2023a) | 63.19/3.18 | 35.35/1.94 | 35.75/2.32 | 55.78/2.90      |\n",
      "| Video-ChatGPT (Maaz et al., 2023) | 58.00/3.30 | 29.74/2.43 | 33.87/2.55 | 57.73/3.26      |\n",
      "| R-VLM (Ours)                      | 64.82/3.39 | 32.51/2.45 | 38.71/2.61 | 63.65/3.40      |\n",
      "\n",
      "Table 2: Comparison with the other video-language models, including Video-LLaMA and the baseline method Video-ChatGPT. We report the accuracy (%)/ average score.\n",
      "\n",
      "We compare our final scheme *R-VLM* (Retrieval based video language model) with the baseline method *Video-ChatGPT* (Maaz et al., 2023) on four unseen datasets. We use the same training dataset as *Video-ChatGPT*. Both *Video-ChatGPT* and our *R-VLM* are built based on *LLaVA*, with similar model size. Table 2 shows the results. Our *R-VLM* outperforms *Video-ChatGPT* significantly by 6.8%, 2.8%, 4.8%, 6.0% in accuracy on the WildQA, QaEgo4D, lifeQA, and Social-IQ\n",
      "2.0 dataset, respectively. Ours consistently achieves higher average score. Note that the number of vision tokens of our *R-VLM* is comparable to that of *Video-ChatGPT* (i.e., 340 vs. 356), making a fair comparison. This demonstrates the effectiveness of our retrieval-based design. Our chunk-wise retrieval design facilitates the exploration of the most informative vision tokens and preservation of necessary vision details for QA.\n",
      "\n",
      "We also evaluate the performance of other video language model, e.g., *Video-LLaMA* (Zhang et al., 2023a), by testing their model directly. As shown in Table 2, our *R-VLM* achieves the best scores on all the four datasets. We found *Video-LLaMA* is prone to give detailed description of the entire video, whereas the answer is usually question-irrelevant. Fig. 2(a) shows some typical examples on the QaEgo4D dataset, where the answer includes much information while the desired answer is submerge in the overall answer. The accuracy metric is not perfect and trades such answer as correct.\n",
      "\n",
      "In contrast, our model provides more concise and accurate answers.\n",
      "\n",
      "## 4.5 Ablation Studies\n",
      "\n",
      "Table 3: Comparison of different methods and chunk selection strategies. All these models are trained using the same video instruction data. *R-VLM* denotes our final scheme with learnable retrieval. *R-VLM w/ Uni.* denotes uniform sampling of K chunks in our framework instead of retrieval-based sampling. *R-VLM w/ CLIP M.* denotes that we use the final CLIP class token feature of vision and text for matching in our framework, without learnable parameters for retrieval. We report the accuracy (%) / average score.\n",
      "\n",
      "| Dataset       | Video-ChatGPT   | R-VLM w/ Uni.   | R-VLM w/ CLIP M.   | R-VLM      |\n",
      "|---------------|-----------------|-----------------|--------------------|------------|\n",
      "| WildQA        | 58.00/3.30      | 61.23/3.36      | 60.31/3.27         | 64.82/3.39 |\n",
      "| QaEgo4D       | 29.74/2.43      | 31.57/2.44      | 31.52/2.43         | 32.51/2.45 |\n",
      "| lifeQA        | 33.87/2.55      | 36.56/2.56      | 31.45/2.42         | 38.71/2.61 |\n",
      "| Social-IQ 2.0 | 57.73/3.26      | 57.96/3.24      | 61.17/3.28         | 63.65/3.40 |\n",
      "\n",
      "In this section, we study and validate the effectiveness of our retrieval designs, chunk-wise design and the soft matching loss, respectively.\n",
      "\n",
      "Effectiveness of Retrieval for Chunk Selection. Under our framework, we compare our retrieval mechanism and the uniform sampling strategy for the selection of K=5 chunks as context input to the LLM. For the uniform sampling setting which we name as *R-VLM w/ Uni.*, the model selects K from N video chunks uniformly instead of based on question guided retrieval. Table 3 shows that our final scheme *R-VLM* with learnable retrieval-based strategy outperforms *R-VLM w/ Uni.* by 3.6%, 0.9%, 2.2%, and 5.7% on WildQA, QaEgo4D, lifeQA, and Social-IQ 2.0, respectively. For a long video, the video chunks relevant to the question usually account for a small portion of the entire video. It is difficult to hit these question-related chunks with uniform sampling. A large language model may can not correctly answer a question when it accepts video chunks not correlated with the question. In contrast, our learnable retrieval learns to select the chunks most relevant to the question, providing more reliable and less redundancy information to the LLM for effective inference. Fig. 2 which visualizes the selected chunks also validate this. Our retrieval based model is interpretable, providing where the model is based on to get the answer. Learnable Retrieval vs. Off-the-shelf CLIP based Retrieval. The CLIP image encoder and text encoder are pre-trained to achieve vision and text caption alignment through contrastive learning over the last layer class token features (Radford et al., 2021). One may wonder how about the performance when we use the CLIP class token features for question and chunk feature matching under our framework. To answer this question, we design a scheme *R-VLM w/ CLIP M.* for comparison.\n",
      "\n",
      "For a chunk, we use the averaged class token feature of the last layer of the CLIP image encoder\n",
      "(CLIP ViT-L/14) as the vision chunk feature, and the class token feature of the last layer of the CLIP\n",
      "text encoder as the question feature for matching. There is no learnable parameters for the retrieval.\n",
      "\n",
      "From Table 3, we can see that our *R-VLM* with learnable retrieval consistently outperforms R-VLM\n",
      "w/ CLIP M., upto 7.3% in accuracy on the lifeQA dataset. That may because the CLIP matching is originally designed for image and caption matching, which is not robust to the image and question matching. Thanks to the adaptation of the text encoder through a learnable MLP block, our question guided retrieval can better identify the relevant video chunks.\n",
      "\n",
      "Effectiveness of Chunk-wise Design. *Video-ChatGPT* performs video level spatial temporal pooling to obtain 356 vision tokens. Such global pooling would result in loss of details, especially when the question related video segments take a small portion of the entire video. In our design, we perform chunk level spatial temporal pooling to preserve more information about each chunk. Table 3 shows that when we uniform sample the chunks to have 340 vision tokens, *R-VLM w/Uni* obviously outperforms *Video-ChatGPT*, demonstrating the effectiveness of our chunk-wise design.\n",
      "\n",
      "| Model        | WildQA     | QaEgo4D    | lifeQA     | Social-IQ 2.0   |\n",
      "|--------------|------------|------------|------------|-----------------|\n",
      "| R-VLM w/o SM | 59.94/3.28 | 31.12/2.36 | 36.29/2.47 | 57.22/3.17      |\n",
      "| R-VLM        | 64.82/3.39 | 32.51/2.43 | 38.71/2.61 | 63.65/3.40      |\n",
      "\n",
      "Table 4: Influence of soft matching (SM) loss, evaluated by the accuracy (%) / average score.\n",
      "\n",
      "Influence of Soft Matching (SM) Loss. In order to regularize the learning of the MLP block for better retrieval, we introduce SM loss. Table 4 shows the comparison of our framework without the SM loss (*R-VLM w/o SM*) and that with SM loss (*R-VLM*). We can see that incorporating the SM loss significantly improves the performance. Our final scheme with the SM loss i.e., *R-VLM* outperforms that without SM Loss. SM Loss uses the cosine similarity between text embedding and video tokens as weight to re-weight the video tokens to obtain new video tokens. Then maximize the similarity between the new video tokens and the text embedding. This facilitates our learnable retrieval layer to find the video clips most similar to the questions.\n",
      "\n",
      "Influence of the Hyperparameter K. In general, when K is too small, it may lead to a loss of information necessary to answer the question. When K is too large, interference may be introduced, confusing the LLM. We found K=5 presents a good trade-off on most datasets and set K as 5. More details can be found in our Appendix. We leave the adaptive design of K as the future work.\n",
      "\n",
      "## 4.6 Visualization Analysis\n",
      "\n",
      "We visualize two examples from the QAEgo4D and WildQA datasets in Fig. 2 , with the following information. 1) The first row shows the video chunk samples by uniformly selecting 5 video chunks.\n",
      "\n",
      "2) The second row shows the retrieved 5 chunks (ordered by time order) in our *R-VLM*. We mark the groudtruth chunks by red box. 3) We show the learned similarity score (see Eq. (2)) curve based on which the top K chunks are selected. The horizontal axis represents the identity of chunk and the vertical axis denotes the similarity score of that chunk. The groundtruth chunks and our retrieved chunks are also marked. 4) The question and answers from different models: *R-VLM*,\n",
      "R-VLM w/Uni., *Video-ChatGPT*, and *Video-LLaMA*, respectively.\n",
      "\n",
      "For the QAEgo4D dataset, the average duration is about 8 minutes (120 video chunks), whereas the duration of the groundtruth segments is located 2% of the total video duration in average. It is difficult to hit groundtruth video segments by uniformly sampling K(= 5) chunks among 120 video chunks. In Fig. 2a, we can see that the fragments selected by uniform sampling are in general irrelevant to the problem, which is not conducive to the LLM reasoning. Our learnable retrieval can accurately find the segments where the answer lies in. Feeding the correct chunks to the LLM makes it possible to obtain the correct answer to the question. For the WildQA dataset, groundtruth segments usually locate at different locations in the video and last for a period of time. Although uniform sampling sometimes hits a certain groundtruth segments, there is no guarantee. In contrast, our learned retrieval can correctly hit the segments where the groundtruth segments are located on. In this way, the LLM can better understand the video and answer some detailed questions (refer to \"vegetation types\" in Fig. 2b).\n",
      "\n",
      "The predicted answers of our *R-VLM* is more accurate than *Video-ChatGPT*, and *Video-LLaMA*.\n",
      "\n",
      "More visualization can be found in our Supplementary.\n",
      "\n",
      "Besides the successful cases, we present some failed cases in our Appendix (see Fig.5). There are two main cases of failure. One is that the retrieval did not select the correct video chunks. The other is that the retrieval correctly identified the correct video chunks, but the answer was wrong. For the later cases, we think more powerful vision feature extractor and LLMs would alleviate the problem.\n",
      "\n",
      "## 5 Conclusion\n",
      "\n",
      "The comprehension of long videos using LLMs remains an under-explored area. There are two main challenges associated with comprehending long videos. 1) Long videos generally lead to abundant vision tokens, which increase computational cost for LLM inference. 2) Global aggregation of vision tokens inevitably results in the loss of vision details especially when the question relevant video chunks take only a small portion of the entire video. Moreover, question irrelevant chunks introduce interference. In this work, we address these issues by introducing a simple yet effective retrievalbased video language model (R-VLM) for long-video QA. Specifically, given a question (query) and a long video, our model identifies and selects the most question-relevant K video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, preserves the most informative information, eliminates noise interference, and thus enhances system performance. Our experimental results demonstrate the effectiveness of our designs for comprehending long videos.\n",
      "\n",
      "![8_image_0.png](8_image_0.png)\n",
      "\n",
      "![8_image_1.png](8_image_1.png)\n",
      "\n",
      "(b)\n",
      "Figure 2: Visualization of video QA examples from (a) QAEgo4D and (b) WildQA. (a)The kitchen towel in question does not appear in the uniformly sampled video chunks. The second chunks selected by our model contain kitchen towel. Our answer states that the tower is hunging on a hook on the wall. Video-LLaMA answers incorrectly, where the towel does not appear in the first frame of the video, and it is not be placed on the countertop in front of a cutting board. (b)In this video, two clips show vegetation and the remaining clips show mountains, rivers, etc. Uniform sampling mainly obtains segments such as mountains and rivers rather than segments with vegetation. Therefore, only the terrain was answered, without giving vegetation types. In contrast, our retrieved chunks contain video clips of vegetation. Thus the types of vegetation are predicted correctly: trees, bushes, forest.\n",
      "\n",
      "Video-ChatGPT gives a global description and does not answer specific vegetation types.\n",
      "\n",
      "## References\n",
      "\n",
      "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*, 35:23716–\n",
      "23736, 2022.\n",
      "\n",
      "Leonard Barmann and Alex Waibel. Where did i leave my keys? - episodic-memory-based question ¨\n",
      "answering on egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 1560–1568, June 2022.\n",
      "\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901, 2020.\n",
      "\n",
      "Santiago Castro, Mahmoud Azab, Jonathan Stroud, Cristina Noujaim, Ruoyao Wang, Jia Deng, and Rada Mihalcea. LifeQA: A real-life dataset for video question answering. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 4352–4358, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https:\n",
      "//aclanthology.org/2020.lrec-1.536.\n",
      "\n",
      "Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai G. Burzo, and Rada Mihalcea. In-thewild video question answering. In *COLING*, pp. 5613–5635, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https:\n",
      "//aclanthology.org/2022.coling-1.496.\n",
      "\n",
      "Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. *arXiv preprint arXiv:2302.14045*, 2023.\n",
      "\n",
      "Yunseok Jang, Yale Song, Chris Dongjoo Kim, Youngjae Yu, Youngjin Kim, and Gunhee Kim.\n",
      "\n",
      "Video Question Answering with Spatio-Temporal Reasoning. *IJCV*, 2019.\n",
      "\n",
      "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.\n",
      "\n",
      "KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. *arXiv preprint arXiv:2305.06355*,\n",
      "2023b.\n",
      "\n",
      "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-ChatGPT:\n",
      "Towards detailed video understanding via large vision and language models. *arXiv preprint* arXiv:2306.05424, 2023.\n",
      "\n",
      "OpenAI. Chatgpt: Large language model for human style conversation. https://chat.\n",
      "\n",
      "openai.com, 2023.\n",
      "\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems*, 35:\n",
      "27730–27744, 2022.\n",
      "\n",
      "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.\n",
      "\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pp.\n",
      "\n",
      "8748–8763. PMLR, 2021.\n",
      "\n",
      "Richard F Thompson and Stephen A Madigan. *Memory: the key to consciousness*, volume 3. Princeton University Press, 2013.\n",
      "\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee´\n",
      "Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and `\n",
      "efficient foundation language models. *arXiv preprint arXiv:2302.13971*, 2023.\n",
      "\n",
      "Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. *arXiv* preprint arXiv:2304.14407, 2023.\n",
      "\n",
      "Alex Wilf, Leena Mathur, Sheryl Mathew, Claire Ko, Youssouf Kebe, Paul Pu Liang, and LouisPhilippe Morency. Social-iq 2.0 challenge: Benchmarking multimodal social understanding.\n",
      "\n",
      "https://github.com/abwilf/Social-IQ-2.0-Challenge, 2023.\n",
      "\n",
      "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.\n",
      "\n",
      "Video question answering via gradually refined attention over appearance and motion. In ACM\n",
      "Multimedia, 2017.\n",
      "\n",
      "Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. *arXiv preprint arXiv:2304.14178*, 2023.\n",
      "\n",
      "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. *arXiv preprint* arXiv:2210.02414, 2022.\n",
      "\n",
      "Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. *arXiv preprint arXiv:2306.02858*, 2023a. URL https:// arxiv.org/abs/2306.02858.\n",
      "\n",
      "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. *arXiv preprint arXiv:2306.02858*, 2023b.\n",
      "\n",
      "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.\n",
      "\n",
      "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. *arXiv preprint* arXiv:2304.10592, 2023.\n",
      "\n",
      "# Appendix\n",
      "\n",
      "## A Ablation Study On Influence Of The Hyperparameter K\n",
      "\n",
      "We study the influence of K. In general, when K is too small, it may lead to a loss of information necessary to answer the question. When K is too large, interference may be introduced, confusing the LLM. Table 5 shows the results of using different K. We found that as K gradually increases from 1 to 5, the average performance increases. When K increases from 5 to 7, the performance decreases. We found K=5 presents a good trade-off on most datasets, even though there are slight differences on different datasets. We leave the adaptive design of K as the future work. Table 5: Ablation study on the influence of K, evaluated in terms of accuracy (%)/score. We use bold to mark the best performance and underline to mark the second-best performance.\n",
      "\n",
      "| Dataset       | Video-ChatGPT   | Ours(K=1)   | Ours(K=3)   | Ours(K=5)   | Ours(K=7)   |\n",
      "|---------------|-----------------|-------------|-------------|-------------|-------------|\n",
      "| WildQA        | 58.00/3.30      | 57.45/3.18  | 60.58/3.31  | 64.82/3.39  | 63.44/3.39  |\n",
      "| QaEgo4D       | 29.74/2.43      | 32.42/2.41  | 32.04/2.42  | 32.51/2.45  | 32.81/2.42  |\n",
      "| lifeQA        | 33.87/2.55      | 37.63/2.62  | 38.44/2.62  | 38.71/2.61  | 37.63/2.65  |\n",
      "| Social-IQ 2.0 | 57.73/3.26      | 63.92/3.44  | 60.89/3.34  | 63.65/3.40  | 62.89/3.34  |\n",
      "| Average       | 44.84/2.89      | 47.86/2.91  | 47.99/2.92  | 49.92/2.96  | 49.19/2.95  |\n",
      "\n",
      "## B More Visualization Results\n",
      "\n",
      "We visualize more examples from the QAEgo4D and WildQA datasets in Fig. 3 and Fig. 4, with the following information. 1) The first row shows the video chunk samples by uniformly selecting 5 video chunks. 2) The second row shows the retrieved 5 chunks (ordered by time order) in our R-VLM. We mark the groudtruth chunks by red box. 3) We show the learned similarity score curve based on which the top K chunks are selected. The horizontal axis represents the identity of chunk and the vertical axis denotes the similarity score of that chunk. The groundtruth chunks and our retrieved chunks are also marked. 4) The question and answers from different models: R-VLM, R-VLM w/Uni., *Video-ChatGPT*, and *Video-LLaMA*, respectively.\n",
      "\n",
      "We also show some failure examples in Fig. 5. A detailed analysis of the reasons for failure is given in the figure caption. There are two main cases of failure. One is that the retrieval does not select the correct video chunks. The other is that the retrieval correctly identified the correct video chunks, but the answer is wrong. For the later cases, we think more powerful vision feature extractor and LLMs would alleviate the problem.\n",
      "\n",
      "## C Computational Complexity\n",
      "\n",
      "The computational cost comes from two parts. The first part is to encode the video frames through the CLIP encoder and the spatial-temporal pooling to get chunks. The second part is the retrieval of K=5 chunks and put them to LLM for inference. The spatial-temporal pooing and retrieval is very fast and negligible. On a single A100, we tested 120 60s videos from Social-IQ 2.0 and calculated the average inference time cost for a video. For a single video, the first part for vision feature extraction takes an average of 0.14s (in parallel for 60 frames), and the second part takes an average of 2.42s. The total time is 2.56s. Actually, for an even longer video, the time consumption for the second part does not increase since the input number of vision tokens is fixed (i.e., 68×5=340) in our scheme, which is favored for long video or streaming video understanding. The GPU memory consumption is about 17GB. Note that the computational cost for the LLM is proportional to the number of tokens.\n",
      "\n",
      "The FLOPs for LLM inference can be roughly estimated as 2PD, where P denotes the number of parameters (model size), and D denotes the number of tokens. The computational complexity of LLM is proportional to the number of tokens which consists of text tokens (question and answer)\n",
      "and vision tokens. The LLM model size P is 6.7B. On the training dataset, the average number of tokens for question and answers is 80, i.e., Dtex = 80. This varies on different datasets. For simplicity, we assume the number is the same for all the datasets. We denote the number of vision tokens as Dvis. The total number of tokens is D = Dtex + Dvis. For the four video datasets, WildQA, QaEgo4D, lifeQA, Social-IQ 2.0, the average number of vision chunks is 19, 122, 20, and 16, where each chunk has 68 tokens. Thanks to the retrieval, only K = 5 chunks (D′vis =\n",
      "5 × 68 = 340 tokens) instead of all the chunks are needed as the input to LLM. Therefore, the computational cost (FLOPs) for LLM inference can be saved approximately Dvis–D′vis Dtex+Dvis\n",
      ", which are 69% (i.e., (19 × 68–5 × 68)/(80 + 19 × 68)), 95%, 71%, and 64%, respectively.\n",
      "\n",
      "![12_image_0.png](12_image_0.png)\n",
      "\n",
      "(a) We can see that the gray car does not appear in the uniformly sampled video chunks. Our R-VLM correctly answers that the car was parked in the parking lot (outdoors), but R-VLM w/Uni.'s answer was the garage\n",
      "(indoors). Video-LLaMA does not answer where the car is and the groundtruth frames do not appear in the frame 100. Video-ChatGPT made the similar mistake as R-VLM w/Uni.\n",
      "\n",
      "![12_image_1.png](12_image_1.png)\n",
      "\n",
      "(b) The broom is small and is on the left in the red boxed image. Our R-VLM captures exactly where the broom is, i.e., on the ground. R-VLM w/Uni. does not capture the video chunks with broom and thus does not answer accurately. The answer of Video-ChatGPT is irrelevant to the question. The answer from Video-LLaMA is redundancy and tedious, where the mentioned blue jean jacket and boat actually do not appear in the video.\n",
      "\n",
      "Figure 3: Visualization of video QA examples from QAEgo4D.\n",
      "\n",
      "![13_image_0.png](13_image_0.png)\n",
      "\n",
      "(a) The uniform sampling miss the chunks for sharpening process in GT-segs (at the beginning of video). As a result, LLM does not see the knife running along the leather, and only see the knife and some delicate small objects. Therefore, R-VLM w/Uni. mistakenly thought that this individual was carving patterns or making designs. Our retrieved chunks retain the process of the knife running on the leather and therefore R-VLM gives the correct answer. Both Video-LLaMA and Video-ChatGPT answered that people are cutting leather with a knife to make art, rather than sharpening the knife.\n",
      "\n",
      "![13_image_1.png](13_image_1.png)\n",
      "\n",
      "(b) In this video, collecting the kindling takes a short time, while placing the tinder on the stones takes a longer time. Uniform sampling makes LLM think that there is no process of collecting kindling and output the wrong answer of \"digging a hole\". Our R-VLM identified the relevant chunks of \"gathering\" even though those chunks only take a small duration in the entire video, generating correct answer. Video-LLaMA's prediction is not accurate since in fact the man did not gather deadwood and leaves in the video.\n",
      "Figure 4: Visualization of video QA examples from WildQA.\n",
      "\n",
      "![14_image_0.png](14_image_0.png)\n",
      "\n",
      "(a) A failure case from WildQA. This is a video of a person firing art. Although our method R-VLM retrieved the correct chunks, it gave the wrong answer of \"cook food\". We think this is due to the visual ambiguity of the target object and the biases of the LLM.\n",
      "\n",
      "![14_image_1.png](14_image_1.png)\n",
      "\n",
      "(b) A failure case from WildQA. Groundtruth chunks correspond to the chunks where three types of animals present, namely dolphins (the first chunk), seagulls, and human. Our method only retrieved the seagull and human chunks, but missed the dolphin chunk. R-VLM provided wrong answer due to the imperfect retrieval and the unsatisfactory reasoning capability of the used LLM.\n",
      "\n",
      "![14_image_2.png](14_image_2.png)\n",
      "\n",
      "(c) A failure case from QAEgo4D. Our method did not find the correct chunks. Therefore, large language model did not correctly answer the question and provided hallucinated answer.\n",
      "\n",
      "Figure 5: Visualization of failure cases from WildQA and QAEgo4D.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_reviewmt_file():\n",
    "    with open(\"reviewmt_train/00.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = load_reviewmt_file()\n",
    "\n",
    "example = data[0]\n",
    "print(\"Instruction:\", example[\"instruction\"])\n",
    "print(\"Input:\", example[\"input\"])\n",
    "\n",
    "example = data[100]\n",
    "print(\"Instruction:\", example[\"instruction\"])\n",
    "print(\"Input:\", example[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f279a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2000\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "print(\"Number of examples:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bacf",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653edb0",
   "metadata": {},
   "source": [
    "## Baseline LLM only\n",
    "\n",
    "Gives you a running system in <100 lines, a sanity‑check for your metric code, and a performance floor you’ll later try to beat.\n",
    "\n",
    "• Draft a prompt template that mirrors real reviews (summary, strengths, weaknesses, score). <br>\n",
    "• CLI script: generate_review.py --model gpt-4o --paper_id X → JSON review. <br>\n",
    "• Serialize outputs for 50–100 papers. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12ee47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "717ce7d0",
   "metadata": {},
   "source": [
    "## Evaluation harness\n",
    "\n",
    "You want tight feedback loops. Implementing metrics now lets you plug in future variants with one line of code.\n",
    "\n",
    "\n",
    "• Automatic overlap metrics (BERTScore, ROUGE). <br>\n",
    "• Placeholder hooks for factual‑consistency checks (e.g., using GPT‑4 to judge). <br>\n",
    "• Skeleton for a human‑rating spreadsheet or web form. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d379b",
   "metadata": {},
   "source": [
    "## RAG plus Vector DB\n",
    "\n",
    "Only once data is fixed can you build a stable RAG back‑end.\n",
    "\n",
    "• Decide on chunk size & overlap; embed with an off‑the‑shelf model (e.g., Instructor or OpenAI Ada‑002). <br>\n",
    "• Build a vector index (FAISS/Qdrant) and a lightweight search API. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a5525",
   "metadata": {},
   "source": [
    "## Retrival powered generation\n",
    "\n",
    "Now you can focus on retrieval + fusion strategies and immediately compare against the baseline using the harness you already trust.\n",
    "\n",
    "• Retrieve top‑k chunks, concatenate with the prompt, generate review. <br>\n",
    "• Experiment with reranking or citation‑insertion heuristics. <br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
