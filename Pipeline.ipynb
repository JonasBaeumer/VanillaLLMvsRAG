{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64b8ced",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "All later components (baseline, RAG, metrics) need the same corpus and review “ground truth.” Cleaning and structuring it early prevents painful re‑work.\n",
    "\n",
    "• Pick a public peer‑review dataset (e.g., PeerRead, ACL Blind Submission). <br>\n",
    "• Write a loader that yields (paper_text, meta, gold_review) triples. <br>\n",
    "• Basic preprocessing: strip markup, split into sections, tokenize. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a96e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a622681",
   "metadata": {},
   "source": [
    "### EDA: Review_mt dataset (not useful for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872c3f3",
   "metadata": {},
   "source": [
    "Each json file contains a list of dictionaries with the respective entries:\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"...\",\n",
    "    \"input\": \"...\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"...\",\n",
    "    \"input\": \"...\"\n",
    "  },\n",
    "  ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d76bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: You areonses from the author and comments from all reviewers to provide the meta-review and determine the final decision. E a decision maker. Please review all respxplicitly state 'Accept' or 'Reject' at the end of your output.\n",
      "Input: Title: Characterizing and Measuring the Similarity of Neural Networks with Persistent Homology . Abstract: Characterizing the structural properties of neural networks is crucial yet poorly understood, and there are no well-established similarity measures between networks. In this work, we observe that neural networks can be represented as abstract simplicial complex and analyzed using their topological 'fingerprints' via Persistent Homology (PH). We then describe a PH-based representation proposed for characterizing and measuring similarity of neural networks. We empirically show the effectiveness of this representation as a descriptor of different architectures in several datasets. This approach based on Topological Data Analysis is a step towards better understanding neural networks and a useful similarity measure. Main Text: Introduction\n",
      "\n",
      "Machine learning practitioners can train different neural networks for the same task. Even for the same neural architecture, there are many hyperparameters, such as the number of neurons per layer or the number of layers. Moreover, the final weights for the same architecture and hyperparameters can vary depending on the initialization and the optimization process itself, which is stochastic. Thus, there is no direct way of comparing neural networks accounting for the fact that neural networks solving the same task should be measured as being similar, regardless of the specific weights. This also prevents one from finding and comparing modules inside neural networks (e.g., determining if a given sub-network does the same function as other sub-network in another model). Moreover, there are no well-known methods for effectively characterizing neural networks.\n",
      "\n",
      "This work aims to characterize neural networks such that they can be measured to be similar once trained for the same task, with independence of the particular architecture, initialization, or optimization process. To characterize neural networks and measuring their similarity, we assume two different similarities that should be related if the characterization is correct, namely, 1. similarity between the tasks a model is trained on, and 2. similarity between neural networks themselves considered as mathematical objects. We do not provide a mathematical definition for the first one, rather an intuitive one, and understand is as qualitative similarity between semantically close tasks.\n",
      "\n",
      "For instance, we think that MNIST (10 labels) is closer to MNIST restricted to 2 labels than to language identification, and that MNIST is closer to MNIST restricted to 8 labels than to MNIST\n",
      "restricted to 2 labels. For the second one, in this work we propose a similarity measure.\n",
      "\n",
      "Ideally, this similarity measure should then capture 1. the semantic similarity between the underlying tasks neural networks have been trained on (understanding that networks performing similar tasks should be similar, regardless of their specific weights and architectures), and 2. intrinsic properties of the neural network should also be captured to some degree. Thus, we consider two scenarios, that is, comparing NNs trained for similar tasks, using the mentioned task similarity, and comparing neural networks trained on identical tasks but with different architectures.\n",
      "\n",
      "We focus on Multi-Layer Perceptrons (MLPs) for the sake of simplicity. We start by observing that we can represent a neural network as a directed weighted graph to which we can associate certain topological concepts.1 Considering it as a simplicial complex, we obtain its associated Persistent Diagram. Then, we can compute distances between Persistent Diagrams of different neural networks.\n",
      "\n",
      "1See Jonsson (2007) for a complete reference on graph topology.\n",
      "\n",
      "1 The proposed experiments aim to show that the selected structural feature, Persistent Homology, serves to relate neural networks trained for similar problems and that such a comparison can be performed by means of a predefined measure between the associated Persistent Homology diagrams.\n",
      "\n",
      "To test the hypothesis, we study different classical problems (MNIST, Fashion MNIST, CIFAR-10, and language identification and text classification datasets), different architectures (number and size of layers) as well as a control experiment (input order, to which neural network similarity should be invariant). The code and results are fully open source in the Supplementary Material under a MIT\n",
      "license.\n",
      "\n",
      "In summary, the main contributions of this work are the following: 1. We propose an effective graph characterization strategy of neural networks based on Persistent Homology. 2. Based on this characterization, we suggest a similarity measure of neural networks. 3. We provide empirical evidence that this Persistent Homology framework captures valuable information from neural networks and that the proposed similarity measure is meaningful.\n",
      "\n",
      "The remainder of this paper is organized as follows. In Section 2, we go through the related work.\n",
      "\n",
      "Then, in Section 3 we describe our proposal and the experimental framework to validate it. Finally, in sections 4 and 5 we report and discuss the results and arrive to conclusions, respectively.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "One of the fundamental papers of Topological Data Analysis (TDA) is presented in Carlsson (2009) and suggests the use of Algebraic Topology to obtain qualitative information and deal with metrics for large amounts of data. For an extensive overview of simplicial topology on graphs, see Giblin\n",
      "(1977); Jonsson (2007). Aktas et al. (2019) provide a thorough analysis of PH methods.\n",
      "\n",
      "More recently, a number of publications have dealt with the study of the capacity of neural networks using PH. Guss & Salakhutdinov (2018) characterize learnability of different neural architectures by computable measures of data complexity. Donier (2019) propose the concept of spatial capacity allocation analysis. Konuk & Smith (2019) propose an empirical study of how NNs handle changes in topological complexity of the input data.\n",
      "\n",
      "Rieck et al. (2019b) introduce the *neural persistence* metric, a complexity measure based on TDA on weighted stratified graphs. This work suggests a representation of the neural network as a multipartite graph. They perform the filtering of the Persistent Homology diagrams independently for each layer. As the filtration contains at most 1-simplices (edges), they only capture zero-dimensional topological information, i.e. connectivity information. When consecutive layer analysis is used, the global topology of the network is not taken into account making the strong assumption that the NNs encode the learned information layer pairwise exclusively. Additionally, there are trivial global transformations of a NN that are not captured by analyzing pairs of layers:\n",
      "- Superfluous cycle insertions: for example, add two neurons and connect their input to a single neuron in a lower layer and their two outputs to a single output neuron in an upper layer with opposite weights.\n",
      "\n",
      "- Identity layer insertions: for instance, insert an intermediate identity layer with neurons and trivially connect to the next layer.\n",
      "\n",
      "- Non-planar neural networks analysis: the analysis of neural networks that use multiple connections between non-consecutive layers require higher order topological analysis.\n",
      "\n",
      "In terms of pure neural network analysis, there are relevant works, like Hofer et al. (2020), that study topological regularization. Clough et al. (2020) introduce a method for training neural networks for image segmentation with prior topology knowledge, specifically via Betti numbers. Corneanu et al.\n",
      "\n",
      "(2020) try to estimate (with limited success) the performance gap between training and testing via neuron activations and linear regression of the Betti numbers. This type of representation depend on the input data and not only on the NN function under study. Instead, we are interested in characterising and comparing NNs as functions, independently of the data to which they are applied.\n",
      "\n",
      "On the other hand, topological analysis of decision boundaries has been a very prolific area. Ramamurthy et al. (2019) propose a labeled Vietoris-Rips complex to perform PH inference of decision boundaries for quantification of the complexity of neural networks. Naitzat et al. (2020) experiment on the PH of a wide range of point cloud input datasets for a binary classification problems to see that NNs transform a topologically rich dataset (in terms of Betti numbers) into a topologically simpler one as it passes through the layers. They also verify that the reduction in Betti numbers is significantly faster for ReLU activations than hyperbolic tangent activations.\n",
      "\n",
      "Regarding neural network representations, one of the most related works to ours, Gebhart et al.\n",
      "\n",
      "(2019), focuses on topological representations of neural networks. They introduce a method for computing PH over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input.\n",
      "\n",
      "Interestingly, in Watanabe & Yamana (2020), authors work on neural network representations through simplicial complexes based on deep Taylor decomposition and they calculate the PH of neural networks in this representation. In Chowdhury et al. (2019), they use directed homology to represent MLPs. They show that the path homology of these networks is non-trivial in higher dimensions and depends on the number and size of the network layers. They investigate homological differences between distinct neural network architectures.\n",
      "\n",
      "As far as neural network similarity measures are concerned, the literature is not especially prolific. In Kornblith et al. (2019), authors examine similarity measures for representations (meaning, outputs of different layers) of neural networks based on canonical correlation analysis. However, note that this method *compares neural network representations (intermediate outputs), not the neural networks* themselves. Remarkably, in Ashmore & Gashler (2015), authors do deal with the intrinsic similarity of neural networks themselves based on Forward Bipartite Alignment. Specifically, they propose an algorithm for aligning the topological structures of two neural networks. Their algorithm finds optimal bipartite matches between the nodes of the two MLPs by solving the well-known graph cutting problem. The alignment enables applications such as visualizations or improving ensembles.\n",
      "\n",
      "However, the methods only works under very restrictive assumptions,2and this line of work does not appear to have been followed up.\n",
      "\n",
      "Finally, we note that there has been a considerable growth of interest in applied topology in the recent years. This popularity increase and the development of new software libraries,3along with the growth of computational capabilities, have empowered new works. Some of the most remarkable libraries are Ripser Tralie et al. (2018); Bauer (2021), and Flagser Lütgehetmann et al. (2019). They are focused on the efficient computation of PH. For GPU-Accelerated computation of Vietoris-Rips PH, Ripser++ Zhang et al. (2020) offers an important speedup. The Python library we are using, Giotto-TDA Tauzin et al. (2020), makes use of both above libraries underneath.\n",
      "\n",
      "We have seen that there is a trend towards the use of algebraic topology methods for having a better understanding of phenomena of neural networks and having more principled deep learning algorithms.\n",
      "\n",
      "Nevertheless, little to no works have proposed neural network characterizations or similarity measures based on intrinsic properties of the networks, which is what we intend to do.\n",
      "\n",
      "## 3 Methodology\n",
      "\n",
      "In this section, we propose our method, which is heavily based on concepts from algebraic topology.\n",
      "\n",
      "We refer the reader to the Supplementary Material for the mathematical definitions. In this section, we also describe the conducted experiments.\n",
      "\n",
      "Intrinsically characterizing and comparing neural networks is a difficult, unsolved problem. First, the network should be represented in an object that captures as much information as possible and then it should be compared with a measure depending on the latent structure. Due to the stochasticity of both the initialization and training procedure, networks are parameterized differently. For the same task, different functions that effectively solve it can be obtained. Being able to compare the trained networks can be helpful to detect similar neural structures.\n",
      "\n",
      "We want to obtain topological characterizations associated to neural networks trained on a given task. For doing so, we use the Persistence Homology (from now on, PH) of the graph associated to a\n",
      "\n",
      "2For example, the two neural networks \"must have the same number of units in each of their corresponding layers\", and the match is done layer by layer.\n",
      "\n",
      "3https://www.math.colostate.edu/~adams/advising/appliedTopologySoftware/\n",
      "3 neural network. We compute the PH for various neural networks learned on different tasks. We then compare all the diagrams for each one of the task.\n",
      "\n",
      "More specifically, for each of the studied tasks (image classification on MNIST, Fashion MNIST\n",
      "and CIFAR-10; language identification, and text classification on the Reuters dataset),4 we proceed as follows: 1. We train several neural network models on the particular problem. 2. We create a directed graph from the weights of the trained neural networks (after changing the direction of the negative edges and normalising the weights of the edges). 3. We consider the directed graph as a simplicial complex and calculate its PH, using the weight of the edges as the filtering parameter, which range from 0 to 1. This way we obtain the so-called Persistence Diagram. 4. We compute the distances between the Persistence Diagrams (prior discretization of the Persistence Diagram so that it can be computed) of the different networks. 5. Finally, we analyze the similarity between different neural networks trained for the same task, for a similar task, and for a completely different task, independently of the concrete architecture, to see whether there is topological similarity.\n",
      "\n",
      "As baselines, we set two standard matrix comparison methods that are the 1-Norm and the Frobenius norm. Having adjacency matrix A and B, we compute the difference as *norm*(A−B). However, these methods only work for matrices of similar size and thus, they are not general enough. We could also have used the Fast Approximate Quadratic assignment algorithm suggested in Vogelstein et al.\n",
      "\n",
      "(2015), but for large networks this method becomes unfeasible to compute.\n",
      "\n",
      "## 3.1 Proposal\n",
      "\n",
      "Our method is as follows. We start by associating to a neural network a weighted directed graph that is analyzed as an abstract simplicial complex consisting on the union of points, edges, triangles, tetrahedrons and larger dimension polytopes (those are the elements referred as simplices). Abstract simplicial complexes are used in opposition to geometric simplicial complexes, generated by a point cloud embedded in the Euclidean space R\n",
      "n.\n",
      "\n",
      "Given a trained neural network, we take the collection of neural network parameters as directed and weighted edges that join neurons, represented by graph nodes. Biases are considered as new vertices that join target neurons with an edge having a given weight. Note that, in this representation, we lose the information about the activation functions, for simplicity and to avoid representing the network as a multiplex network. Bias information could also have been ignored because we want large PH\n",
      "groups that characterize the network, while these connections will not change the homology group dimension of any order.\n",
      "\n",
      "For negative edge weights, we reverse edge directions and maintain the absolute value of the weights.\n",
      "\n",
      "We discard the use of weight absolute value since neural networks are not invariant under weight sign transformations. This representation is consistent with the fact that every neuron can be replaced by a neuron from which two edges with opposite weights emerge and converge again on another neuron with opposite weights. From the point of view of homology, this would be represented as a closed cycle. We then normalize the weights of all the edges as expressed in Equation 1 where w is the weight to normalize, W are all the weights and ζ is an smoothing parameter that we set to 0.000001. This smoothing parameter is necessary as we want to avoid normalized weights of edges to be 0. This is because 0 implies a lack of connection.\n",
      "\n",
      "$$m a x(1-{\\frac{|w|}{m a x(|W|)}},\\zeta)$$\n",
      "$$(1)$$\n",
      ",ζ ) (1)\n",
      "Given this weighted directed graph, we then define a directed flag complex associated to it. Topology of this directed flag complex can be studied using homology groups Hn. In this work we calculate homology groups up to degree 3 (H0-H3) due to computational complexity and our neural network representation method's layer connectivity limit.\n",
      "\n",
      "The dimensions of these homology groups are known as Betti numbers. The i-th Betti number is the number of i-dimensional voids in the simplicial complex (β0 gives the number of connected components of the simplicial complex, β1 gives the number of non reducible loops and so on). For a deeper introduction to algebraic topology and computational topology, we refer to Edelsbrunner &\n",
      "Harer (2009); Ghrist (2014).\n",
      "\n",
      "We work with a family of simplicial complexes, Kε , for a range of values of ε ∈ R so that the complex at step εiis embedded in the complex at ε j for i ≤ j, i.e. Kεi ⊆ Kε j\n",
      ". In our case, ε is the minimum weight of included edges of our graph representation of neural networks. Filtration parameter could also be used to select active nodes thought, the library we used does not include this capability.\n",
      "\n",
      "The nested family of simplicial complexes is called a *filtration*. We calculate a sequence of homology groups by varying the ε parameter, obtaining a persistence homology diagram. PH calculations are performed on Z2.\n",
      "\n",
      "This filtration gives a collection of contained directed weighted graph or simplicial complex Kεmin ⊆\n",
      "... ⊆ Kεt ⊆ Kεt+1 ⊆ ... ⊆ Kεmax , where t ∈ [0,1] and εmin = 0, εmax = 1 (recall that edge weights are normalized).\n",
      "\n",
      "Given a filtration, one can look at the birth, when a homology class appears, and death, the time when the homology class disappears. The PH treats the birth and the death of these homological features in Kε for different ε values. Lifespan of each homological feature can be represented as an interval\n",
      "(birth,*death*), of the homological feature. For each filtration, we can record all these intervals by a Persistence Barcode (PB) Carlsson (2009), or in a Persistence Diagram (PD), as a collection of multiset of intervals.\n",
      "\n",
      "As mentioned previously, our interest in this work is to compare PDs from two different simplicial complexes. There are two distances traditionally used to compare PDs, Wasserstein distance and Bottleneck distance. Their stability with respect to perturbations on PDs has been object of different studies Chazal et al. (2012); Cohen-Steiner et al. (2005). As shown in comparative studies such as in Fasy et al. (2020), different distances and different ways of vectorising persistence diagrams have results with different levels of stability and quality. In order to make computations feasible and to obviate noisy intervals, we filter the PDs by limiting the minimum PD interval size. We do so by setting a minimum threshold η = 0.01. Intervals with a lifespan under this value are not considered. Additionally, for computing distances, we need to remove infinity values. As we are only interested in the deaths until the maximum weight value, we replace all the infinity values by 1.0.\n",
      "\n",
      "Wasserstein distance calculations are computationally hard for large PDs (each PD of our NN models has a million persistence intervals per diagram). Therefore we use a vectorized version of PDs instead, also called PD discretization. This vectorized version summaries have been proposed and used on recent literature Adams et al. (2017); Berry et al. (2020); Bubenik (2015); Lawson et al. (2019); Rieck et al. (2019a). For the persistence diagram distance calculation, we use the Giotto-TDA library Tauzin et al. (2020) and compute the following supported vectorized persistence summaries: 1. Persistence landscape. 2. Weighted silhouette. 3. Heat vectorizations.\n",
      "\n",
      "## 3.2 Experimental Framework\n",
      "\n",
      "To determine the topological structural properties of trained NNs, we select different kinds of datasets.\n",
      "\n",
      "We opt for four well-known benchmarks in the machine learning community and one regarding language identification: (1) the MNIST5 dataset for classifying handwritten digit images, (2) the Fashion MNIST Xiao et al. (2017) dataset for classifying clothing images into 10 categories, (3) the CIFAR-106(CIFAR) dataset for classifying 10 different objects, (4) the Reuters dataset for classifying news into 46 topics, and (5) the Language Identification Wikipedia dataset7for identifying 7 different languages. For CIFAR-10 and Fashion MNIST, we pre-train a Convolutional NN (CNN), and the convolutional layers are shared between all the models of the same dataset as a feature extractor.\n",
      "\n",
      "Recall that we are focusing on MLPs, so we do not consider that convolutional weights. For MNIST,\n",
      "Reuters and Language Identification, we use an MLP. For Reuters and Language identification datasets, we vectorize the sentences with character frequency.\n",
      "\n",
      "5http://yann.lecun.com/exdb/mnist/\n",
      "6https://www.cs.toronto.edu/~kriz/cifar.html 7https://www.floydhub.com/floydhub/datasets/language-identification/1/\n",
      "data\n",
      "\n",
      "| Number   | Experiment       | Index   |\n",
      "|----------|------------------|---------|\n",
      "| 1        | Layer size       | 1-4     |\n",
      "| 2        | Number of layers | 5-9     |\n",
      "| 3        | Input order      | 10-14   |\n",
      "| 4        | Number of labels | 15-19   |\n",
      "\n",
      "Table 1: Indices of the experiments of the distance matrices.\n",
      "We study the following variables (hyperparameters): 1. Layer width, 2. Number of layers, 3. Input order8 4. Number of labels (number of considered classes). We define the *base* architecture as the one with a layer width of 512, 2 layers, the original features order, and considering all the classes\n",
      "(10 in the case of MNIST, Fashion MNIST and CIFAR, 46 in the case of Reuters and 7 in the case of the language identification task). Then, doing one change at a time, keeping the rest of the base architecture hyperparameters, we experiment with architectures with the following configurations:\n",
      "1. **Layer width**: 128, 256, 512 (*base*) and 1024. 2. **Number of layers**: 2 (*base*), 4, 6, 8 and 10.\n",
      "\n",
      "3. **Input order**: 5 different randomizations (with *base* structure), the control experiment. 4. **Number**\n",
      "of labels (MNIST, Fashion MNIST, CIFAR-10): 2, 4, 6, 8 and 10 (*base*). 5. **Number of labels**\n",
      "(Reuters): 2, 6, 12, 23 and 46 (*base*). 6. **Number of labels** (Language Identification): 2, 3, 4, 6 and 7 (*base*). Note that this is not a grid search over all the combinations. We always modify one hyperparameter at a time, and keep the rest of them as in the base architecture. In other words, we experiment with all the combinations such that only one of the hyperparameters is set to a non-base value at a time. For each dataset, we train 5 times (each with a different random weight initialization)\n",
      "each of these neural network configurations. Then, we compute the topological distances (persistence landscape, weighted silhouette, heat) among the different architectures. In total, we obtain 5×5×3 distance matrices (5 datasets, 5 random initializations, 3 distance measures). Finally, we average the 5 random initializations, such that we get 5×3 matrices, one for each distance on each dataset.\n",
      "\n",
      "All the matrices have dimensions 19×19, since 19 is the number of experiments for each dataset\n",
      "(corresponding to the total the number of architectural configurations mentioned above). Note that the base architecture appears 8 times (1, on the number of neurons per layer, 1 on the number of layers, 1 on the number of labels and the 5 randomizations of weight initializations). All experiments were executed in a machine with 2 NVIDIA V100 of 32GB, 2 Intel(R) Xeon(R) Platinum 8176 CPU\n",
      "@ 2.10GHz, and of 1.5TB RAM, for a total of around 3 days.\n",
      "\n",
      "Note that this work focuses on MLPs, however, the method proposed is also applicable to CNNs, as they have a MLP equivalent.9 The resulting NN is a highly sparse simplicial complex with a large number of parameters which makes PH calculation out of our computation capability for all combination of proposed experiments.\n",
      "\n",
      "## 4 Results & Discussion\n",
      "\n",
      "Results from control experiments can be seen in the third group on Figures 1 and 4. In these figures, groups are separated visually using white dashed lines. Experiments groups are specified in Table 1.\n",
      "\n",
      "Control experiments in all the images appear very dimmed, which means that they are very similar, as expected. Recall that the control experiments consist of 5 (randomizations) × 5 (executions) and that 25 different neural networks have been trained; each one of the network has more than 690,000 parameters that have been randomly initialized. After the training, results show that these networks have very close topological distance, as expected.\n",
      "\n",
      "For Figure 2 we computed both 1-norm and Frobenius norm (the baselines) for graphs' adjacency matrices of control experiments. Note that as we ran the experiment five times, we make the mean for each value of the matrix. In order to show whether the resulting values are positive or negative, we subtract to the maximum difference of each dataset the norm of each cell separately, we take the absolute value and we divide by the maximum difference of each dataset. Therefore, we obtain five values per dataset. Table 2 shows the statistics reflecting that the distance among the experiments are large and, thus, they are not characterizing any similarity but rather an important dissimilarity.\n",
      "\n",
      "8Order of the input features, the control experiment.\n",
      "\n",
      "9https://aul12.me/machinelearning/2019/06/20/cnn-mlp-2.html\n",
      "\n",
      "![6_image_0.png](6_image_0.png)\n",
      "\n",
      "![6_image_1.png](6_image_1.png)\n",
      "\n",
      "(b) Language Identification 0.0 1.0\n",
      "\n",
      "![6_image_2.png](6_image_2.png)\n",
      "\n",
      "Figure 1: Distance matrices using Silhouette discretization.\n",
      "\n",
      "Figure 2: Control experiments using norms.\n",
      "\n",
      "| Norm      | Minimum   | Maximum   | Mean   | Standard deviation   |\n",
      "|-----------|-----------|-----------|--------|----------------------|\n",
      "| 1-Norm    | 0.6683    | 4.9159    | 1.9733 | 1.5693               |\n",
      "| Frobenius | 0.0670    | 0.9886    | 0.4514 | 0.3074               |\n",
      "\n",
      "Table 2: Normalized difference comparison of self-norm against the maximum mean distance of the experiment.\n",
      "\n",
      "![7_image_0.png](7_image_0.png)\n",
      "\n",
      "Figure 3: Control experiment comparison matrix using Silhouette discretization.\n",
      "\n",
      "| Heat distance   |        | Silhouette distance   |        |           |\n",
      "|-----------------|--------|-----------------------|--------|-----------|\n",
      "| Dataset         | Mean   | Deviation             | Mean   | Deviation |\n",
      "| MNIST           | 0.0291 | 0.0100                | 0.1115 | 0.0364    |\n",
      "| F. MNIST        | 0.0308 | 0.0132                | 0.0824 | 0.0353    |\n",
      "| CIFAR-10        | 0.0243 | 0.0068                | 0.0769 | 0.0204    |\n",
      "| Language I.     | 0.0159 | 0.0040                | 0.0699 | 0.0159    |\n",
      "| Reuters         | 0.0166 | 0.0051                | 0.0387 | 0.0112    |\n",
      "\n",
      "Table 3: PH distances across input order (control) experiments, normalized by dataset.\n",
      "In contrast, Figure 3, with our method (Silhouette), shows perfect diagonal of similarity blocks. In the corresponding numeric results, we obtained small distances, as shown in Table 3. We can appreciate that each dataset has its own hub. This confirms the validity of our proposed similarity measure.\n",
      "\n",
      "The method we present also seems to capture some parts of hyperparameter setup. For instance, in Figure 4 we can observe gradual increase of distances in the first group regarding layer size meaning that, as layer size increases, the topological distance increases too. Similarly, for the number of layers\n",
      "(second group) and number of labels (fourth group) the same situation holds. Note that in Fashion MNIST and CIFAR-10, the distances are dimmer because we are not dealing with the weights of the CNNs. Recall that the CNN acts as a frozen extractor and are pretrained for all runs (with the same weights), such that the MLP layers themselves are the only potential source of dissimilarity between runs. Thus, our characterization is sensitive to the architecture (e.g., if we increase the capacity, distances vary), but at the same time, as we saw before, it is not dataset-agnostic, meaning that it also captures whether two neural networks are learning the same problem or not.\n",
      "\n",
      "In Figure 4, Fashion MNIST (Figure 4b) and CIFAR (Figure 4c) dataset results are interestingly different from those of MNIST (Figure 4a) dataset. This is, presumably, because both Fashion MNIST and CIFAR use a pretrained CNN for the problem. Thus, we must analyze the results taking into account this perspective. The first fully connected layer size is important as it can avoid a bottleneck from the previous CNN output. Some works in the literature show that adding multiple fully connected layers does not necessarily enhance the prediction capability of CNNs Basha et al.\n",
      "\n",
      "(2019), which is congruent with our results when adding fully connected layers (experiments 5 to 9)\n",
      "that result in dimmer matrices than the one from. Concerning the experiments on input order, there is slightly more homogeneity than in MNIST, again showing that the order of sample has negligible influence. Moreover, there could have been even more homogeneity taking into account that the fully connected network reduced its variance thanks to the frozen weights of the CNN. This also supports the fact that the CNN is the main feature extractor of the network. As in MNIST results, CIFAR results show that the topological properties are, indeed, a mapping of the practical properties of neural networks. We refer to the Supplementary Material for all distance matrices for all datasets and all distances, as well as for the standard deviations matrices and experiment group statistics.\n",
      "\n",
      "![8_image_0.png](8_image_0.png)\n",
      "\n",
      "Figure 4: Distance matrices using Heat discretization.\n",
      "\n",
      "## 5 Conclusions & Future Work\n",
      "\n",
      "1.0\n",
      "\n",
      "![8_image_1.png](8_image_1.png)\n",
      "\n",
      "0.0\n",
      "\n",
      "Results in five different datasets from computer vision and natural language lead to similar topological properties and are interpretable, which yields to general applicability. The best discretizations found are Heat and Silhouette. They show better separation of experiment groups, and are effectively reflecting changes in a sensitive way (unlike Landscape discretization).\n",
      "\n",
      "The most remarkable conclusion comes from the control experiments. The corresponding neural networks, with different input order but the same architecture, are very close to each other.\n",
      "\n",
      "The PH framework does, indeed, abstract away the specific weight values, and captures latent information from the networks, allowing comparisons to be based on the function they approximate. The selected neural network representation is reliable and complete, and yields coherent and meaningful results. Instead, the baseline measures, the 1-Norm and the Frobenius norm, implied an important dissimilarity between the experiments in the control experiments, meaning that they did not capture the fact that these neural networks were very similar in terms of the solved problem.\n",
      "\n",
      "Our proposed characterization does, indeed, capture meaningful properties. To the best of our knowledge, our proposed similarity measure between neural networks is the first of its kind. As future work, we suggest adapting the method to architectures such as CNNs, RNNs, and Transformers\n",
      "(Vaswani et al., 2017). Finally, we suggest performing more analysis regarding the learning of a neural network from a topological point of view.\n",
      "\n",
      "Figure 5: Language Identification (Landscape).\n",
      "\n",
      "## References\n",
      "\n",
      "H. Adams, Tegan Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, Sofya Chepushtanova, E. Hanson, F. Motta, and Lori Ziegelmeier. Persistence images: A stable vector representation of persistent homology. *J. Mach. Learn. Res.*, 18:8:1–8:35, 2017.\n",
      "\n",
      "M. Aktas, E. Akba¸s, and Ahmed El Fatmaoui. Persistence homology of networks: methods and applications. *Applied Network Science*, 4:1–28, 2019.\n",
      "\n",
      "Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer perceptrons by forward bipartite alignment. In *2015 International Joint Conference on Neural* Networks (IJCNN), pp. 1–7, 2015. doi: 10.1109/IJCNN.2015.7280769.\n",
      "\n",
      "S. H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, and Snehasis Mukherjee. Impact of fully connected layers on performance of convolutional neural networks for image classification.\n",
      "\n",
      "CoRR, abs/1902.02771, 2019. URL http://arxiv.org/abs/1902.02771.\n",
      "\n",
      "Ulrich Bauer. Ripser: efficient computation of vietoris-rips persistence barcodes, 2021.\n",
      "\n",
      "E. Berry, Yen-Chi Chen, J. Cisewski-Kehe, and Brittany Terese Fasy. Functional summaries of persistence diagrams. *Journal of Applied and Computational Topology*, 4:211–262, 2020.\n",
      "\n",
      "Peter Bubenik. Statistical topological data analysis using persistence landscapes. *J. Mach. Learn.*\n",
      "Res., 16:77–102, 2015.\n",
      "\n",
      "G. Carlsson. Topology and data. *Bulletin of the American Mathematical Society*, 46:255–308, 2009.\n",
      "\n",
      "F. Chazal, V. D. Silva, and S. Oudot. Persistence stability for geometric complexes. *Geometriae* Dedicata, 173:193–214, 2012.\n",
      "\n",
      "Samir Chowdhury, T. Gebhart, Steve Huntsman, and Matvey Yutin. Path homologies of deep feedforward networks. *2019 18th IEEE International Conference On Machine Learning And* Applications (ICMLA), pp. 1077–1082, 2019.\n",
      "\n",
      "J. Clough, I. Öksüz, Nicholas Byrne, V. Zimmer, J. A. Schnabel, and A. P. King. A topological loss function for deep-learning based image segmentation using persistent homology. IEEE transactions on pattern analysis and machine intelligence, PP, 2020.\n",
      "\n",
      "D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. *Proceedings of* the twenty-first annual symposium on Computational geometry, 2005.\n",
      "\n",
      "C. Corneanu, M. Madadi, S. Escalera, and A. Martínez. Computing the testing error without a testing set. *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.\n",
      "\n",
      "2674–2682, 2020.\n",
      "\n",
      "Jonathan Donier. Capacity allocation analysis of neural networks: A tool for principled architecture design. *ArXiv*, abs/1902.04485, 2019.\n",
      "\n",
      "H. Edelsbrunner and J. Harer. *Computational Topology - an Introduction*. American Mathematical Society, 2009.\n",
      "\n",
      "Brittany Fasy, Yu Qin, Brian Summa, and Carola Wenk. Comparing distance metrics on vectorized persistence summaries. In *NeurIPS 2020 Workshop on Topological Data Analysis and Beyond*,\n",
      "2020.\n",
      "\n",
      "T. Gebhart, Paul Schrater, and A. Hylton. Characterizing the shape of activation space in deep neural networks. 2019 18th IEEE International Conference On Machine Learning And Applications\n",
      "(ICMLA), pp. 1537–1542, 2019.\n",
      "\n",
      "R. Ghrist. *Elementary Applied Topology*. Self-published, 2014.\n",
      "\n",
      "P. Giblin. *Graphs, surfaces, and homology : an introduction to algebraic topology*. Chapman and Hall, 1977.\n",
      "\n",
      "William H. Guss and R. Salakhutdinov. On characterizing the capacity of neural networks using algebraic topology. *ArXiv*, abs/1802.04443, 2018.\n",
      "\n",
      "C. Hofer, Florian Graf, M. Niethammer, and R. Kwitt. Topologically densified distributions. *ArXiv*,\n",
      "abs/2002.04805, 2020.\n",
      "\n",
      "J. Jonsson. *Simplicial complexes of graphs*. PhD thesis, KTH Royal Institute of Technology, 2007.\n",
      "\n",
      "Emir Konuk and K. Smith. An empirical study of the relation between network architecture and complexity. *2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)*,\n",
      "pp. 4597–4599, 2019.\n",
      "\n",
      "Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. *CoRR*, abs/1905.00414, 2019. URL http://arxiv.org/ abs/1905.00414.\n",
      "\n",
      "P. Lawson, A. Sholl, J. Brown, Brittany Terese Fasy, and C. Wenk. Persistent homology for the quantitative evaluation of architectural features in prostate cancer histology. *Scientific Reports*, 9, 2019.\n",
      "\n",
      "Daniel Lütgehetmann, Dejan Govc, Jason Smith, and R. Levi. Computing persistent homology of directed flag complexes. *arXiv: Algebraic Topology*, 2019.\n",
      "\n",
      "Gregory Naitzat, A. Zhitnikov, and L. Lim. Topology of deep neural networks. *J. Mach. Learn. Res.*,\n",
      "21:184:1–184:40, 2020.\n",
      "\n",
      "K. Ramamurthy, Kush R. Varshney, and Krishnan Mody. Topological data analysis of decision boundaries with application to model selection. *ArXiv*, abs/1805.09949, 2019.\n",
      "\n",
      "Bastian Alexander Rieck, F. Sadlo, and H. Leitte. Topological machine learning with persistence indicator functions. *ArXiv*, abs/1907.13496, 2019a.\n",
      "\n",
      "Bastian Alexander Rieck, Matteo Togninalli, C. Bock, Michael Moor, Max Horn, Thomas Gumbsch, and K. Borgwardt. Neural persistence: A complexity measure for deep neural networks using algebraic topology. *ArXiv*, abs/1812.09764, 2019b.\n",
      "\n",
      "Guillaume Tauzin, Umberto Lupo, Lewis Tunstall, Julian Burella Pérez, Matteo Caorsi, Anibal Medina-Mardones, Alberto Dassatti, and Kathryn Hess. giotto-tda: A topological data analysis toolkit for machine learning and data exploration, 2020.\n",
      "\n",
      "Christopher Tralie, Nathaniel Saul, and Rann Bar-On. Ripser.py: A lean persistent homology library for python. *The Journal of Open Source Software*, 3(29):925, Sep 2018. doi: 10.21105/joss.00925.\n",
      "\n",
      "URL https://doi.org/10.21105/joss.00925.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. *CoRR*, abs/1706.03762, 2017. URL\n",
      "http://arxiv.org/abs/1706.03762.\n",
      "\n",
      "Joshua T. Vogelstein, John M. Conroy, Vince Lyzinski, Louis J. Podrazik, Steven G. Kratzer, Eric T.\n",
      "\n",
      "Harley, Donniell E. Fishkind, R. Jacob Vogelstein, and Carey E. Priebe. Fast approximate quadratic programming for graph matching. *PLOS ONE*, 10(4):1–17, 04 2015. doi: 10.1371/journal.pone.\n",
      "\n",
      "0121002. URL https://doi.org/10.1371/journal.pone.0121002.\n",
      "\n",
      "S. Watanabe and Hayato Yamana. Topological measurement of deep neural networks using persistent homology. In *ISAIM*, 2020.\n",
      "\n",
      "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n",
      "\n",
      "S. Zhang, Mengbai Xiao, and H. Wang. Gpu-accelerated computation of vietoris-rips persistence barcodes. In *Symposium on Computational Geometry*, 2020.\n",
      "Instruction: You areonses from the author and comments from all reviewers to provide the meta-review and determine the final decision. E a decision maker. Please review all respxplicitly state 'Accept' or 'Reject' at the end of your output.\n",
      "Input: Title: Characterizing and Measuring the Similarity of Neural Networks with Persistent Homology . Abstract: Characterizing the structural properties of neural networks is crucial yet poorly understood, and there are no well-established similarity measures between networks. In this work, we observe that neural networks can be represented as abstract simplicial complex and analyzed using their topological 'fingerprints' via Persistent Homology (PH). We then describe a PH-based representation proposed for characterizing and measuring similarity of neural networks. We empirically show the effectiveness of this representation as a descriptor of different architectures in several datasets. This approach based on Topological Data Analysis is a step towards better understanding neural networks and a useful similarity measure. Main Text: Introduction\n",
      "\n",
      "Machine learning practitioners can train different neural networks for the same task. Even for the same neural architecture, there are many hyperparameters, such as the number of neurons per layer or the number of layers. Moreover, the final weights for the same architecture and hyperparameters can vary depending on the initialization and the optimization process itself, which is stochastic. Thus, there is no direct way of comparing neural networks accounting for the fact that neural networks solving the same task should be measured as being similar, regardless of the specific weights. This also prevents one from finding and comparing modules inside neural networks (e.g., determining if a given sub-network does the same function as other sub-network in another model). Moreover, there are no well-known methods for effectively characterizing neural networks.\n",
      "\n",
      "This work aims to characterize neural networks such that they can be measured to be similar once trained for the same task, with independence of the particular architecture, initialization, or optimization process. To characterize neural networks and measuring their similarity, we assume two different similarities that should be related if the characterization is correct, namely, 1. similarity between the tasks a model is trained on, and 2. similarity between neural networks themselves considered as mathematical objects. We do not provide a mathematical definition for the first one, rather an intuitive one, and understand is as qualitative similarity between semantically close tasks.\n",
      "\n",
      "For instance, we think that MNIST (10 labels) is closer to MNIST restricted to 2 labels than to language identification, and that MNIST is closer to MNIST restricted to 8 labels than to MNIST\n",
      "restricted to 2 labels. For the second one, in this work we propose a similarity measure.\n",
      "\n",
      "Ideally, this similarity measure should then capture 1. the semantic similarity between the underlying tasks neural networks have been trained on (understanding that networks performing similar tasks should be similar, regardless of their specific weights and architectures), and 2. intrinsic properties of the neural network should also be captured to some degree. Thus, we consider two scenarios, that is, comparing NNs trained for similar tasks, using the mentioned task similarity, and comparing neural networks trained on identical tasks but with different architectures.\n",
      "\n",
      "We focus on Multi-Layer Perceptrons (MLPs) for the sake of simplicity. We start by observing that we can represent a neural network as a directed weighted graph to which we can associate certain topological concepts.1 Considering it as a simplicial complex, we obtain its associated Persistent Diagram. Then, we can compute distances between Persistent Diagrams of different neural networks.\n",
      "\n",
      "1See Jonsson (2007) for a complete reference on graph topology.\n",
      "\n",
      "1 The proposed experiments aim to show that the selected structural feature, Persistent Homology, serves to relate neural networks trained for similar problems and that such a comparison can be performed by means of a predefined measure between the associated Persistent Homology diagrams.\n",
      "\n",
      "To test the hypothesis, we study different classical problems (MNIST, Fashion MNIST, CIFAR-10, and language identification and text classification datasets), different architectures (number and size of layers) as well as a control experiment (input order, to which neural network similarity should be invariant). The code and results are fully open source in the Supplementary Material under a MIT\n",
      "license.\n",
      "\n",
      "In summary, the main contributions of this work are the following: 1. We propose an effective graph characterization strategy of neural networks based on Persistent Homology. 2. Based on this characterization, we suggest a similarity measure of neural networks. 3. We provide empirical evidence that this Persistent Homology framework captures valuable information from neural networks and that the proposed similarity measure is meaningful.\n",
      "\n",
      "The remainder of this paper is organized as follows. In Section 2, we go through the related work.\n",
      "\n",
      "Then, in Section 3 we describe our proposal and the experimental framework to validate it. Finally, in sections 4 and 5 we report and discuss the results and arrive to conclusions, respectively.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "One of the fundamental papers of Topological Data Analysis (TDA) is presented in Carlsson (2009) and suggests the use of Algebraic Topology to obtain qualitative information and deal with metrics for large amounts of data. For an extensive overview of simplicial topology on graphs, see Giblin\n",
      "(1977); Jonsson (2007). Aktas et al. (2019) provide a thorough analysis of PH methods.\n",
      "\n",
      "More recently, a number of publications have dealt with the study of the capacity of neural networks using PH. Guss & Salakhutdinov (2018) characterize learnability of different neural architectures by computable measures of data complexity. Donier (2019) propose the concept of spatial capacity allocation analysis. Konuk & Smith (2019) propose an empirical study of how NNs handle changes in topological complexity of the input data.\n",
      "\n",
      "Rieck et al. (2019b) introduce the *neural persistence* metric, a complexity measure based on TDA on weighted stratified graphs. This work suggests a representation of the neural network as a multipartite graph. They perform the filtering of the Persistent Homology diagrams independently for each layer. As the filtration contains at most 1-simplices (edges), they only capture zero-dimensional topological information, i.e. connectivity information. When consecutive layer analysis is used, the global topology of the network is not taken into account making the strong assumption that the NNs encode the learned information layer pairwise exclusively. Additionally, there are trivial global transformations of a NN that are not captured by analyzing pairs of layers:\n",
      "- Superfluous cycle insertions: for example, add two neurons and connect their input to a single neuron in a lower layer and their two outputs to a single output neuron in an upper layer with opposite weights.\n",
      "\n",
      "- Identity layer insertions: for instance, insert an intermediate identity layer with neurons and trivially connect to the next layer.\n",
      "\n",
      "- Non-planar neural networks analysis: the analysis of neural networks that use multiple connections between non-consecutive layers require higher order topological analysis.\n",
      "\n",
      "In terms of pure neural network analysis, there are relevant works, like Hofer et al. (2020), that study topological regularization. Clough et al. (2020) introduce a method for training neural networks for image segmentation with prior topology knowledge, specifically via Betti numbers. Corneanu et al.\n",
      "\n",
      "(2020) try to estimate (with limited success) the performance gap between training and testing via neuron activations and linear regression of the Betti numbers. This type of representation depend on the input data and not only on the NN function under study. Instead, we are interested in characterising and comparing NNs as functions, independently of the data to which they are applied.\n",
      "\n",
      "On the other hand, topological analysis of decision boundaries has been a very prolific area. Ramamurthy et al. (2019) propose a labeled Vietoris-Rips complex to perform PH inference of decision boundaries for quantification of the complexity of neural networks. Naitzat et al. (2020) experiment on the PH of a wide range of point cloud input datasets for a binary classification problems to see that NNs transform a topologically rich dataset (in terms of Betti numbers) into a topologically simpler one as it passes through the layers. They also verify that the reduction in Betti numbers is significantly faster for ReLU activations than hyperbolic tangent activations.\n",
      "\n",
      "Regarding neural network representations, one of the most related works to ours, Gebhart et al.\n",
      "\n",
      "(2019), focuses on topological representations of neural networks. They introduce a method for computing PH over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input.\n",
      "\n",
      "Interestingly, in Watanabe & Yamana (2020), authors work on neural network representations through simplicial complexes based on deep Taylor decomposition and they calculate the PH of neural networks in this representation. In Chowdhury et al. (2019), they use directed homology to represent MLPs. They show that the path homology of these networks is non-trivial in higher dimensions and depends on the number and size of the network layers. They investigate homological differences between distinct neural network architectures.\n",
      "\n",
      "As far as neural network similarity measures are concerned, the literature is not especially prolific. In Kornblith et al. (2019), authors examine similarity measures for representations (meaning, outputs of different layers) of neural networks based on canonical correlation analysis. However, note that this method *compares neural network representations (intermediate outputs), not the neural networks* themselves. Remarkably, in Ashmore & Gashler (2015), authors do deal with the intrinsic similarity of neural networks themselves based on Forward Bipartite Alignment. Specifically, they propose an algorithm for aligning the topological structures of two neural networks. Their algorithm finds optimal bipartite matches between the nodes of the two MLPs by solving the well-known graph cutting problem. The alignment enables applications such as visualizations or improving ensembles.\n",
      "\n",
      "However, the methods only works under very restrictive assumptions,2and this line of work does not appear to have been followed up.\n",
      "\n",
      "Finally, we note that there has been a considerable growth of interest in applied topology in the recent years. This popularity increase and the development of new software libraries,3along with the growth of computational capabilities, have empowered new works. Some of the most remarkable libraries are Ripser Tralie et al. (2018); Bauer (2021), and Flagser Lütgehetmann et al. (2019). They are focused on the efficient computation of PH. For GPU-Accelerated computation of Vietoris-Rips PH, Ripser++ Zhang et al. (2020) offers an important speedup. The Python library we are using, Giotto-TDA Tauzin et al. (2020), makes use of both above libraries underneath.\n",
      "\n",
      "We have seen that there is a trend towards the use of algebraic topology methods for having a better understanding of phenomena of neural networks and having more principled deep learning algorithms.\n",
      "\n",
      "Nevertheless, little to no works have proposed neural network characterizations or similarity measures based on intrinsic properties of the networks, which is what we intend to do.\n",
      "\n",
      "## 3 Methodology\n",
      "\n",
      "In this section, we propose our method, which is heavily based on concepts from algebraic topology.\n",
      "\n",
      "We refer the reader to the Supplementary Material for the mathematical definitions. In this section, we also describe the conducted experiments.\n",
      "\n",
      "Intrinsically characterizing and comparing neural networks is a difficult, unsolved problem. First, the network should be represented in an object that captures as much information as possible and then it should be compared with a measure depending on the latent structure. Due to the stochasticity of both the initialization and training procedure, networks are parameterized differently. For the same task, different functions that effectively solve it can be obtained. Being able to compare the trained networks can be helpful to detect similar neural structures.\n",
      "\n",
      "We want to obtain topological characterizations associated to neural networks trained on a given task. For doing so, we use the Persistence Homology (from now on, PH) of the graph associated to a\n",
      "\n",
      "2For example, the two neural networks \"must have the same number of units in each of their corresponding layers\", and the match is done layer by layer.\n",
      "\n",
      "3https://www.math.colostate.edu/~adams/advising/appliedTopologySoftware/\n",
      "3 neural network. We compute the PH for various neural networks learned on different tasks. We then compare all the diagrams for each one of the task.\n",
      "\n",
      "More specifically, for each of the studied tasks (image classification on MNIST, Fashion MNIST\n",
      "and CIFAR-10; language identification, and text classification on the Reuters dataset),4 we proceed as follows: 1. We train several neural network models on the particular problem. 2. We create a directed graph from the weights of the trained neural networks (after changing the direction of the negative edges and normalising the weights of the edges). 3. We consider the directed graph as a simplicial complex and calculate its PH, using the weight of the edges as the filtering parameter, which range from 0 to 1. This way we obtain the so-called Persistence Diagram. 4. We compute the distances between the Persistence Diagrams (prior discretization of the Persistence Diagram so that it can be computed) of the different networks. 5. Finally, we analyze the similarity between different neural networks trained for the same task, for a similar task, and for a completely different task, independently of the concrete architecture, to see whether there is topological similarity.\n",
      "\n",
      "As baselines, we set two standard matrix comparison methods that are the 1-Norm and the Frobenius norm. Having adjacency matrix A and B, we compute the difference as *norm*(A−B). However, these methods only work for matrices of similar size and thus, they are not general enough. We could also have used the Fast Approximate Quadratic assignment algorithm suggested in Vogelstein et al.\n",
      "\n",
      "(2015), but for large networks this method becomes unfeasible to compute.\n",
      "\n",
      "## 3.1 Proposal\n",
      "\n",
      "Our method is as follows. We start by associating to a neural network a weighted directed graph that is analyzed as an abstract simplicial complex consisting on the union of points, edges, triangles, tetrahedrons and larger dimension polytopes (those are the elements referred as simplices). Abstract simplicial complexes are used in opposition to geometric simplicial complexes, generated by a point cloud embedded in the Euclidean space R\n",
      "n.\n",
      "\n",
      "Given a trained neural network, we take the collection of neural network parameters as directed and weighted edges that join neurons, represented by graph nodes. Biases are considered as new vertices that join target neurons with an edge having a given weight. Note that, in this representation, we lose the information about the activation functions, for simplicity and to avoid representing the network as a multiplex network. Bias information could also have been ignored because we want large PH\n",
      "groups that characterize the network, while these connections will not change the homology group dimension of any order.\n",
      "\n",
      "For negative edge weights, we reverse edge directions and maintain the absolute value of the weights.\n",
      "\n",
      "We discard the use of weight absolute value since neural networks are not invariant under weight sign transformations. This representation is consistent with the fact that every neuron can be replaced by a neuron from which two edges with opposite weights emerge and converge again on another neuron with opposite weights. From the point of view of homology, this would be represented as a closed cycle. We then normalize the weights of all the edges as expressed in Equation 1 where w is the weight to normalize, W are all the weights and ζ is an smoothing parameter that we set to 0.000001. This smoothing parameter is necessary as we want to avoid normalized weights of edges to be 0. This is because 0 implies a lack of connection.\n",
      "\n",
      "$$m a x(1-{\\frac{|w|}{m a x(|W|)}},\\zeta)$$\n",
      "$$(1)$$\n",
      ",ζ ) (1)\n",
      "Given this weighted directed graph, we then define a directed flag complex associated to it. Topology of this directed flag complex can be studied using homology groups Hn. In this work we calculate homology groups up to degree 3 (H0-H3) due to computational complexity and our neural network representation method's layer connectivity limit.\n",
      "\n",
      "The dimensions of these homology groups are known as Betti numbers. The i-th Betti number is the number of i-dimensional voids in the simplicial complex (β0 gives the number of connected components of the simplicial complex, β1 gives the number of non reducible loops and so on). For a deeper introduction to algebraic topology and computational topology, we refer to Edelsbrunner &\n",
      "Harer (2009); Ghrist (2014).\n",
      "\n",
      "We work with a family of simplicial complexes, Kε , for a range of values of ε ∈ R so that the complex at step εiis embedded in the complex at ε j for i ≤ j, i.e. Kεi ⊆ Kε j\n",
      ". In our case, ε is the minimum weight of included edges of our graph representation of neural networks. Filtration parameter could also be used to select active nodes thought, the library we used does not include this capability.\n",
      "\n",
      "The nested family of simplicial complexes is called a *filtration*. We calculate a sequence of homology groups by varying the ε parameter, obtaining a persistence homology diagram. PH calculations are performed on Z2.\n",
      "\n",
      "This filtration gives a collection of contained directed weighted graph or simplicial complex Kεmin ⊆\n",
      "... ⊆ Kεt ⊆ Kεt+1 ⊆ ... ⊆ Kεmax , where t ∈ [0,1] and εmin = 0, εmax = 1 (recall that edge weights are normalized).\n",
      "\n",
      "Given a filtration, one can look at the birth, when a homology class appears, and death, the time when the homology class disappears. The PH treats the birth and the death of these homological features in Kε for different ε values. Lifespan of each homological feature can be represented as an interval\n",
      "(birth,*death*), of the homological feature. For each filtration, we can record all these intervals by a Persistence Barcode (PB) Carlsson (2009), or in a Persistence Diagram (PD), as a collection of multiset of intervals.\n",
      "\n",
      "As mentioned previously, our interest in this work is to compare PDs from two different simplicial complexes. There are two distances traditionally used to compare PDs, Wasserstein distance and Bottleneck distance. Their stability with respect to perturbations on PDs has been object of different studies Chazal et al. (2012); Cohen-Steiner et al. (2005). As shown in comparative studies such as in Fasy et al. (2020), different distances and different ways of vectorising persistence diagrams have results with different levels of stability and quality. In order to make computations feasible and to obviate noisy intervals, we filter the PDs by limiting the minimum PD interval size. We do so by setting a minimum threshold η = 0.01. Intervals with a lifespan under this value are not considered. Additionally, for computing distances, we need to remove infinity values. As we are only interested in the deaths until the maximum weight value, we replace all the infinity values by 1.0.\n",
      "\n",
      "Wasserstein distance calculations are computationally hard for large PDs (each PD of our NN models has a million persistence intervals per diagram). Therefore we use a vectorized version of PDs instead, also called PD discretization. This vectorized version summaries have been proposed and used on recent literature Adams et al. (2017); Berry et al. (2020); Bubenik (2015); Lawson et al. (2019); Rieck et al. (2019a). For the persistence diagram distance calculation, we use the Giotto-TDA library Tauzin et al. (2020) and compute the following supported vectorized persistence summaries: 1. Persistence landscape. 2. Weighted silhouette. 3. Heat vectorizations.\n",
      "\n",
      "## 3.2 Experimental Framework\n",
      "\n",
      "To determine the topological structural properties of trained NNs, we select different kinds of datasets.\n",
      "\n",
      "We opt for four well-known benchmarks in the machine learning community and one regarding language identification: (1) the MNIST5 dataset for classifying handwritten digit images, (2) the Fashion MNIST Xiao et al. (2017) dataset for classifying clothing images into 10 categories, (3) the CIFAR-106(CIFAR) dataset for classifying 10 different objects, (4) the Reuters dataset for classifying news into 46 topics, and (5) the Language Identification Wikipedia dataset7for identifying 7 different languages. For CIFAR-10 and Fashion MNIST, we pre-train a Convolutional NN (CNN), and the convolutional layers are shared between all the models of the same dataset as a feature extractor.\n",
      "\n",
      "Recall that we are focusing on MLPs, so we do not consider that convolutional weights. For MNIST,\n",
      "Reuters and Language Identification, we use an MLP. For Reuters and Language identification datasets, we vectorize the sentences with character frequency.\n",
      "\n",
      "5http://yann.lecun.com/exdb/mnist/\n",
      "6https://www.cs.toronto.edu/~kriz/cifar.html 7https://www.floydhub.com/floydhub/datasets/language-identification/1/\n",
      "data\n",
      "\n",
      "| Number   | Experiment       | Index   |\n",
      "|----------|------------------|---------|\n",
      "| 1        | Layer size       | 1-4     |\n",
      "| 2        | Number of layers | 5-9     |\n",
      "| 3        | Input order      | 10-14   |\n",
      "| 4        | Number of labels | 15-19   |\n",
      "\n",
      "Table 1: Indices of the experiments of the distance matrices.\n",
      "We study the following variables (hyperparameters): 1. Layer width, 2. Number of layers, 3. Input order8 4. Number of labels (number of considered classes). We define the *base* architecture as the one with a layer width of 512, 2 layers, the original features order, and considering all the classes\n",
      "(10 in the case of MNIST, Fashion MNIST and CIFAR, 46 in the case of Reuters and 7 in the case of the language identification task). Then, doing one change at a time, keeping the rest of the base architecture hyperparameters, we experiment with architectures with the following configurations:\n",
      "1. **Layer width**: 128, 256, 512 (*base*) and 1024. 2. **Number of layers**: 2 (*base*), 4, 6, 8 and 10.\n",
      "\n",
      "3. **Input order**: 5 different randomizations (with *base* structure), the control experiment. 4. **Number**\n",
      "of labels (MNIST, Fashion MNIST, CIFAR-10): 2, 4, 6, 8 and 10 (*base*). 5. **Number of labels**\n",
      "(Reuters): 2, 6, 12, 23 and 46 (*base*). 6. **Number of labels** (Language Identification): 2, 3, 4, 6 and 7 (*base*). Note that this is not a grid search over all the combinations. We always modify one hyperparameter at a time, and keep the rest of them as in the base architecture. In other words, we experiment with all the combinations such that only one of the hyperparameters is set to a non-base value at a time. For each dataset, we train 5 times (each with a different random weight initialization)\n",
      "each of these neural network configurations. Then, we compute the topological distances (persistence landscape, weighted silhouette, heat) among the different architectures. In total, we obtain 5×5×3 distance matrices (5 datasets, 5 random initializations, 3 distance measures). Finally, we average the 5 random initializations, such that we get 5×3 matrices, one for each distance on each dataset.\n",
      "\n",
      "All the matrices have dimensions 19×19, since 19 is the number of experiments for each dataset\n",
      "(corresponding to the total the number of architectural configurations mentioned above). Note that the base architecture appears 8 times (1, on the number of neurons per layer, 1 on the number of layers, 1 on the number of labels and the 5 randomizations of weight initializations). All experiments were executed in a machine with 2 NVIDIA V100 of 32GB, 2 Intel(R) Xeon(R) Platinum 8176 CPU\n",
      "@ 2.10GHz, and of 1.5TB RAM, for a total of around 3 days.\n",
      "\n",
      "Note that this work focuses on MLPs, however, the method proposed is also applicable to CNNs, as they have a MLP equivalent.9 The resulting NN is a highly sparse simplicial complex with a large number of parameters which makes PH calculation out of our computation capability for all combination of proposed experiments.\n",
      "\n",
      "## 4 Results & Discussion\n",
      "\n",
      "Results from control experiments can be seen in the third group on Figures 1 and 4. In these figures, groups are separated visually using white dashed lines. Experiments groups are specified in Table 1.\n",
      "\n",
      "Control experiments in all the images appear very dimmed, which means that they are very similar, as expected. Recall that the control experiments consist of 5 (randomizations) × 5 (executions) and that 25 different neural networks have been trained; each one of the network has more than 690,000 parameters that have been randomly initialized. After the training, results show that these networks have very close topological distance, as expected.\n",
      "\n",
      "For Figure 2 we computed both 1-norm and Frobenius norm (the baselines) for graphs' adjacency matrices of control experiments. Note that as we ran the experiment five times, we make the mean for each value of the matrix. In order to show whether the resulting values are positive or negative, we subtract to the maximum difference of each dataset the norm of each cell separately, we take the absolute value and we divide by the maximum difference of each dataset. Therefore, we obtain five values per dataset. Table 2 shows the statistics reflecting that the distance among the experiments are large and, thus, they are not characterizing any similarity but rather an important dissimilarity.\n",
      "\n",
      "8Order of the input features, the control experiment.\n",
      "\n",
      "9https://aul12.me/machinelearning/2019/06/20/cnn-mlp-2.html\n",
      "\n",
      "![6_image_0.png](6_image_0.png)\n",
      "\n",
      "![6_image_1.png](6_image_1.png)\n",
      "\n",
      "(b) Language Identification 0.0 1.0\n",
      "\n",
      "![6_image_2.png](6_image_2.png)\n",
      "\n",
      "Figure 1: Distance matrices using Silhouette discretization.\n",
      "\n",
      "Figure 2: Control experiments using norms.\n",
      "\n",
      "| Norm      | Minimum   | Maximum   | Mean   | Standard deviation   |\n",
      "|-----------|-----------|-----------|--------|----------------------|\n",
      "| 1-Norm    | 0.6683    | 4.9159    | 1.9733 | 1.5693               |\n",
      "| Frobenius | 0.0670    | 0.9886    | 0.4514 | 0.3074               |\n",
      "\n",
      "Table 2: Normalized difference comparison of self-norm against the maximum mean distance of the experiment.\n",
      "\n",
      "![7_image_0.png](7_image_0.png)\n",
      "\n",
      "Figure 3: Control experiment comparison matrix using Silhouette discretization.\n",
      "\n",
      "| Heat distance   |        | Silhouette distance   |        |           |\n",
      "|-----------------|--------|-----------------------|--------|-----------|\n",
      "| Dataset         | Mean   | Deviation             | Mean   | Deviation |\n",
      "| MNIST           | 0.0291 | 0.0100                | 0.1115 | 0.0364    |\n",
      "| F. MNIST        | 0.0308 | 0.0132                | 0.0824 | 0.0353    |\n",
      "| CIFAR-10        | 0.0243 | 0.0068                | 0.0769 | 0.0204    |\n",
      "| Language I.     | 0.0159 | 0.0040                | 0.0699 | 0.0159    |\n",
      "| Reuters         | 0.0166 | 0.0051                | 0.0387 | 0.0112    |\n",
      "\n",
      "Table 3: PH distances across input order (control) experiments, normalized by dataset.\n",
      "In contrast, Figure 3, with our method (Silhouette), shows perfect diagonal of similarity blocks. In the corresponding numeric results, we obtained small distances, as shown in Table 3. We can appreciate that each dataset has its own hub. This confirms the validity of our proposed similarity measure.\n",
      "\n",
      "The method we present also seems to capture some parts of hyperparameter setup. For instance, in Figure 4 we can observe gradual increase of distances in the first group regarding layer size meaning that, as layer size increases, the topological distance increases too. Similarly, for the number of layers\n",
      "(second group) and number of labels (fourth group) the same situation holds. Note that in Fashion MNIST and CIFAR-10, the distances are dimmer because we are not dealing with the weights of the CNNs. Recall that the CNN acts as a frozen extractor and are pretrained for all runs (with the same weights), such that the MLP layers themselves are the only potential source of dissimilarity between runs. Thus, our characterization is sensitive to the architecture (e.g., if we increase the capacity, distances vary), but at the same time, as we saw before, it is not dataset-agnostic, meaning that it also captures whether two neural networks are learning the same problem or not.\n",
      "\n",
      "In Figure 4, Fashion MNIST (Figure 4b) and CIFAR (Figure 4c) dataset results are interestingly different from those of MNIST (Figure 4a) dataset. This is, presumably, because both Fashion MNIST and CIFAR use a pretrained CNN for the problem. Thus, we must analyze the results taking into account this perspective. The first fully connected layer size is important as it can avoid a bottleneck from the previous CNN output. Some works in the literature show that adding multiple fully connected layers does not necessarily enhance the prediction capability of CNNs Basha et al.\n",
      "\n",
      "(2019), which is congruent with our results when adding fully connected layers (experiments 5 to 9)\n",
      "that result in dimmer matrices than the one from. Concerning the experiments on input order, there is slightly more homogeneity than in MNIST, again showing that the order of sample has negligible influence. Moreover, there could have been even more homogeneity taking into account that the fully connected network reduced its variance thanks to the frozen weights of the CNN. This also supports the fact that the CNN is the main feature extractor of the network. As in MNIST results, CIFAR results show that the topological properties are, indeed, a mapping of the practical properties of neural networks. We refer to the Supplementary Material for all distance matrices for all datasets and all distances, as well as for the standard deviations matrices and experiment group statistics.\n",
      "\n",
      "![8_image_0.png](8_image_0.png)\n",
      "\n",
      "Figure 4: Distance matrices using Heat discretization.\n",
      "\n",
      "## 5 Conclusions & Future Work\n",
      "\n",
      "1.0\n",
      "\n",
      "![8_image_1.png](8_image_1.png)\n",
      "\n",
      "0.0\n",
      "\n",
      "Results in five different datasets from computer vision and natural language lead to similar topological properties and are interpretable, which yields to general applicability. The best discretizations found are Heat and Silhouette. They show better separation of experiment groups, and are effectively reflecting changes in a sensitive way (unlike Landscape discretization).\n",
      "\n",
      "The most remarkable conclusion comes from the control experiments. The corresponding neural networks, with different input order but the same architecture, are very close to each other.\n",
      "\n",
      "The PH framework does, indeed, abstract away the specific weight values, and captures latent information from the networks, allowing comparisons to be based on the function they approximate. The selected neural network representation is reliable and complete, and yields coherent and meaningful results. Instead, the baseline measures, the 1-Norm and the Frobenius norm, implied an important dissimilarity between the experiments in the control experiments, meaning that they did not capture the fact that these neural networks were very similar in terms of the solved problem.\n",
      "\n",
      "Our proposed characterization does, indeed, capture meaningful properties. To the best of our knowledge, our proposed similarity measure between neural networks is the first of its kind. As future work, we suggest adapting the method to architectures such as CNNs, RNNs, and Transformers\n",
      "(Vaswani et al., 2017). Finally, we suggest performing more analysis regarding the learning of a neural network from a topological point of view.\n",
      "\n",
      "Figure 5: Language Identification (Landscape).\n",
      "\n",
      "## References\n",
      "\n",
      "H. Adams, Tegan Emerson, M. Kirby, R. Neville, C. Peterson, P. Shipman, Sofya Chepushtanova, E. Hanson, F. Motta, and Lori Ziegelmeier. Persistence images: A stable vector representation of persistent homology. *J. Mach. Learn. Res.*, 18:8:1–8:35, 2017.\n",
      "\n",
      "M. Aktas, E. Akba¸s, and Ahmed El Fatmaoui. Persistence homology of networks: methods and applications. *Applied Network Science*, 4:1–28, 2019.\n",
      "\n",
      "Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer perceptrons by forward bipartite alignment. In *2015 International Joint Conference on Neural* Networks (IJCNN), pp. 1–7, 2015. doi: 10.1109/IJCNN.2015.7280769.\n",
      "\n",
      "S. H. Shabbeer Basha, Shiv Ram Dubey, Viswanath Pulabaigari, and Snehasis Mukherjee. Impact of fully connected layers on performance of convolutional neural networks for image classification.\n",
      "\n",
      "CoRR, abs/1902.02771, 2019. URL http://arxiv.org/abs/1902.02771.\n",
      "\n",
      "Ulrich Bauer. Ripser: efficient computation of vietoris-rips persistence barcodes, 2021.\n",
      "\n",
      "E. Berry, Yen-Chi Chen, J. Cisewski-Kehe, and Brittany Terese Fasy. Functional summaries of persistence diagrams. *Journal of Applied and Computational Topology*, 4:211–262, 2020.\n",
      "\n",
      "Peter Bubenik. Statistical topological data analysis using persistence landscapes. *J. Mach. Learn.*\n",
      "Res., 16:77–102, 2015.\n",
      "\n",
      "G. Carlsson. Topology and data. *Bulletin of the American Mathematical Society*, 46:255–308, 2009.\n",
      "\n",
      "F. Chazal, V. D. Silva, and S. Oudot. Persistence stability for geometric complexes. *Geometriae* Dedicata, 173:193–214, 2012.\n",
      "\n",
      "Samir Chowdhury, T. Gebhart, Steve Huntsman, and Matvey Yutin. Path homologies of deep feedforward networks. *2019 18th IEEE International Conference On Machine Learning And* Applications (ICMLA), pp. 1077–1082, 2019.\n",
      "\n",
      "J. Clough, I. Öksüz, Nicholas Byrne, V. Zimmer, J. A. Schnabel, and A. P. King. A topological loss function for deep-learning based image segmentation using persistent homology. IEEE transactions on pattern analysis and machine intelligence, PP, 2020.\n",
      "\n",
      "D. Cohen-Steiner, H. Edelsbrunner, and J. Harer. Stability of persistence diagrams. *Proceedings of* the twenty-first annual symposium on Computational geometry, 2005.\n",
      "\n",
      "C. Corneanu, M. Madadi, S. Escalera, and A. Martínez. Computing the testing error without a testing set. *2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp.\n",
      "\n",
      "2674–2682, 2020.\n",
      "\n",
      "Jonathan Donier. Capacity allocation analysis of neural networks: A tool for principled architecture design. *ArXiv*, abs/1902.04485, 2019.\n",
      "\n",
      "H. Edelsbrunner and J. Harer. *Computational Topology - an Introduction*. American Mathematical Society, 2009.\n",
      "\n",
      "Brittany Fasy, Yu Qin, Brian Summa, and Carola Wenk. Comparing distance metrics on vectorized persistence summaries. In *NeurIPS 2020 Workshop on Topological Data Analysis and Beyond*,\n",
      "2020.\n",
      "\n",
      "T. Gebhart, Paul Schrater, and A. Hylton. Characterizing the shape of activation space in deep neural networks. 2019 18th IEEE International Conference On Machine Learning And Applications\n",
      "(ICMLA), pp. 1537–1542, 2019.\n",
      "\n",
      "R. Ghrist. *Elementary Applied Topology*. Self-published, 2014.\n",
      "\n",
      "P. Giblin. *Graphs, surfaces, and homology : an introduction to algebraic topology*. Chapman and Hall, 1977.\n",
      "\n",
      "William H. Guss and R. Salakhutdinov. On characterizing the capacity of neural networks using algebraic topology. *ArXiv*, abs/1802.04443, 2018.\n",
      "\n",
      "C. Hofer, Florian Graf, M. Niethammer, and R. Kwitt. Topologically densified distributions. *ArXiv*,\n",
      "abs/2002.04805, 2020.\n",
      "\n",
      "J. Jonsson. *Simplicial complexes of graphs*. PhD thesis, KTH Royal Institute of Technology, 2007.\n",
      "\n",
      "Emir Konuk and K. Smith. An empirical study of the relation between network architecture and complexity. *2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)*,\n",
      "pp. 4597–4599, 2019.\n",
      "\n",
      "Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. *CoRR*, abs/1905.00414, 2019. URL http://arxiv.org/ abs/1905.00414.\n",
      "\n",
      "P. Lawson, A. Sholl, J. Brown, Brittany Terese Fasy, and C. Wenk. Persistent homology for the quantitative evaluation of architectural features in prostate cancer histology. *Scientific Reports*, 9, 2019.\n",
      "\n",
      "Daniel Lütgehetmann, Dejan Govc, Jason Smith, and R. Levi. Computing persistent homology of directed flag complexes. *arXiv: Algebraic Topology*, 2019.\n",
      "\n",
      "Gregory Naitzat, A. Zhitnikov, and L. Lim. Topology of deep neural networks. *J. Mach. Learn. Res.*,\n",
      "21:184:1–184:40, 2020.\n",
      "\n",
      "K. Ramamurthy, Kush R. Varshney, and Krishnan Mody. Topological data analysis of decision boundaries with application to model selection. *ArXiv*, abs/1805.09949, 2019.\n",
      "\n",
      "Bastian Alexander Rieck, F. Sadlo, and H. Leitte. Topological machine learning with persistence indicator functions. *ArXiv*, abs/1907.13496, 2019a.\n",
      "\n",
      "Bastian Alexander Rieck, Matteo Togninalli, C. Bock, Michael Moor, Max Horn, Thomas Gumbsch, and K. Borgwardt. Neural persistence: A complexity measure for deep neural networks using algebraic topology. *ArXiv*, abs/1812.09764, 2019b.\n",
      "\n",
      "Guillaume Tauzin, Umberto Lupo, Lewis Tunstall, Julian Burella Pérez, Matteo Caorsi, Anibal Medina-Mardones, Alberto Dassatti, and Kathryn Hess. giotto-tda: A topological data analysis toolkit for machine learning and data exploration, 2020.\n",
      "\n",
      "Christopher Tralie, Nathaniel Saul, and Rann Bar-On. Ripser.py: A lean persistent homology library for python. *The Journal of Open Source Software*, 3(29):925, Sep 2018. doi: 10.21105/joss.00925.\n",
      "\n",
      "URL https://doi.org/10.21105/joss.00925.\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. *CoRR*, abs/1706.03762, 2017. URL\n",
      "http://arxiv.org/abs/1706.03762.\n",
      "\n",
      "Joshua T. Vogelstein, John M. Conroy, Vince Lyzinski, Louis J. Podrazik, Steven G. Kratzer, Eric T.\n",
      "\n",
      "Harley, Donniell E. Fishkind, R. Jacob Vogelstein, and Carey E. Priebe. Fast approximate quadratic programming for graph matching. *PLOS ONE*, 10(4):1–17, 04 2015. doi: 10.1371/journal.pone.\n",
      "\n",
      "0121002. URL https://doi.org/10.1371/journal.pone.0121002.\n",
      "\n",
      "S. Watanabe and Hayato Yamana. Topological measurement of deep neural networks using persistent homology. In *ISAIM*, 2020.\n",
      "\n",
      "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n",
      "\n",
      "S. Zhang, Mengbai Xiao, and H. Wang. Gpu-accelerated computation of vietoris-rips persistence barcodes. In *Symposium on Computational Geometry*, 2020.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_reviewmt_file_train():\n",
    "    with open(\"data/reviewmt_train/00.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "data = load_reviewmt_file_train()\n",
    "\n",
    "example_train = data[0]\n",
    "print(\"Instruction:\", example_train[\"instruction\"])\n",
    "print(\"Input:\", example_train[\"input\"])\n",
    "\n",
    "example = data[100]\n",
    "print(\"Instruction:\", example_train[\"instruction\"])\n",
    "print(\"Input:\", example_train[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d1f89b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mexample_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 'Summary'"
     ]
    }
   ],
   "source": [
    "print(\"Test\", example_train[\"Summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09d348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f279a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2000\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "print(\"Number of examples:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c80d4cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: You are a decision maker. Please review all responses from the author and comments from all reviewers to provide the meta-review and determine the final decision. Explicitly state 'Accept' or 'Reject' at the end of your output.\n",
      "Input: Title: The Infinite Contextual Graph Markov Model. Abstract: The Contextual Graph Markov Model is a deep, unsupervised, and probabilistic model for graphs that is trained incrementally on a layer-by-layer basis. As with most Deep Graph Networks, an inherent limitation is the lack of an automatic mechanism to choose the size of each layer's latent representation. In this paper, we circumvent the problem by extending the Contextual Graph Markov Model with Hierarchical Dirichlet Processes. The resulting model for graphs can automatically adjust the complexity of each layer without the need to perform an extensive model selection. To improve the scalability of the method, we introduce a novel approximated inference procedure that better deals with larger graph topologies. The quality of the learned unsupervised representations is then evaluated across a set of eight graph classification tasks, showing competitive performances against end-to-end supervised methods. The analysis is complemented by studies on the importance of depth, hyper-parameters, and compression of the graph embeddings. We believe this to be an important step towards the theoretically grounded and automatic construction of deep probabilistic architectures for graphs. Main Text: introduce a novel approximated inference procedure that better deals with larger graph topologies.\n",
      "\n",
      "The quality of the learned unsupervised representations is then evaluated across a set of eight graph classification tasks, showing competitive performances against end-to-end supervised methods. The analysis is complemented by studies on the importance of depth, hyper-parameters, and compression of the graph embeddings.\n",
      "\n",
      "We believe this to be an important step towards the theoretically grounded and automatic construction of deep probabilistic architectures for graphs.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "It can be argued that one of the most daunting processes in machine learning is the selection of appropriate hyper-parameters for the task at hand. Indeed, due to the data-dependent nature of the learning problem, there usually exists no single model configuration that works well in all contexts.\n",
      "\n",
      "The most straightforward approach to mitigate this issue has typically been to rely on standard model selection techniques such as grid and random searches (Bergstra & Bengio, 2012), where the range of values to try are fixed a priori by the user. Nonetheless, there has always been an interest in alternative methods that automatically choliteratureose the \"right\" values for some hyper-parameters\n",
      "(Gershman & Blei, 2012; He et al., 2021). In the Bayesian nonparametric (BNP) literature, which is of particular interest for this work, the complexity of Bayesian models automatically grows with the data (Teh et al., 2006), e.g., a BNP mixture model can adjust the number of its mixtures to better fit the empirical data distribution, thus freeing the user from the burden of choosing the most important\n",
      "(if not all) hyper-parameters.\n",
      "\n",
      "In recent years, much research effort has been devoted to the theoretical and practical development of Deep Graph Networks (DGNs), which originated from Micheli (2009); Scarselli et al. (2009). DGNs can deal with graphs of varying topology without the need for human intervention, and they rely on local and iterative processing of information commonly known as *message passing*; for a thorough description of some of the most popular DGNs in the literature (and of the more general graph representation learning field) we refer the reader to recent surveys on the topic (Bronstein et al.,\n",
      "2017; Battaglia et al., 2018; Bacciu et al., 2020b; Wu et al., 2020). Despite most of these methods belonging to the neural world, the Contextual Graph Markov Model (CGMM) stands out as a deep, unsupervised, constructive and fully probabilistic model that has shown competitive performances on downstream graph classification tasks (Bacciu et al., 2018; 2020a). CGMM trains a stack of Bayesian networks, where each layer is conditioned on the *frozen* posteriors of the nodes of the graph computed at previous layers. Each layer optimizes the likelihood of the data using the Expectation Maximization (EM) algorithm (Moon, 1996) with closed-form solutions. Like its neural counterparts, for which the number of hidden units in each layer has typically been selected as a hyper-parameter, CGMM relies on model selection to choose the \"reasonable\" number of hidden states associated with the categorical latent variables. Differently from the neural methods though, CGMM is amenable to a BNP extension, as each layer is essentially a conditional mixture model.\n",
      "\n",
      "1 The main challenge we tackle in this work is the adaptation of CGMM to the elegant theoretical framework of BNP methods, in order to automatize the choice of its hyper-parameters, e.g., the number of states. The principal difficulty lies in how to handle the variable-size number of neighbors of each node inside this framework, which in CGMM is solved by (possibly weighted) convex combinations of the neighbors' posteriors. The resulting model, called Infinite Contextual Graph Markov Model\n",
      "(ICGMM), can generate as many latent states as needed to solve the unsupervised density estimation task at each layer. To the extent of our knowledge, this is the first Bayesian nonparametric model for adaptive graph processing. As a second contribution, we provide a faster implementation of our method that scales to the social datasets considered in this work while still providing state of the art results. We compare ICGMM against CGMM as well as end-to-end supervised methods on eight different graph classification tasks, following a fair, robust and reproducible experimental procedure\n",
      "(Errica et al., 2020). Results show that ICGMM performs on par or better than the related models.\n",
      "\n",
      "We complement the analysis with studies on the effects of depth and generation of our model's latent states. All in all, we believe that ICGMM is an important (if not the first) step towards a theoretically grounded and automatic construction of Deep Bayesian Graph Networks.\n",
      "\n",
      "## 2 Related Works\n",
      "\n",
      "The fundamental Bayesian nonparametric literature that is relevant to our work relates to the families of Dirichlet Processes (DPs) (Gershman & Blei, 2012) and Hierarchical Dirichlet Processes (HDPs)\n",
      "(Teh et al., 2006). In its most essential definition, a DP is a stochastic process that defines a probability distribution over other probability distributions. A DP is parametrized by a base distribution G0, i.e., the expected value of the process, and a scaling parameter α0 that controls the concentration of DP realizations around G0 (Teh, 2010). In particular, the Chinese Restaurant Process (Aldous, 1985),\n",
      "the Stick-breaking Construction (Sethuraman, 1994) and the Pòlya urn scheme (Hoppe, 1984) are all alternative ways to formalize a DP. Moving to HDPs is conceptually straightforward, in that it considers the base distribution G0 as a draw from another DP parametrized by a base distribution H and a scaling parameter γ. For a detailed treatment of learning with DP and HDPs, the reader can check a number of tutorials and surveys (Teh et al., 2006; Orbanz & Teh, 2010; Gershman &\n",
      "Blei, 2012). Our work shares similarities with the Infinite Hidden Markov Model for temporal series\n",
      "(Beal et al., 2002), with the fundamental differences that causality assumptions have to be relaxed to deal with graphs and that the hidden variables' distributions are conditioned on a varying number of observations.\n",
      "\n",
      "Most of the recent advances of the graph representation learning field are based on the so called feedforward DGNs (Bacciu et al., 2020b). These models rely on \"spatial\" graph convolutional layers, i.e., the state of each node in the graph is determined by applying a permutation invariant function to its neighboring states computed at the previous layers. Combined with the depth of the architecture, these models propagate contextual information across the graph, a process also known as \"message passing\" (Gilmer et al., 2017). However, to the best of our knowledge, the only neural method for graphs that automatically constructs part of its architecture in a principled way is the pioonering work of Micheli (2009). In fact, the Neural Network for Graphs (NN4G), known to be the first spatial DGN, relies on the Cascade Correlation learning algorithm (Fahlman & Lebiere, 1990) to determine the number of layers to use for the task under investigation.\n",
      "\n",
      "Instead, despite being loosely related to our work, AutoML methods for graphs are yet another way to automatize the selection of all hyper-parameters of a DGN (He et al., 2021). In particular, the Auto-GNN technique relies on Neural Architecture Search to discover, based on performance trends, an adequate configuration for the supervised task (Zhou et al., 2019). We differ from these approaches in two fundamental respects: first, we build upon theoretical grounds rooted in the BNP literature; secondly, we determine the right number of states in a completely unsupervised fashion.\n",
      "\n",
      "In what follows, we provide a formalization of the Infinite Contextual Graph Markov Model. Apart from the technical details, our hope is to show how the cross-fertilization of ideas from different research fields can help us advance the state of the art, both in the theoretical and empirical sense.\n",
      "\n",
      "## 3 Method\n",
      "\n",
      "This Section introduces the details of our method. Since we borrow ideas from two relatively distant fields, we define a unified mathematical notation and jargon as well as a high-level overview of the CGMM and HDP models to ease the subsequent exposition.\n",
      "\n",
      "We define a graph as a tuple g = (Vg, Eg, Xg) where Vg is the set of entities (also referred to as nodes or vertices), Eg is the set of oriented edges (*u, v*) connecting node u to v, and the symbol Xg stands for the set of node attributes associated with the graph g. Also, the neighborhood of a node u is the set of nodes connected to u, i.e., Nu = {v ∈ Vg|(v, u) ∈ Eg}. For the purpose of this work, we will define the (categorical or continuous) node feature of a node u with the term xu ∈ Xg.\n",
      "\n",
      "## 3.1 Basics Of Cgmm\n",
      "\n",
      "To best understand how and why this work extends CGMM, we now give a brief but essential description of its main characteristics. CGMM is, first and foremost, a deep architecture for the adaptive processing of graphs. Like other DGNs, it maps the entities of a graph, if not the graph itself, into latent representations. More specifically, we can get one of such representations for each layer of the architecture and then concatenate all of them to obtain richer node and graph embeddings. The latter is usually obtained as a global aggregation of the former.\n",
      "\n",
      "The second peculiarity of CGMM is that it is constructive, i.e., trained in an incremental fashion:\n",
      "after one layer is trained, another one can be stacked atop of it and trained using the frozen outputs of the previous layer. This idea is borrowed from NN4G (Micheli, 2009), and it allows CGMM to relax the mutual dependencies between latent variables in a cyclic graph. However, because the local and iterative message passing mechanism used by spatial methods (Micheli, 2009; Kipf & Welling, 2017)\n",
      "is responsible for information propagation across the graph, this relaxation is not restrictive.\n",
      "\n",
      "Thirdly, the node/graph embedding construction is fully probabilistic and unsupervised, since layer\n",
      "` is represented as the Bayesian network on the left hand-side of Figure 1. A latent variable q\n",
      "`\n",
      "u is attached to each node u, and it is responsible for the the generation of the node feature xu. To take into account structural information, the hidden state q\n",
      "`\n",
      "u is conditioned on the neighboring hidden states computed at the previous layer, i.e., the set {q\n",
      "`−1 v| v ∈ Nu}. Importantly, the constructive approach allows us to treat the hidden (frozen) states of the previous layer as observable variables. Each layer is trained to fit the data distribution of node features using the EM algorithm, thus guaranteeing the convergence to a local minima. Once inference is performed, the state of each node is frozen and we can move to the subsequent layer. Lastly, the embedding of each node at layer ` is encoded as the posterior of its hidden state.\n",
      "\n",
      "## 3.2 Basics Of Hdp\n",
      "\n",
      "The HDP is a Bayesian nonparametric prior for the generation of grouped data using different infinite mixture models with shared mixture components. Let {x1, x2*, . . .* } be a set of observations that are grouped into J groups, i.e., each observation xu belongs to the group ju ∈ {1*, . . . , J*}. Using the stick-breaking representation (Sethuraman, 1994), the HDP mixture model that generates the observations can be defined as (Teh et al., 2006):\n",
      "\n",
      "(1)  $\\frac{1}{2}$ ................................. \n",
      "$\\beta\\mid\\gamma\\sim\\mbox{Stick}(\\gamma)$$q_{u}\\mid j_{u},(\\pi_{j})_{j=1}^{J}\\sim\\pi_{j_{u}}$$\\pi_{j}\\mid\\beta,\\alpha_{0}\\sim\\mbox{DP}(\\alpha_{0},\\beta)$$x_{u}\\mid q_{u},(\\theta_{c})_{c=1}^{\\infty}\\sim F(\\theta_{q_{u}})$$\\theta\\mid H\\sim H$,\n",
      "where F(θqu\n",
      ") denotes the emission distribution, parametrized by θqu\n",
      ", that generates the observation xu. The latent state qu indicates which mixture component should be used to generate xu. The value of qu is sampled from the distribution πju\n",
      ", which stands for the mixture weights of group ju. All\n",
      "(πj )\n",
      "J\n",
      "j=1 are obtained from a DP with concentration parameter α0 and base distribution β. Notably, all groups' mixture weights are defined on the same set of mixture components, meaning there is a form of parameter sharing across different groups. Finally, we sample the distribution β via the stick-breaking process Stick(γ) of Sethuraman (1994).\n",
      "\n",
      "To generate a possibly infinite number of emission distributions, we exploit a prior distribution H that allows us to create new mixture components on demand. Thanks to the stick-breaking construction,\n",
      "\n",
      "![3_image_0.png](3_image_0.png)\n",
      "\n",
      "Figure 1: Differences between layer `'s graphical model of the original CGMM and the proposed ICGMM. Observable variables are blue circles, latent ones are empty circles, and white boxes denote prior knowledge. Each ICGMM is an HDP mixture model where the group j for each node observation xu is pre-determined by the set of states of neighboring nodes q\n",
      "`−1 Nu computed at layer\n",
      "` − 1. Contrarily to CGMM, the number of values that the latent indicator variable qu can assume is automatically adjusted to fit the underlying data distribution. Dashed arrows denote the flow of contextual information from previous layers through the neighbors of each node u.\n",
      "\n",
      "even though an infinite number of mixture components can be used, only a finite number of them is istantiated during the inference phase. Hereinafter, we indicate with the symbol C the number of mixture components that are chosen by the HDP at inference time.\n",
      "\n",
      "## 3.3 Model Definition\n",
      "\n",
      "Architecturally speaking, ICGMM shares the same characteristics of CGMM described in Section 3.1, whereas the differences of each layer's graphical model are highlighted in Figure 1. In particular, ICGMM assumes that the generation of the node features xu at each layer is governed by a HDP\n",
      "mixture model. Thus, following the stick-breaking construction detailed in Section 3.2, the generative process of a single ICGMM layer ` can be formalized as follows:\n",
      "\n",
      "$\\beta^{\\ell}\\mid\\gamma^{\\ell}\\sim\\text{Stick}(\\gamma^{\\ell})$$j_{u}^{\\ell}\\mid\\mathbf{q}_{\\mathsf{N}_{u}}^{\\ell-1}=\\psi(\\mathbf{q}_{\\mathsf{N}_{u}}^{\\ell-1})$$\\pi_{j}^{\\ell}\\mid\\beta^{\\ell},\\alpha_{0}^{\\ell}\\sim\\text{DP}(\\alpha_{0}^{\\ell},\\beta^{\\ell})$$q_{u}^{\\ell}\\mid j_{u}^{\\ell},(\\pi_{j})_{j=1}^{C^{\\ell-1}}\\sim\\pi_{j_{u}}^{\\ell}$$\\theta^{\\ell}\\mid\\mathbf{H}\\sim\\mathbf{H}$$x_{u}\\mid q_{u}^{\\ell},(\\theta_{c}^{\\ell})_{c=1}^{\\infty}\\sim F(\\mathbf{\\theta}_{q_{u}}^{\\ell})$,\n",
      "$${\\mathrm{(2)}}$$\n",
      "where we add the superscript ` to the HDP mixture model quantities to highlight that they are different at each ICGMM layer. Similarly to the HDP case, we use C\n",
      "`to denote the number of states chosen by the model at the current layer. When clear from the context, we will omit such a superscript to ease the notation.\n",
      "\n",
      "In any HDP mixture model, each observation must be assigned to a group *in advance*. In this work, the assignment of a node feature to a group is not known a priori, but that is the key to propagate contextual information across the graph. We select the group j\n",
      "`\n",
      "u of the feature node xu based on the neighbors' observable posteriors computed at the previous layer, i.e., q\n",
      "`−1 Nu\n",
      "= {q\n",
      "`−1 v ∈ [0, 1]C\n",
      "l−1| v ∈ Nu}.\n",
      "\n",
      "However, due to known intractability issues, each posterior distribution q\n",
      "`−1 vis approximated by the inference phase of the previous layer (see Section 3.4). To stick as much as possible to the original CGMM formalism, ju is chosen as the most likely position in the C\n",
      "`−1-sized macrostate, obtained by averaging the neighbors' probabilities in q\n",
      "`−1 Nu\n",
      ":\n",
      "\n",
      "$$j_{u}^{\\ell}=\\psi({\\bf q}_{{\\mathcal{N}}_{u}}^{\\ell-1})=\\operatorname*{arg\\,max}_{j\\in\\{1,\\ldots,C^{\\ell-1}\\}}\\left({\\frac{1}{|{\\mathcal{N}}_{u}|}}\\sum_{v\\in{\\mathcal{N}}_{u}}{\\boldsymbol{q}}_{v}^{\\ell-1}\\right)_{j}.$$\n",
      "$$({\\mathcal{I}})$$\n",
      "\n",
      "It follows that nodes with the same feature may have a different latent state c, due to the fact that they are assigned to different groups, i.e., different πj , on the basis of their neighborhood; this mimics the role of CGMM neighborhood aggregation but in an HDP mixture model. In the first layer, where no previous layer exists, we shall just assume that all node features belong to the same group.\n",
      "\n",
      "Summing up, we depart from the basic CGMM layer of Bacciu et al. (2020a) in more than one way. First and foremost, we do not parametrize nor learn the CGMM transition distribution, which was responsible for the convex combination of neighboring states when computing the E-step of the EM algorithm. Instead, we rely on the most probable choice of the group ju that is encoded by the neighbors' macrostate. Secondly, due to the sheer complexity of the Bayesian nonparametric treatment, we do not train the model via EM as done with CGMM; instead, we will exploit Gibbs sampling (Geman & Geman, 1984) to compute the quantities of interest. Finally, ICGMM retains one important architectural characteristic of CGMM, i.e., it prevents vanishing gradient effects and over-smoothing by default (Bacciu et al., 2020a), thus allowing us to construct deeper architectures that propagate contextual information.\n",
      "\n",
      "## 3.4 Inference\n",
      "\n",
      "The inference phase of every BNP method is meant to estimate the posterior of the model parameters.\n",
      "\n",
      "For each ICGMM layer `, we wish to compute the quantities q\n",
      "`\n",
      "u\n",
      ", β\n",
      "`,π\n",
      "`\n",
      "j and θ\n",
      "`. Thanks to the incremental construction of the ICGMM architecture, we can do so one layer at a time. It is worth mentioning that the constructive approach of CGMM is not an approximation of a more complex graphical model, rather it is a design choice that applies the basic principle of iterative computation underpinning all DGNs. Thus, since each ICGMM layer is an HDP mixture model, we can infer its parameters following the Gibbs sampling schema of Teh et al. (2006). Note that it is also possible to estimate the hyper-parameters α\n",
      "`\n",
      "0 and γ\n",
      "`: whenever that is the case, we shall append a subscript\n",
      "\"auto\" to our model's name. In the interest of space, we report the ICGMM complete Gibbs sampling equations and pseudo-code in Appendix A and B, respectively.\n",
      "\n",
      "Graph Embedding Generation. In a similar vein with (Bacciu et al., 2020a), we prefer to use the sample distribution of qu (Eq. 4) at the last iteration, rather than the last sampled state, as an approximation of node u's posterior distribution. This way, we encode more information about state occupancy into node/graph embeddings.\n",
      "\n",
      "As in Bacciu et al. (2020a), node embeddings of each layer are represented as *unibigrams*. A\n",
      "unibigram concatenates the posterior of a node, i.e., a vector called *unigram*, with its *bigram*. A\n",
      "bigram counts, for each possible state qu, how many of u's neighbors are in another state, and it is represented as a vector of size C\n",
      "2. The final graph representation that is fed into the classifier is obtained by concatenation of node unibigrams across all layers followed by global aggregation.\n",
      "\n",
      "Faster Inference with Node Batches (I**CGMM**f ). Due to the sequential nature of the Gibbs sampling procedure, a naive implementation is slow when applied to the larger social graphs considered in this work. In the literature, there exist several exact distributed inference methods for HDP (Lovell et al., 2012; Williamson et al., 2013; Chang & Fisher III, 2014; Ge et al., 2015), but their effectiveness might be limited due to the unbalanced workload among workers or the elevated rejection rate (Gal &\n",
      "Ghahramani, 2014).\n",
      "\n",
      "In this work, we prefer to speed-up the inference procedure by introducing an approximation rather than relying on an exact distributed computation. As suggested in Gal & Ghahramani (2014), an approximated inference procedure may indeed suffice for many problems. What we propose is based on a straightforward idea, which is to perform sampling for a batch of node observations in parallel.\n",
      "\n",
      "This way, the necessary statistics are updated in batch rather than individually, and matrix operations can be used to gain efficiency. To maintain a good trade-off between the quality and speedup, we stick to 1 graph as the size of our batch. Such a trade-off provides a CPU speedup of up to 60× at training time, and we empirically observed that performances remain unchanged with respect to the original version on the smaller chemical tasks considered. While this faster version of ICGMM, which we call ICGMMf , does not strictly adhere to the technical specifications of the previous section, we believe that the pros largely outperform the cons. The interested reader can refer to Appendix D for an analysis of the speedup gains on the different datasets.\n",
      "\n",
      "## 3.5 Limitations\n",
      "\n",
      "Due to the complexity of the BNP treatment, one limitation of this work is that naive Gibbs sampling does not scale to very large datasets. The node independence assumption made by CGMM enables a faster batch computation, which can also be run on GPU. Despite having provided a simple, but approximated, sampling process that guarantees a substantial speedup and allows us to process graphs of non-negligible size, it would be interesting in the future to explore other inference methods to increase ICGMM's speedup, e.g., variational inference (Bryant & Sudderth, 2012; Wang & Blei, 2012; Hoffman et al., 2013; Hughes et al., 2015). The second limitation of ICGMM is that edge features are not taken into account. While there exist many neural models that do the same, we know that CGMM and its variant E-CGMM (Atzeni et al., 2021) can deal with discrete and arbitrary features, respectively. Our research directions for the future will investigate these aspects, providing an exact and efficient version of ICGMM that can process edge features as well.\n",
      "\n",
      "## 4 Experiments\n",
      "\n",
      "We evaluated the performances of ICGMM using the fair, robust, and reproducible evaluation setup for graph classification defined in Errica et al. (2020). It consists of an external 10-fold cross validation for model assessment, followed by an internal hold-out model selection for each of the external folds. Stratified data splits were already provided; in this respect, we had to re-assess CGMM and E-CGMM (Atzeni et al., 2021), a recently proposed variant, by trying all the hyper-parameters specified in the original papers (in particular, the values of C tried were 5, 10 and 20). We first experiment on the three chemical datasets D&D (Dobson & Doig, 2003), NCI1 (Wale et al., 2008)\n",
      "and PROTEINS (Borgwardt et al., 2005), where node features represent atom types. Then, we consider social datasets, including IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDITMULTI-5K, and COLLAB (Yanardag & Vishwanathan, 2015), where the degree of each node is the sole continuous feature available. All datasets are publicly available (Kersting et al., 2016) and their statistics are summarized in Appendix C. Finally, we relied on Pytorch Geometric (Fey & Lenssen, 2019) for the implementation of our method.1 Apart from CGMM's variants, we will compare ICGMM against the following end-to-end supervised neural architectures for graphs: DGCNN (Zhang et al., 2018), DIFFPOOL (Ying et al., 2018), ECC\n",
      "(Simonovsky & Komodakis, 2017), GIN (Xu et al., 2019), GRAPHSAGE (Hamilton et al., 2017),\n",
      "and a structure-agnostic baseline method BASELINE, described in Errica et al. (2020), which was competitive on a number of benchmarks. We recall that these supervised methods construct the graph embeddings leveraging the supervision information coming from the target label; on the contrary, ICGMM embeddings are built in an unsupervised and constructive way, thus this represents a challenging comparison for our approach. Results for the supervised models are taken from (Errica et al., 2020).\n",
      "\n",
      "We have discussed how ICGMM can automatize the choice its hyper-parameters, e.g., the size of the latent representation. In general, the choice of the Bayesian hyper-parameters is much less important than that of the number of states C, as in principle one can recursively introduce hyper-priors over these hyper-parameters (Bernardo & Smith, 2009; Goel & Degroot, 1981). That said, being this the first work to study HDP methods in the context of graph classification, we both i) explored the hyper-parameter space to best assess and characterize the behaviour of the model and ii) introduced hyper-priors to estimate α\n",
      "`\n",
      "0 and γ\n",
      "`at each layer, thus further reducing the need for an extensive model selection.\n",
      "\n",
      "For the chemical tasks, the prior H over the emission parameters θc was the uniform Dirichlet distribution. The range of ICGMM hyper-parameters tried in this case were: number of layers 1The code to rigorously reproduce our results is provided in the supplementary material.\n",
      "\n",
      "Table 1: Results on chemical datasets (mean accuracy and standard deviation) are shown. The best performances are highlighted in bold.\n",
      "\n",
      "| D&D        | NCI1       | PROTEINS   |            |\n",
      "|------------|------------|------------|------------|\n",
      "| BASELINE   | 78.4 ± 4.5 | 69.8 ± 2.2 | 75.8 ± 3.7 |\n",
      "| DGCNN      | 76.6 ± 4.3 | 76.4 ± 1.7 | 72.9 ± 3.5 |\n",
      "| DIFFPOOL   | 75.0 ± 3.5 | 76.9 ± 1.9 | 73.7 ± 3.5 |\n",
      "| ECC        | 72.6 ± 4.1 | 76.2 ± 1.4 | 72.3 ± 3.4 |\n",
      "| GIN        | 75.3 ± 2.9 | 80.0 ± 1.4 | 73.3 ± 4.0 |\n",
      "| GRAPHSAGE  | 72.9 ± 2.0 | 76.0 ± 1.8 | 73.0 ± 4.5 |\n",
      "| CGMM       | 74.9 ± 3.4 | 76.2 ± 2.0 | 74.0 ± 3.9 |\n",
      "| E-CGMM     | 73.9 ± 4.1 | 78.5 ± 1.7 | 73.3 ± 4.1 |\n",
      "| ICGMM      | 75.6 ± 4.3 | 76.5 ± 1.8 | 72.7 ± 3.4 |\n",
      "| ICGMMf     | 75.0 ± 5.6 | 76.7 ± 1.7 | 73.3 ± 2.9 |\n",
      "| ICGMMauto  | 76.3 ± 5.6 | 77.6 ± 1.5 | 73.1 ± 3.9 |\n",
      "| ICGMMfauto | 75.1 ± 3.8 | 76.4 ± 1.4 | 73.2 ± 3.9 |\n",
      "\n",
      "| IMDB-B     | IMDB-M     | REDDIT-B   | REDDIT-5K   | COLLAB     |             |\n",
      "|------------|------------|------------|-------------|------------|-------------|\n",
      "| BASELINE   | 70.8 ± 5.0 | 49.1 ± 3.5 | 82.2 ± 3.0  | 52.2 ± 1.5 | 70.2 ± 1.5  |\n",
      "| DGCNN      | 69.2 ± 3.0 | 45.6 ± 3.4 | 87.8 ± 2.5  | 49.2 ± 1.2 | 71.2 ± 1.9  |\n",
      "| DIFFPOOL   | 68.4 ± 3.3 | 45.6 ± 3.4 | 89.1 ± 1.6  | 53.8 ± 1.4 | 68.9 ± 2.0  |\n",
      "| ECC        | 67.7 ± 2.8 | 43.5 ± 3.1 | -           | -          | -           |\n",
      "| GIN        | 71.2 ± 3.9 | 48.5 ± 3.3 | 89.9 ± 1.9  | 56.1 ± 1.7 | 75.6 ± 2.3  |\n",
      "| GRAPHSAGE  | 68.8 ± 4.5 | 47.6 ± 3.5 | 84.3 ± 1.9  | 50.0 ± 1.3 | 73.9 ± 1.7  |\n",
      "| CGMM       | 72.7 ± 3.6 | 47.5 ± 3.9 | 88.1 ± 1.9  | 52.4 ± 2.2 | 77.32 ± 2.2 |\n",
      "| E-CGMM     | 70.7 ± 3.8 | 48.3 ± 4.1 | 89.5 ± 1.3  | 53.7 ± 1.0 | 77.45 ± 2.3 |\n",
      "| ICGMMf     | 73.0 ± 4.3 | 48.6 ± 3.4 | 91.3 ± 1.8  | 55.5 ± 1.9 | 78.6 ± 2.8  |\n",
      "| ICGMMfauto | 71.8 ± 4.4 | 49.0 ± 3.8 | 91.6 ± 2.1  | 55.6 ± 1.7 | 78.9 ± 1.7  |\n",
      "\n",
      "Table 2: Results on social datasets (mean accuracy and standard deviation) are shown, where the node degree is used as the only node feature. The best performances are highlighted in bold.\n",
      "\n",
      "∈ {5, 10, 15, 20}, α0 ∈ {1, 5}, γ ∈ {1, 2, 3}, unibigram aggregation ∈ {sum, mean}, and Gibbs sampling iterations ∈ {10, 20, 50}. Instead, for the social tasks we implemented a Normal-Gamma prior H over a Gaussian distribution. Here the prior is parametrized by the following hyper-priors: µ0, the mean node degree extracted from the data; λ0, which is inversely proportional to the prior variance of the mean; and (a0, b0), whose ratio t =\n",
      "b0 a0 represents the expected variance of the data. The ICGMM hyper-parameters here were: number of layers ∈ {5, 10, 15, 20}, λ0 ∈ {1e-6}, a0 ∈ {1.},\n",
      "b0 ∈ {0.09, 1.}, α0 ∈ {1, 5, 10}, γ ∈ {2, 5, 10}, unibigram aggregation {sum, mean}, and Gibbs Sampling iterations ∈ {100}. To further automate learning of ICGMM's unsupervised layers, we place uninformative Gamma(1*, rate* = 0.01) hyper-priors on both α\n",
      "`\n",
      "0\n",
      ", γ` hyper-parameters. To prevent the model from getting stuck in a local minimum on COLLAB (due to bimodal degree distribution and large variances), we tried λ0 ∈ {1e-4, 1e-5}.\n",
      "\n",
      "To conclude, we list the hyper-parameters tried for the one-layer MLP classifier trained on the unsupervised graph embeddings: optimizer ∈ {Adam}, batch size ∈ {32}, hidden units ∈ {32, 128},\n",
      "learning rate ∈ {1e-3}, L2 regularization ∈ {0., 5e-4}, epochs ∈ {2000}, ReLU activation, and early stopping on validation accuracy with patience 300 on chemical tasks and 100 on social ones.\n",
      "\n",
      "## 5 Results\n",
      "\n",
      "The empirical results on chemical and social benchmarks are reported in Tables 1 and 2, respectively.\n",
      "\n",
      "There are several observations to be made, starting with the chemical tasks. First of all, ICGMM\n",
      "performs similarly to CGMM, E-CGMM, and most of the *supervised* neural models; this suggests that the selection of ju based on the neighboring recommendations is a subtle but effective form of information propagation between the nodes of the graph. In addition, results indicate that we have succeeded in effectively automatizing the choice of the number of latent states without compromising the accuracy, which was the main goal of this work. Finally, ICGMMf performs as well as the exact version, and for this reason we safely applied the faster variant to the larger social datasets (including IMDB-B and IMDB-M to ease the exposition). Moving to the social datasets, we observe that ICGMM achieves better average performances than other methods on IMDB-B, REDDIT-B and COLLAB. One possible reason for such an improvement with respect to CGMM variants may be how the emission distributions are initialized. On the one hand, and differently from the chemical tasks, CGMM and E-CGMM use the k-means algorithm\n",
      "(with fixed k=C), to initialize the mean values of the C Gaussian distributions, which can be stuck in a local minimum around the most frequent degree values. One the other hand, ICGMM adopts a fully Bayesian treatment, which combined with the automatic selection of the latent states allows to better model outliers by adding a new state when the posterior probability of a data point is too low.\n",
      "\n",
      "In what follows, we will try to shed more light into the improved generalization performances of ICGMM, by analyzing the exact model from a layer-wise perspective.\n",
      "\n",
      "![7_image_1.png](7_image_1.png)\n",
      "\n",
      "![7_image_0.png](7_image_0.png) \n",
      "(a) Effect of depth on training/validation accuracy\n",
      "(b) Average VL accuracy (solid line) and number of chosen states (dashed line) w.r.t α0 and γ values Figure 2: Figures 2a and 2b analyze the relation between depth, performances, and the number of chosen states on NCI1.\n",
      "On the effectiveness of depth and hyper-parameters. To confirm our intuition about the benefits of the proposed information propagation mechanism, Figure 2a shows the NCI1 training and validation performances of both CGMM and ICGMM as we add more layers. For simplicity, we picked the best ICGMM configuration on the first external fold, and we compared it against the CGMM\n",
      "configuration with the most similar performances. Note that C = 20 was the most frequent choice of CGMM states by the best model configurations across the 10 outer folds: this is because having more emission distributions to choose from allows the CGMM model to find better local minima, whereas ICGMM can automatically add states whenever the data point's sampling probabilities are too low. We trained the same classifier at different depths, and we averaged scores across the 10 outer folds. We observe that the validation performance of both models are similar, with an asymptotic behavior as we reach 20 layers; it follows that depth remains fundamental to improve the generalization performances (Bacciu et al., 2020a). Importantly, we see that gap between ICGMM training and validation scores is thinner than its non-BNP counterpart, suggesting that the classifier is less prone to overfitting the data.\n",
      "\n",
      "We now study how ICGMM behaves as we vary the main hyper-parameters α0 and γ. We continue our experimentation on NCI1; Figure 2b depicts the average validation performance and number of states C over all configurations and folds, subject to changes of α0 and γ values. The trend indicates how greater values for both hyper-parameters achieve, on average, better validation performance.\n",
      "\n",
      "Also, smaller values of the two hyper-parameters tend to strongly regularize the model by creating fewer states, with consequent reduction in validation accuracy. The relation between the number of states and these hyper-parameters remains consistent with the mathematical details of Section 3.\n",
      "\n",
      "On the quality of graph embeddings. So far, we have argued that ICGMM selects the appropriate number of states for its unsupervised task at each layer. As a matter of fact, Figure 3a reports such a statistic on the same NCI1 configuration as before: ICGMM preferred a lower number of latent states than CGMM, i.e., around 5 per layer. In turn, the resulting graph embeddings become much smaller, with important savings in terms of memory footprint and computational costs to train the subsequent classifier. Figure 3b displays the cumulative graph embedding size across layers, using\n",
      "\n",
      "![8_image_0.png](8_image_0.png) \n",
      "\n",
      "(a) # states chosen at each layer\n",
      "(b) Cumulative graph embedding size on NCI1 Figure 3: We show comparative results on the size and quality of graph embeddings between CGMM\n",
      "and ICGMM. Overall, ICGMM generates ≈ 0 unused latent states, with consequent savings in terms of memory and compute time of the classifier with respect to CGMM. See the text for more details.\n",
      "the unibigram representation without loss of generality. We see that, when compared with CGMM\n",
      "(C=20), the size of graph embeddings produced by ICGMM is approximately 7% of those of the original model, while still preserving the same performance as CGMM.\n",
      "\n",
      "On the automatic estimation of α\n",
      "` and γ\n",
      "`. We conclude this work with a performance analysis of the fully automated versions of ICGMM and ICGMMf , namely those with an *\"auto\"* subscript in Tables 1 and 2; in particular, we observe no statistically significant performance differences with respect to the original models. By estimating all hyper-parameters of our models using uninformative priors, we almost always (but for COLLAB) managed to *avoid the model selection* for the unsupervised graph embeddings creation. In turn, this amounted to a 6× reduction in the overall number of configurations to be tried, but most importantly it frees the user from making hard choices about which configurations of hyper-parameters to try. Additionally, we observe that the number of chosen states and the consequent graph embedding size is very similar to that of ICGMM\n",
      "with α0 = 5, γ = 3, but this time the two hyper-parameters have been automatically adjusted by the model on the basis of the data.\n",
      "\n",
      "To sum up, we have shown that: i) our model has very competitive performances with respect to the state of the art; ii) the information propagation mechanism introduced in the HDP is effective; iii)\n",
      "the model can automatically selects the number of states; iv) we can get a much lower memory and computational footprints due to the previous points without sacrificing the predictive performance; v) we can fully automatize the choice of the hyper-parameters using uninformative priors, which drastically reduces the cost of the model selection phase.\n",
      "\n",
      "## 6 Conclusions\n",
      "\n",
      "With the Infinite Contextual Graph Markov Model, we have bridged the gap between Bayesian nonparametric techniques and machine learning for graphs. We have described how our approach can automatically adjust the number of states and most hyper-parameters of each unsupervised layer, thus freeing the user from the burden of selecting them a priori. As the empirical analyses show, not only can the model exploit depth to increase its generalization performances, but it also produces smaller embeddings than CGMM, with consequent savings in terms of memory footprint and training time of the subsequent classifier. For these reasons, we believe that ICGMM represents a first relevant step towards the automatic construction of fully probabilistic deep learning models for graphs.\n",
      "\n",
      "## Reproducibility Statement\n",
      "\n",
      "To ensure that the results in this paper are reproducible, we relied on the PyDGN library (https://\n",
      "pypi.org/project/PyDGN/), which automatically handles both data and experiment pipelines, thus letting the researcher focus on the model definition. We follow the robust and reproducible settings of Errica et al. (2020); Section 4 reports further experimental details and the hyper-parameters tried for the models considered. Likewise, the appendix contains a detailed description of the inference procedure, as well as the pseudocode that has been implemented in the supplementary material. The data splits used can be retrieved from Errica et al. (2020), but they are nonetheless stored in the supplementary material alongside the code for ICGMM. To completely reproduce the experiments\n",
      "(model selection, model assessment) and the data pre-processing steps, we have also provided the necessary PyDGN configuration files.\n",
      "\n",
      "## References\n",
      "\n",
      "David J Aldous. Exchangeability and related topics. In École d'Été de Probabilités de Saint-Flour XIII—1983, pp. 1–198. Springer, 1985.\n",
      "\n",
      "Daniele Atzeni, Davide Bacciu, Federico Errica, and Alessio Micheli. Modeling edge features with deep bayesian graph networks. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2021.\n",
      "\n",
      "Davide Bacciu, Federico Errica, and Alessio Micheli. Contextual Graph Markov Model: A deep and generative approach to graph processing. In Proceedings of the 35th International Conference on Machine Learning (ICML), volume 80, pp. 294–303. PMLR, 2018.\n",
      "\n",
      "Davide Bacciu, Federico Errica, and Alessio Micheli. Probabilistic learning on graphs via contextual architectures. *Journal of Machine Learning Research*, 21(134):1–39, 2020a.\n",
      "\n",
      "Davide Bacciu, Federico Errica, Alessio Micheli, and Marco Podda. A gentle introduction to deep learning for graphs. *Neural Networks*, 129:203–221, 9 2020b.\n",
      "\n",
      "Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, and others.\n",
      "\n",
      "Relational inductive biases, deep learning, and graph networks. *arXiv preprint arXiv:1806.01261*,\n",
      "2018.\n",
      "\n",
      "Matthew J Beal, Zoubin Ghahramani, and Carl Edward Rasmussen. The infinite hidden markov model. *Proceedings of the 16th Conference on Neural Information Processing Systems (NIPS)*, 1:\n",
      "577–584, 2002.\n",
      "\n",
      "James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of machine learning research, 13(2):281–305, 2012.\n",
      "\n",
      "José M Bernardo and Adrian FM Smith. *Bayesian theory*, volume 405. John Wiley & Sons, 2009.\n",
      "\n",
      "Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. *Bioinformatics*, 21(suppl_1):\n",
      "i47–i56, 2005.\n",
      "\n",
      "Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond Euclidean data. *IEEE Signal Processing Magazine*, 34(4):25. 18–42, 2017.\n",
      "\n",
      "Michael Bryant and Erik Sudderth. Truly nonparametric online variational inference for hierarchical dirichlet processes. In Proceedings of the 26th Conference on Neural Information Processing Systems (NIPS), 2012.\n",
      "\n",
      "Jason Chang and John W Fisher III. Parallel sampling of hdps using sub-cluster splits. In *Proceedings* of the 28th Conference on Neural Information Processing Systems (NIPS), 2014.\n",
      "\n",
      "Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. *Journal of molecular biology*, 330(4):771–783, 2003.\n",
      "\n",
      "Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. In Proceedings of the 8th International Conference on Learning Representations (ICLR), 2020.\n",
      "\n",
      "Scott E. Fahlman and Christian Lebiere. The Cascade-Correlation learning architecture. In *Proceedings of the 3rd Conference on Neural Information Processing Systems (NIPS)*, pp. 524–532, 1990.\n",
      "\n",
      "Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.\n",
      "\n",
      "In Representation Learning on Graphs and Manifolds Workshop, International Conference on Learning Representations (ICLR), 2019.\n",
      "\n",
      "Emily B Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. The sticky hdp-hmm: Bayesian nonparametric hidden markov models with persistent states. *Preprint*, 2007.\n",
      "\n",
      "Emily B Fox, Erik B Sudderth, Michael I Jordan, and Alan S Willsky. An hdp-hmm for systems with state persistence. In Proceedings of the 25th International Conference on Machine Learning\n",
      "(ICML), pp. 312–319, 2008.\n",
      "\n",
      "Yarin Gal and Zoubin Ghahramani. Pitfalls in the use of parallel inference for the dirichlet process.\n",
      "\n",
      "In *Proceedings of the 31st International Conference on Machine Learning (ICML)*, volume 32, pp.\n",
      "\n",
      "208–216. PMLR, 22–24 Jun 2014.\n",
      "\n",
      "Hong Ge, Yutian Chen, Moquan Wan, and Zoubin Ghahramani. Distributed inference for dirichlet process mixture models. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, pp. 2276–2284. PMLR, 2015.\n",
      "\n",
      "Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. *IEEE Transactions on pattern analysis and machine intelligence*, PAMI-6\n",
      "(6):721–741, 1984.\n",
      "\n",
      "Samuel J Gershman and David M Blei. A tutorial on bayesian nonparametric models. *Journal of* Mathematical Psychology, 56(1):1–12, 2012.\n",
      "\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In *International Conference on Machine Learning*, pp.\n",
      "\n",
      "1263–1272. PMLR, 2017.\n",
      "\n",
      "Prem K. Goel and Morris H. Degroot. Information about hyperparamters in hierarchical models.\n",
      "\n",
      "Journal of the American Statistical Association, 76(373):140–147, 1981. ISSN 01621459. URL\n",
      "http://www.jstor.org/stable/2287059.\n",
      "\n",
      "Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems (NIPS), pp. 1024–1034. Curran Associates, Inc., 2017.\n",
      "\n",
      "Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art. *KnowledgeBased Systems*, 212:106622, 2021.\n",
      "\n",
      "Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference.\n",
      "\n",
      "Journal of Machine Learning Research, 14(5), 2013.\n",
      "\n",
      "Fred M Hoppe. Pólya-like urns and the ewens' sampling formula. *Journal of Mathematical Biology*,\n",
      "20(1):91–94, 1984.\n",
      "\n",
      "Michael Hughes, Dae Il Kim, and Erik Sudderth. Reliable and scalable variational inference for the hierarchical dirichlet process. In *Proceedings of the 18th International Conference on Artificial* Intelligence and Statistics (AISTATS), 2015.\n",
      "\n",
      "Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels, 2016. URL http://graphkernels.cs.tu-dortmund.\n",
      "\n",
      "de.\n",
      "\n",
      "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n",
      "\n",
      "In *5th International Conference on Learning Representations (ICLR)*, 2017.\n",
      "\n",
      "Dan Lovell, Ryan P. Adams, and Vikash K. Mansinghka. Parallel markov chain monte carlo for dirichlet process mixtures. In Workshop on Big Learning, Advances in Neural Information Processing Systems (NIPS), 2012.\n",
      "\n",
      "Alessio Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions on Neural Networks, 20(3):498–511, 2009. Publisher: IEEE.\n",
      "\n",
      "Todd K Moon. The expectation-maximization algorithm. *IEEE Signal Processing Magazine*, 13(6):\n",
      "47–60, 1996. Publisher: IEEE.\n",
      "\n",
      "Radford M Neal. Markov chain sampling methods for dirichlet process mixture models. Journal of computational and graphical statistics, 9(2):249–265, 2000.\n",
      "\n",
      "Peter Orbanz and Yee Whye Teh. Bayesian nonparametric models. *Encyclopedia of machine learning*,\n",
      "1, 2010.\n",
      "\n",
      "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\n",
      "\n",
      "The graph neural network model. *IEEE Transactions on Neural Networks*, 20(1):61–80, 2009.\n",
      "\n",
      "Publisher: IEEE.\n",
      "\n",
      "Jayaram Sethuraman. A constructive definition of dirichlet priors. *Statistica sinica*, pp. 639–650, 1994.\n",
      "\n",
      "Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neural networks on graphs. In *Proceedings of the IEEE Conference on Computer Vision and Pattern* Recognition (CVPR), pp. 29–38, 2017.\n",
      "\n",
      "Yee Whye Teh. *Dirichlet Process*, pp. 280–287. Springer US, Boston, MA, 2010. ISBN 978-0-38730164-8.\n",
      "\n",
      "Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet processes.\n",
      "\n",
      "Journal of the american statistical association, 101(476):1566–1581, 2006.\n",
      "\n",
      "Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. *Knowledge and Information Systems*, 14(3):347–375, 2008.\n",
      "\n",
      "Chong Wang and David Blei. Truncation-free stochastic variational inference for bayesian nonparametric models. In *Proceedings of the 26th Conference on Neural Information Processing Systems*\n",
      "(NIPS), 2012.\n",
      "\n",
      "Sinead Williamson, Avinava Dubey, and Eric Xing. Parallel Markov chain Monte Carlo for nonparametric mixture models. In Proceedings of the 30th International Conference on Machine Learning\n",
      "(ICML), volume 28, pp. 98–106. PMLR, 2013.\n",
      "\n",
      "Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\n",
      "comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020.\n",
      "\n",
      "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Proceedings of the 7th International Conference on Learning Representations\n",
      "(ICLR), 2019.\n",
      "\n",
      "Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In *Proceedings of the 21th ACM*\n",
      "International Conference on Knowledge Discovery and Data Mining (SIGKDD), pp. 1365–1374, 2015.\n",
      "\n",
      "Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.\n",
      "\n",
      "Hierarchical graph representation learning with differentiable pooling. In *Advances in Neural* Information Processing Systems (NeurIPS), pp. 4800–4810. Curran Associates, Inc., 2018.\n",
      "\n",
      "Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In *Proceedings of the AAAI Conference on Artificial Intelligence*\n",
      "(AAAI), pp. 4438–4445, 2018.\n",
      "\n",
      "Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. *arXiv preprint arXiv:1909.03184*, 2019.\n",
      "\n",
      "## A Icgmm Gibbs Sampling Procedure\n",
      "\n",
      "HDP Gibbs sampling is an iterative procedure (Neal, 2000; Teh et al., 2006; Fox et al., 2007) that we use to estimate all node latent states and ICGMM's parameters at each layer. Hereinafter, to keep the notation less cluttered, we shall omit the superscript ` of the current layer and define C¯=C\n",
      "`−1.\n",
      "\n",
      "Sampling qu. The conditional distribution qu of qu given all the other variables is given by:\n",
      "\n",
      "$${\\mathrm{:~is~given~by:}}$$\n",
      "$$\\operatorname{union}\\,q_{u}\\circ\\,$$\n",
      "$$(4)$$\n",
      "$$q^{-u},\\beta,\\theta,x)\\propto(\\alpha_{0}$$\n",
      "\n",
      "P(qu = c | ju = j, q\n",
      "−u, β, θ, x) ∝ (α0βc + n\n",
      "−u\n",
      "\n",
      "$$\\,_{|}f(x_{u}\\mid\\theta_{c}),\\quad c\\in\\{1,\\ldots,C+1\\};$$\n",
      "\n",
      "where we recall that C denotes the number of current states in the mixture model, f is the *p.d.f.*\n",
      "associated with emission distribution F(θ) and the distribution πj has been integrated out (Teh et al.,\n",
      "2006). Here, n\n",
      "−u jc indicates the number of observations assigned so far to the mixture component c of group j. Whenever we have that qu = C + 1, we create a new state and sample a new emission distribution θC+1 from H. On the contrary, if at the end of an iteration there are no observation of any group associated with a certain mixture component, we can remove that mixture component and decrement the current number of states C. This is how ICGMM varies in complexity to fit the data distribution. Also, note that q\n",
      "`\n",
      "u will be used in Eq. 3 at the next layer ` + 1. When inferring the latent states of a new data point, no statistics of the model are updated.\n",
      "\n",
      "Sampling β. In the HDP stick-breaking representation that we use to define the ICGMM in Section 3.3, we require an auxiliary variable method to sample the base distribution β (Teh et al., 2006). We\n",
      "therefore introduce the auxiliary variables m = {mjc | ∀j ∈ {1, . . . , C¯}, ∀c ∈ {1*, . . . , C*}} that\n",
      "need to be sampled in order to compute β. However, being mjc dependent on njc, the sampling step of these variables is very inefficient for large values of njc, as the probability values are proportional\n",
      "the Stirling number of the first-kind s(njc, ·) (Fox et al., 2008). Luckily, we can avoid this step by\n",
      "observing that the value mjc corresponds to the number of tables where dish qu = c is served at\n",
      "restaurant j in the Chines Restaurant Franchise (CRF) representation (Teh et al., 2006; Fox et al.,\n",
      "2007). Thus, we can compute each mjc by simply simulating the table assignments process. We\n",
      "recall that, in the CRF representation, each customer (i.e., observation) of each restaurant (i.e., group)\n",
      "is assigned to a table where just a single dish (i.e., mixture component) is served. Thus, while all\n",
      "customers sitting at the same table must be eating the same dish, there can be multiple tables serving\n",
      "the same dish as well.\n",
      "Knowing that customer u is eating the dish qu = c, its table assignment tu can be sampled according\n",
      "to:\n",
      "$$P(t_{u}=t\\mid q_{u}=c,j_{u}=j,\\mathbf{c},\\mathbf{t}^{-u},\\mathbf{\\beta},\\alpha_{0})\\propto\\begin{cases}\\hat{n}_{jt}^{-u},&\\forall t\\text{s.t.}c_{jt}=c;\\\\ \\alpha_{0}\\beta_{c},&t=t_{new},\\end{cases}\\tag{5}$$\n",
      "where t\n",
      "−urepresents the tables assigned to all the other nodes except u, cjt ∈ c specifies the dish\n",
      "assigned to table t at restaurant j and n˜\n",
      "−u\n",
      "jt denotes the number of customers (except u) sitting at table\n",
      "t of restaurant j. Since we know the dish qu selected by the customer u, there is zero probability that\n",
      "the customer sits to a table where that dish is not served. The creation and deletion of tables is very similar to that of Eq. 4, so we skip it in the interest of the exposition and refer to the pseudocode in\n",
      "Appendix B for a complete treatment.\n",
      "At last, after computing mjc as described above (i.e., Pt\n",
      "0 I[cjt0 = c]), the base distribution β is\n",
      "sampled from:\n",
      "$$\\beta\\mid\\mathbf{q},\\mathbf{m}\\sim\\operatorname{Dir}(\\sum_{j=1}^{\\tilde{C}}m_{j1},\\ldots,\\sum_{j=1}^{\\tilde{C}}m_{jC},\\gamma),$$\n",
      "$$(\\mathbf{6})$$\n",
      "$$\\left(7\\right)$$\n",
      "where Dir stands for the Dirichlet distribution.\n",
      "\n",
      "Sampling θ. To update the emission parameters θ, we rely on its posterior given q and x:\n",
      "\n",
      "$$P(\\mathbf{\\theta}_{c}\\mid\\mathbf{q},\\mathbf{x})\\propto h(\\mathbf{\\theta}_{c})\\prod_{\\forall u\\mid q_{u}=c}f(x_{u}\\mid\\mathbf{\\theta}_{c}).$$\n",
      "\n",
      "By choosing the family of the base distribution H to be a conjugate prior for F, e.g., a Dirichlet distribution for Categorical emissions or a Normal-Gamma distribution for Normal emissions, we can compute the posterior in closed form.\n",
      "\n",
      "Let the emission distribution be a categorical distribution with M possible states. When creating a new state, we can sample the emission parameter according to a Dirichlet distribution, which is a conjugate prior for the categorical distribution:\n",
      "\n",
      "$$\\theta_{c}\\sim\\mathrm{Dir}(\\eta,\\ldots,\\eta),$$\n",
      "$$(8)$$\n",
      "$$({\\mathfrak{g}})$$\n",
      "θc ∼ Dir(*η, . . . , η*), (8)\n",
      "where the subscript c indicates the mixture component. Thanks to the conjugate prior, the emission parameters can be updated by sampling its Dirichlet posterior distribution:\n",
      "\n",
      "θ 0 c ∼ Dir(η + N 1 c , . . . , η + N M c ), (9) where Nm cindicates the number of times the visible label m has been associated with the latent state\n",
      "c, i.e., Ncm =Pu I[qu = c ∧ xu = m].\n",
      "\n",
      "Similarly to the categorical case, let the emission distribution be an univariate Gaussian. In this case, for each state, we can sample the emission parameter according to a Normal-Gamma distribution:\n",
      "\n",
      "$$\\mu_{c}\\sim{\\cal N}(\\mu_{0},1/(\\lambda_{0}\\tau_{c}))\\tag{10}$$ $$\\tau_{c}\\sim{\\rm Gamma}(a_{0},b_{0}),\\tag{11}$$\n",
      "\n",
      "where the subscript c indicates a mixture component ant τc is the inverse of the variance. Then, the emission parameters of the Gaussian can be updated as follows:\n",
      "\n",
      "$$\\mu^{\\prime}_{c}\\sim{\\cal N}\\left(\\frac{\\lambda_{0}\\mu_{0}+N_{c}\\bar{x}_{c}}{\\lambda_{0}+N_{c}},\\frac{1}{(\\lambda_{0}+N_{c})\\tau^{\\prime}_{c}}\\right)\\tag{12}$$ $$\\tau^{\\prime}_{c}\\sim\\mbox{Gamma}\\left(a_{0}+\\frac{N_{c}}{2},b_{0}+\\frac{1}{2}\\left(N_{c}s_{c}+\\frac{\\lambda_{0}N_{c}(\\bar{x}_{c}-\\mu_{0})^{2}}{\\lambda_{0}+N_{c}}\\right)\\right),\\tag{13}$$\n",
      "\n",
      "where P\n",
      "Nc indicates the number of visible labels associated with the latent state c (i.e., Nc =\n",
      "u I[qu = c]), x¯c is the mean of the data associated with the class c (i.e., x¯c =1 Nc P∀u|qu=c xu),\n",
      "and sc is the variance of the data associated with the class c (i.e., sc =1 Nc P∀u|qu=c\n",
      "(xu − x¯u)\n",
      "2).\n",
      "\n",
      "Sampling α0. Following (Teh et al., 2006), the concentration parameter α0 can be updated between Gibbs sampling iterations by exploiting an auxiliary variable schema. Assume that α0 has a Gamma prior distribution Gamma(*a, b*) (i.e., α0 ∼ Gamma(*a, b*)). Then, we define the auxiliary variables w1*, . . . , w*C¯ and s1*, . . . , s*C¯, where each wj variable takes a value between 0 and 1, and each sj is a binary variable. Then, the value of α0 can be sampled according to the following schema:\n",
      "where nj. is the number of costumer eating in the j-th restaurant, and m.is the total number of tables in all the restaurants.\n",
      "\n",
      "Sampling γ. Similarly, assuming that the hyperparameter γ has a gamma prior distribution Gamma(a 0, b0) (i.e., γ ∼ Gamma(a 0, b0)), its value can be updated by following the auxiliary variable schema below (Teh et al., 2006; Fox et al., 2008):\n",
      "\n",
      "$$\\begin{array}{l}{{r\\sim\\mathrm{Beta}(\\gamma+1,m_{.}),}}\\\\ {{p\\sim\\mathrm{Bernoulli}\\left(\\frac{m_{.}}{m_{.}+\\gamma}\\right),}}\\\\ {{\\gamma\\sim\\mathrm{Gamma}(a^{\\prime}+C-p,b^{\\prime}-\\log r).}}\\end{array}$$\n",
      "$$\\begin{array}{l}{{w_{j}\\sim\\mathrm{Beta}(\\alpha_{0}+1,n_{j.}),}}\\\\ {{s_{j}\\sim\\mathrm{Bernoulli}\\left(\\frac{n_{j.}}{n_{j.}+\\alpha_{0}}\\right),}}\\\\ {{\\alpha_{0}\\sim\\mathrm{Gamma}\\left(a+m_{.}-\\sum_{j=1}^{\\mathcal{C}}s_{j},b-\\sum_{j=1}^{\\mathcal{C}}\\log w_{j}\\right),}}\\end{array}$$\n",
      "$$(14)$$\n",
      "$$(15)$$\n",
      "$$(16)$$\n",
      "$$(17)$$  $$(18)$$\n",
      "$$(19)$$\n",
      "\n",
      "## B Icgmm Pseudocode\n",
      "\n",
      "To facilitate the practical understanding of our model, we provide the pseudocode of the Gibbs sampling method employed in this work.\n",
      "\n",
      "Algorithm 1 Gibbs sampling method for exact ICGMM\n",
      "Require: A dataset of graphs D = {g1*, . . . , g*N }. Initialize C = 1, θ = {θ1} (where θ1 ∼ H), Tj = ∅ (for all restaurant j), q = t = c = ⊥, and n = n˜ = 0.\n",
      "\n",
      "repeat for g ∈ D do . For each graph for u ∈ Vg do . For each node\n",
      "// assign the restaurant ju ← ψ(q\n",
      "`−1 Nu\n",
      ") . Can be done once ∀u\n",
      "// assign the dish njuqu ← njuqu − 1 . If qu 6= ⊥, remove qu from the counting qu ← SAMPLING(ju, n, θ, x, β, α0) . Sample the dish according to Eq. (4)\n",
      "if qu is new **then** . Create a new state θnew ∼ H θ ← θ ∪ {θnew}\n",
      "C ← C + 1 njqu ← 0 ∀j ∈ {1*, . . . ,* C¯} . Initialize the counters end if njuqu ← njuqu + 1 . Update the counter\n",
      "// assign the table n˜jutu ← n˜jutu − 1 . If tu 6= ⊥, remove tu from the counting tu ← SAMPLING(ju, qu, c, n˜, β, α0) . Sample the table according to Eq. (5)\n",
      "if tu is new **then** . Create a new table Tj ← Tj ∪ {tu}\n",
      "cjutu ← qu . Save the dish-table assignment mjuqu ← mjuqu + 1 . Update the table count n˜jutu ← 0 . Initialize customer counter end if n˜jutu ← n˜jutu + 1 end for end for // remove unused dishes for c ∈ {1*, . . . , C*} do if PC¯\n",
      "j=1 njc = 0 **then** . No customers eat the dish c θ ← θ \\ {θc}\n",
      "C ← C − 1 end if end for // remove empty tables for j ∈ {1*, . . . ,* C¯} do for t ∈ Tj do if n˜jt = 0 **then** . No customers eat at the table t in the restaurant j Tj ← Tj \\ {t}\n",
      "mjcjt ← mjcjt − 1 end if end for end for\n",
      "// update model parameters β ← SAMPLING(q,m) . Sample according to Eq. (6) θ ← SAMPLING(q, x) . Sample according to Eq. (7)\n",
      "if ICGMM*auto* **then**\n",
      "α0 ← SAMPLING(*a, b,* n) . Sample according to Eq. (14), (15), (16)\n",
      "γ ← SAMPLING(a 0, b0,m) . Sample according to Eq. (17), (18), (19)\n",
      "end if until stopping criteria\n",
      "\n",
      "## C Dataset Statistics\n",
      "\n",
      "Below we report some statistics for the chosen benchmarks.\n",
      "\n",
      "Table 3: Dataset statistics.\n",
      "\n",
      "|               | # Graphs    | # Classes   | # Nodes   | # Edges   | # Node labels   |    |\n",
      "|---------------|-------------|-------------|-----------|-----------|-----------------|----|\n",
      "| DD            | 1178        | 2           | 284.32    | 715.66    | 89              |    |\n",
      "| CHEM.         | NCI1        | 4110        | 2         | 29.87     | 32.30           | 37 |\n",
      "| PROTEINS      | 1113        | 2           | 39.06     | 72.82     | 3               |    |\n",
      "| SOCIAL        | IMDB-BINARY | 1000        | 2         | 19.77     | 96.53           | -  |\n",
      "| IMDB-MULTI    | 1500        | 3           | 13.00     | 65.94     | -               |    |\n",
      "| REDDIT-BINARY | 2000        | 2           | 429.63    | 497.75    | -               |    |\n",
      "| REDDIT-5K     | 4999        | 5           | 508.82    | 594.87    | -               |    |\n",
      "| COLLAB        | 5000        | 3           | 74.49     | 2457.78   | -               |    |\n",
      "\n",
      "## D Speedup Gains With Faster Inference\n",
      "\n",
      "We compare the performances of the exact version of ICGMM against the faster implementation. As we can see, the speedup increases for the datasets with larger average number of nodes (see Table 3).\n",
      "\n",
      "|           | ICGMM   | ICGMMf      |           |\n",
      "|-----------|---------|-------------|-----------|\n",
      "|           | ref.    | min/max     |           |\n",
      "| DD        | 1×      | 17.8×/30.8× |           |\n",
      "| CHEM.     | NCI1    | 1×          | 3.1×/5.1× |\n",
      "| PROTEINS  | 1×      | 4.2×/5.7×   |           |\n",
      "| SOCIAL    | IMDB-B  | 1×          | 2.4×/5.1× |\n",
      "| IMDB-M    | 1×      | 1.6×/3.6×   |           |\n",
      "| REDDIT-B  | 1×      | 11.1×/45.6× |           |\n",
      "| REDDIT-5K | 1×      | 36.7×/60.6× |           |\n",
      "| COLLAB    | 1×      | 3.1×/8.6×   |           |\n",
      "\n",
      "Table 4: Approximate speedup between the exact ICGMM and the faster version on all datasets.\n"
     ]
    }
   ],
   "source": [
    "def load_reviewmt_file_test():\n",
    "    with open(\"data/reviewmt_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "test_data= load_reviewmt_file_test()\n",
    "\n",
    "example_test = test_data[0]\n",
    "print(\"Instruction:\", example_test[\"instruction\"])\n",
    "print(\"Input:\", example_test[\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bacf",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58afd513",
   "metadata": {},
   "source": [
    "### EDA: ICLR 2017 dataset (promising)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ca70a",
   "metadata": {},
   "source": [
    "#### I still dont really understand what these dataset contains, it look like it only has the LLM instruction plus the full paper title, where is the rest?? -> Look at the other dataset on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0273089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['instruction', 'input', 'output', 'system', 'history'])\n"
     ]
    }
   ],
   "source": [
    "def load_iclr_file():\n",
    "    with open(\"data/ICLR_2017.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "test_data = load_iclr_file()\n",
    "\n",
    "print(test_data[0].keys())\n",
    "\n",
    "example_test = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3063b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_input_sections(text: str) -> dict:\n",
    "    pattern = r\"Title:\\s*(.*?)\\s*Abstract:\\s*(.*?)\\s*Main Text:\\s*(.*)\"\n",
    "    match = re.match(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return {\n",
    "            \"title\": match.group(1).strip(),\n",
    "            \"abstract\": match.group(2).strip(),\n",
    "            \"main_text\": match.group(3).strip()\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Input text does not match expected pattern.\")\n",
    "\n",
    "def parse_review_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse a review into 'Summary', 'Strengths and Weaknesses', and optionally 'Questions'.\n",
    "    \"\"\"\n",
    "    pattern = (\n",
    "        r\"Summary:\\s*(.*?)\\s*\"\n",
    "        r\"Strengths and Weaknesses:\\s*(.*?)(?:\\s*Questions:\\s*(.*))?$\"\n",
    "    )\n",
    "    match = re.match(pattern, text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"Review does not match expected format.\")\n",
    "    \n",
    "    result = {\n",
    "        \"summary\": match.group(1).strip(),\n",
    "        \"strengths_and_weaknesses\": match.group(2).strip(),\n",
    "        \"questions\": match.group(3).strip() if match.group(3) else None\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fe20c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Role: Decision Maker. Task: Review all responses from the author and comments from all reviewers to provide the meta-review and determine the final decision. Explicitly state 'Accept' or 'Reject' at the end of your output.\n"
     ]
    }
   ],
   "source": [
    "print(\"Instruction:\", example_test[\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4f2dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Title: Identity Matters in Deep Learning Abstract: An emerging design principle in deep learning is that each layer of a deep\n",
      "artificial neural network should be able to easily express the identity\n",
      "transformation. This idea not only motivated various normalization techniques,\n",
      "such as batch normalization, but was also key to the immense success of\n",
      "residual networks.\n",
      "\n",
      "In this work, we put the principle of identity parameterization on a more \n",
      "solid theoretical footing alongside further empirical progress. We first\n",
      "give a strikingly simple proof that arbitrarily deep linear residual networks\n",
      "have no spurious local optima. The same result for feed-forward networks in\n",
      "their standard parameterization is substantially more delicate.  Second, we\n",
      "show that residual networks with ReLu activations have universal finite-sample\n",
      "expressivity in the sense that the network can represent any function of its\n",
      "sample provided that the model has more parameters than the sample size.\n",
      "\n",
      "Directly inspired by our theory, we experiment with a radically simple\n",
      "residual architecture consisting of only residual convolutional layers and\n",
      "ReLu activations, but no batch normalization, dropout, or max pool. Our model\n",
      "improves significantly on previous all-convolutional networks on the CIFAR10,\n",
      "CIFAR100, and ImageNet classification benchmarks.\n",
      " Main Text: ## 1 Introduction\n",
      "\n",
      "Traditional convolutional neural networks for image classification, such as AlexNet (Krizhevsky et al. (2012)), are parameterized in such a way that when all trainable weights are 0, a convolutional layer represents the 0-mapping. Moreover, the weights are initialized symmetrically around 0. This standard parameterization makes it non-trivial for a convolutional layer trained with stochastic gradient methods to preserve features that were already good. Put differently, such convolutional layers cannot easily converge to the identity transformation at training time. This shortcoming was observed and partially addressed by Ioffe & Szegedy (2015) through batch normalization, i.e., layer-wise whitening of the input with a learned mean and covariance. But the idea remained somewhat implicit until *residual networks* (He et al. (2015); He et al. (2016)) explicitly introduced a reparameterization of the convolutional layers such that when all trainable weights are 0, the layer represents the identity function. Formally, for an input x, each residual layer has the form x + h(x), rather than h(x). This simple reparameterization allows for much deeper architectures largely avoiding the problem of vanishing (or exploding) gradients. Residual networks, and subsequent architectures that use the same parameterization, have since then consistently achieved state-of-the-art results on various computer vision benchmarks such as CIFAR10 and ImageNet.\n",
      "\n",
      "## 1.1 Our Contributions\n",
      "\n",
      "In this work, we consider identity parameterizations from a theoretical perspective, while translating some of our theoretical insight back into experiments. Loosely speaking, our first result underlines how identity parameterizations make optimization easier, while our second result shows the same is true for representation.\n",
      "\n",
      "Linear residual networks.\n",
      "\n",
      "Since general non-linear neural networks, are beyond the reach of current theoretical methods in optimization, we consider the case of deep *linear* networks as a simplified model. A linear network represents an arbitrary linear map as a sequence of matrices Aℓ · · · A2A1.\n",
      "\n",
      "The objective function is E∥y − Aℓ *· · ·* A1x∥2, where y = Rx for some unknown linear transformation R and x is drawn from a distribution. Such linear networks have been studied actively in recent years as a stepping stone toward the general non-linear case (see Section 1.2). Even though Aℓ *· · ·* A1 is just a linear map, the optimization problem over the factored variables (Aℓ*, . . . , A*1) is non-convex. In analogy with residual networks, we will instead parameterize the objective function asmin A1,...,Aℓ E∥y − (I + Aℓ) · · · (I + A1)x∥2 . (1.1)\n",
      " To give some intuition, when the depth ℓ is large enough, we can hope that the target function R has a factored representation in which each matrix Ai has small norm. Any symmetric positive semidefinite matrix O can, for example, be written as a product O = Oℓ *· · ·* O1, where each Oi = O1/ℓ is very close to the identity for large ℓ so that Ai = Oi − I has small spectral norm. We first prove that an analogous claim is true for all linear transformations R. Specifically, we prove that for every linear transformation R, there exists a global optimizer (A1*, . . . , A*ℓ) of (1.1) such that for large enough depth ℓ, max 1≤i≤ℓ ∥Ai∥ ≤ O(1/ℓ).\n",
      "\n",
      "(1.2)\n",
      "Here, ∥A∥ denotes the spectral norm of A. The constant factor depends on the conditioning of R.\n",
      "\n",
      "We give the formal statement in Theorem 2.1. The theorem has the interesting consequence that as the depth increases, smaller norm solutions exist and hence regularization may offset the increase in parameters. Having established the existence of small norm solutions, our main result on linear residual networks shows that the objective function (1.1) is, in fact, easy to optimize when all matrices have sufficiently small norm. More formally, letting A = (A1*, . . . , A*ℓ) and f(A) denote the objective function in (1.1), we can show that the gradients of vanish only when f(A) = 0 provided that maxi ∥Ai∥ ≤\n",
      " O(1/ℓ). See Theorem 2.2. This result implies that linear residual networks have no critical points other than the global optimum. In contrast, for standard linear neural networks we only know, by work of Kawaguchi (2016) that these networks don't have local optima except the global optimum, but it doesn't rule out other critical points. In fact, setting Ai = 0 will always lead to a bad critical point in the standard parameterization.\n",
      "\n",
      "Universal finite sample expressivity.\n",
      "\n",
      "Going back to non-linear residual networks with ReLU activations, we can ask: How expressive are deep neural networks that are solely based on residual layers with ReLU activations? To answer this question, we give a very simple construction showing that such residual networks have perfect finite sample expressivity. In other words, a residual network with ReLU activations can easily express any functions of a sample of size n, provided that it has sufficiently more than n parameters. Note that this requirement is easily met in practice. On CIFAR 10 (n = 50000), for example, successful residual networks often have more than 106 parameters. More formally, for a data set of size n with r classes, our construction requires O(n log n+r2)\n",
      " parameters. Theorem 3.2 gives the formal statement. Each residual layer in our construction is of the form x + V ReLU(Ux), where U and V are linear transformations. These layers are significantly simpler than standard residual layers, which typically have two ReLU activations as well as two instances of batch normalization.\n",
      "\n",
      "The power of all-convolutional residual networks.\n",
      "\n",
      "Directly inspired by the simplicity of our expressivity result, we experiment with a very similar architecture on the CIFAR10, CIFAR100, and ImageNet data sets. Our architecture is merely a chain of convolutional residual layers each with a single ReLU activation, but without batch normalization, dropout, or max pooling as are common in standard architectures. The last layer is a fixed random projection that is not trained. In line with our theory, the convolutional weights are initialized near 0, using Gaussian noise mainly as a symmetry breaker. The only regularizer is standard weight decay (ℓ2-regularization) and there is no need for dropout. Despite its simplicity, our architecture reaches 6.38% top-1 classification error on the CIFAR10 benchmark (with standard data augmentation). This is competitive with the best residual network reported in He et al. (2015), which achieved 6.43%. Moreover, it improves upon the performance of the previous best *all-convolutional* network, 7.25%, achieved by Springenberg et al. (2014). Unlike ours, this previous all-convolutional architecture additionally required dropout and a non-standard preprocessing (ZCA) of the entire data set. Our architecture also improves significantly upon Springenberg et al. (2014) on both Cifar100 and ImageNet.\n",
      "\n",
      "## 1.2 Related Work\n",
      "\n",
      "Since the advent of residual networks (He et al. (2015); He et al. (2016)), most state-of-the-art networks for image classification have adopted a residual parameterization of the convolutional layers. Further impressive improvements were reported by Huang et al. (2016) with a variant of residual networks, called *dense nets*. Rather than adding the original input to the output of a convolutional layer, these networks preserve the original features directly by concatenation. In doing so, dense nets are also able to easily encode an identity embedding in a higher-dimensional space. It would be interesting to see if our theoretical results also apply to this variant of residual networks.\n",
      "\n",
      "There has been recent progress on understanding the optimization landscape of neural networks, though a comprehensive answer remains elusive.\n",
      "\n",
      "Experiments in Goodfellow et al. (2014)\n",
      " and Dauphin et al. (2014) suggest that the training objectives have a limited number of bad local minima with large function values. Work by Choromanska et al. (2015) draws an analogy between the optimization landscape of neural nets and that of the spin glass model in physics (Auffinger et al. (2013)). Soudry & Carmon (2016) showed that 2-layer neural networks have no bad differentiable local minima, but they didn't prove that a good differentiable local minimum does exist. Baldi & Hornik (1989) and Kawaguchi (2016) show that linear neural networks have no bad local minima. In contrast, we show that the optimization landscape of deep linear residual networks has no bad critical point, which is a stronger and more desirable property. Our proof is also notably simpler illustrating the power of re-parametrization for optimization. Our results also indicate that deeper networks may have more desirable optimization landscapes compared with shallower ones.\n",
      "\n",
      "## 2 Optimization Landscape Of Linear Residual Networks\n",
      "\n",
      "Consider the problem of learning a linear transformation R: Rd → Rd from noisy measurements y = Rx + ξ, where ξ *∈ N*(0, Idd) is a d-dimensional spherical Gaussian vector. Denoting by D the distribution of the input data x, let Σ = Ex∼D[xx⊤] be its covariance matrix.\n",
      "\n",
      "There are, of course, many ways to solve this classical problem, but our goal is to gain insights into the optimization landscape of neural nets, and in particular, residual networks. We therefore parameterize our learned model by a sequence of weight matrices $A_{1},\\ldots,A_{\\ell}\\in\\mathbb{R}^{d\\times d}$,\n",
      "\n",
      "$$h_{0}=x\\,,\\qquad h_{j}=h_{j-1}+A_{j}h_{j-1}\\,,\\qquad\\hat{y}=h_{\\ell}\\,.\\tag{2.1}$$\n",
      "\n",
      "Here $h_{1},\\ldots,h_{\\ell-1}$ are the $\\ell-1$ hidden layers and $\\hat{y}=h_{\\ell}$ are the predictions of the learned model on input $x$. More succinctly, we have\n",
      "\n",
      "$$\\hat{y}=(\\mathrm{Id}_{d}+A_{\\ell})\\ldots(\\mathrm{Id}+A_{1})x\\,.$$\n",
      "It is easy to see that this model can express any linear transformation R. We will use A as a shorthand for all of the weight matrices, that is, the ℓ × d × d-dimensional tensor the contains A1*, . . . , A*ℓ as slices. Our objective function is the maximum likelihood estimator,\n",
      "\n",
      "$$f(A,(x,y))=\\|\\hat{y}-y\\|^{2}=\\|({\\rm Id}+A_{\\ell})\\dots({\\rm Id}+A_{1})x-Rx-\\xi\\|^{2}\\;.\\tag{2.2}$$\n",
      "\n",
      "We will analyze the landscape of the _population risk_, defined as,\n",
      "\n",
      "$$f(A):=\\mathbb{E}\\left[f(A,(x,y))\\right]\\;.$$\n",
      "Recall that ∥Ai∥ is the spectral norm of Ai. We define the norm *|||·|||* for the tensor A as the maximum of the spectral norms of its slices,\n",
      "\n",
      "$$\\|A\\|:=\\operatorname*{max}_{1\\leq i\\leq\\ell}\\|A_{i}\\|\\,.$$\n",
      "The first theorem of this section states that the objective function f has an optimal solution with small *|||·|||*-norm, which is *inversely* proportional to the number of layers ℓ.\n",
      "\n",
      "Thus, when\n",
      "\n",
      "the architecture is deep, we can shoot for fairly small norm solutions. We define $\\gamma:=\\max\\{|\\log\\sigma_{\\max}(R)|,|\\log\\sigma_{\\min}(R)|\\}$. Here $\\sigma_{\\min}(\\cdot),\\sigma_{\\max}(\\cdot)$ denote the least and largest singular values of $R$ respectively.\n",
      "\n",
      "**Theorem 2.1**.: _Suppose $\\ell\\geq3\\gamma$. Then, there exists a global optimum solution $A^{*}$ of the population risk $f(\\cdot)$ with norm_\n",
      "\n",
      "$$\\|A^{*}\\|\\leq2(\\sqrt{\\pi}+\\sqrt{3\\gamma})^{2}/\\ell\\,.$$\n",
      "\n",
      "Here $\\gamma$ should be thought of as a constant since if $R$ is too large (or too small), we can scale the Here γ should be thought of as a constant since if R is too large (or too small), we can scale the data properly so that σmin(R) ≤ 1 ≤ σmax(R). Concretely, if σmax(R)/σmin(R) = κ, then we can scaling for the outputs properly so that σmin(R) = 1/√κ and σmax(R) = √κ. In this case, we have γ = log √κ, which will remain a small constant for fairly large condition number κ. We also point out that we made no attempt to optimize the constant factors here in the analysis. The proof of Theorem 2.1 is rather involved and is deferred to Section A.\n",
      "\n",
      "Given the observation of Theorem 2.1, we restrict our attention to analyzing the landscape of $f(\\cdot)$ in the set of $A$ with $\\|\\cdot\\|$-norm less than $\\tau$,\n",
      "\n",
      "$$\\mathcal{B}_{\\tau}=\\left\\{A\\in\\mathbb{R}^{\\ell\\times d\\times d}:\\|A\\|\\leq\\tau\\right\\}.$$\n",
      "Here using Theorem 2.1, the radius τ should be thought of as on the order of 1/ℓ. Our main theorem in this section claims that there is no bad critical point in the domain Bτ for any *τ <* 1. Recall that a critical point has vanishing gradient.\n",
      "\n",
      "Theorem 2.2. For any τ < 1, we have that any critical point A of the objective function f(·) inside the domain Bτ must also be a global minimum.\n",
      "\n",
      "Theorem 2.2 suggests that it is sufficient for the optimizer to converge to critical points of the population risk, since all the critical points are also global minima.\n",
      "\n",
      "Moreover, in addition to Theorem 2.2, we also have that any A inside the domain Bτ satisfies that ∥∇f(A)∥2\n",
      " F ≥ 4ℓ(1 − τ)ℓ−1σmin(Σ)2(f(A) − Copt) .\n",
      "\n",
      "(2.3)\n",
      "Here Copt is the global minimal value of f(·) and ∥∇f(A)∥F denotes the euclidean norm1 of the ℓ × d × d-dimensional tensor ∇f(A). Note that σmin(Σ) denote the minimum singular value of Σ.\n",
      "\n",
      "Equation (2.3) says that the gradient has fairly large norm compared to the error, which guarantees convergence of the gradient descent to a global minimum (Karimi et al. (2016)) if the iterates stay inside the domain Bτ, which is not guaranteed by Theorem 2.2 by itself.\n",
      "\n",
      "Towards proving Theorem 2.2, we start off with a simple claim that simplifies the population risk.\n",
      "\n",
      "We also use *∥·∥*F to denote the Frobenius norm of a matrix.\n",
      "\n",
      "Claim 2.3. In the setting of this section, we have,\n",
      "\n",
      "F + C . (2.4) f(A) = ���((Id + Aℓ) *. . .* (Id + A1) − R)Σ1/2��� 2\n",
      "Here C is a constant that doesn't depend on A, and Σ1/2 denote the square root of Σ, that is, the unique symmetric matrix B *that satisfies* B2 = Σ.\n",
      "\n",
      "Proof of Claim 2.3. Let tr(A) denotes the trace of the matrix A. Let E = (Id+Aℓ) *. . .* (Id+A1)−R.\n",
      "\n",
      "Recalling the definition of f(A) and using equation (2.2), we have\n",
      "\n",
      "f(A) = E � ∥Ex − ξ∥2� (by equation (2.2)) = E � ∥Ex∥2 + ∥ξ∥2 − 2⟨Ex, ξ⟩ � = E � tr(Exx⊤E⊤) � + E � ∥ξ∥2� (since E [⟨*Ex, ξ*⟩] = E [⟨Ex, E [ξ|x]⟩] = 0) = tr � E E � xx⊤� E⊤� + C (where C = E[xx⊤]) $=\\operatorname{tr}(E\\Sigma E^{\\top})+C=\\|E\\Sigma^{1/2}\\|_{F}^{2}+C\\,.$ (since $\\mathbb{E}\\left[xx^{\\top}\\right]=\\Sigma$)\n",
      "ijk T 2\n",
      "ijk.\n",
      "\n",
      "1That is, ∥T∥F :=\n",
      "��\n",
      "\n",
      "Next we compute the gradients of the objective function $f(\\cdot)$ from straightforward matrix calculus. We defer the full proof to Section A.\n",
      "\n",
      "**Lemma 2.4**.: _The gradients of $f(\\cdot)$ can be written as,_\n",
      "\n",
      "$$\\frac{\\partial f}{\\partial A_{\\rm i}}=2({\\rm Id}+A_{i}^{\\top})\\ldots({\\rm Id}+A_{i+1}^{\\top})E\\Sigma({\\rm Id}+A_{i-1}^{\\top})\\ldots({\\rm Id}+A_{1}^{\\top})\\;,\\tag{2.5}$$\n",
      "where E = (Id + Aℓ) . . . (Id + A1) − R .\n",
      "\n",
      "Now we are ready to prove Theorem 2.2. The key observation is that each matric Aj has small norm and cannot cancel the identity matrix. Therefore, the gradients in equation (2.5) is a product of non-zero matrices, except for the error matrix E. Therefore, if the gradient vanishes, then the only possibility is that the matrix E vanishes, which in turns implies A is an optimal solution.\n",
      "\n",
      "Proof of Theorem 2.2. Using Lemma 2.4, we have, ���� ∂f ∂Ai ���� F = 2 ��(Id + A⊤ ℓ ) . . . (Id + A⊤ i+1)EΣ(Id + A⊤ i−1) . . . (Id + A⊤ 1 ) �� F (by Lemma 2.4) j̸=i σmin(Id + A⊤ i ) · σmin(Σ)∥E∥F (by Claim C.2) ≥ 2 � ≥ 2(1 − τ)ℓ−1σmin(Σ)∥E∥ . (since σmin(Id + A) ≥ 1 *− ∥*A∥)\n",
      "It follows that\n",
      "\n",
      "2 ∥∇f(A)∥2 F = F ≥ 4ℓ(1 − τ)ℓ−1σmin(Σ)2∥E∥2 i=1 ℓ � ���� ∂f ∂Ai ���� ≥ 4ℓ(1 − τ)ℓ−1σmin(Σ)2(f(A) − C) (by the definition of E and Claim 2.3) ≥ 4ℓ(1 − τ)ℓ−1σmin(Σ)2(f(A) − Copt) . (since Copt = minA f(A) ≥ C by Claim 2.3) Therefore we complete the proof of equation (2.3). Finally, if $A$ is a critical point, namely, $\\nabla f(A)=0$, then by equation (2.3) we have that $f(A)=C_{\\text{opt}}$. That is, $A$ is a global minimum.\n",
      "\n",
      "## 3 Representational Power Of The Residual Networks\n",
      "\n",
      "In this section we characterize the finite-sample emissivity of residual networks. We consider a residual layers with a single ReLU activation and no batch normalization. The basic residual building block is a function $\\mathcal{T}_{U,V,s}(\\cdot):\\mathbb{R}^{k}\\rightarrow\\mathbb{R}^{k}$ that is parameterized by two weight matrices $U\\in\\mathbb{R}^{\\times k},V\\in\\mathbb{R}^{k\\times k}$ and a bias vector $s\\in\\mathbb{R}^{k}$,\n",
      "\n",
      "$$\\mathcal{T}_{U,V,s}(h)=V\\text{ReLU}(Uh+s)\\,.\\tag{3.1}$$\n",
      "\n",
      "A residual network is composed of a sequence of such residual blocks. In comparison with the full pre-activation architecture in He et al. (2016), we remove two batch normalization layers and one ReLU layer in each building block.\n",
      "\n",
      "We assume the data has r labels, encoded as r standard basis vectors in Rr, denoted by e1*, . . . , e*r.\n",
      "\n",
      "We have n training examples (x(1), y(1)), . . . , (x(n), y(n)), where x(i) ∈ Rd denotes the i-th data and y(i) ∈ {e1*, . . . , e*r} denotes the i-th label. Without loss of generality we assume the data are normalized so that x(i) = 1. We also make the mild assumption that no two data points are very close to each other.\n",
      "\n",
      "Assumption 3.1. We assume that for every 1 ≤ i < j ≤ n, we have ∥x(i) − x(j)∥2 ≥ ρ for some absolute constant ρ > 0.\n",
      "\n",
      "Images, for example, can always be imperceptibly perturbed in pixel space so as to satisfy this assumption for a small but constant ρ.\n",
      "\n",
      "Under this mild assumption, we prove that residual networks have the power to express any possible labeling of the data as long as the number of parameters is a logarithmic factor larger than n.\n",
      "\n",
      "Theorem 3.2. Suppose the training examples satisfy Assumption 3.1. Then, there exists a residual network N *(specified below) with* O(n log n + r2) parameters that perfectly expresses the training data, i.e., for all i ∈ {1, . . . , n}, the network N maps x(i) to y(i).\n",
      "\n",
      "It is common in practice that *n > r*2, as is for example the case for the Imagenet data set where n > 106 and r = 1000.\n",
      "\n",
      "We construct the following residual net using the building blocks of the form T*U,V,s* as defined in equation (3.1). The network consists of ℓ + 1 hidden layers h0*, . . . , h*ℓ, and the output is denoted by\n",
      " ˆy ∈ Rr. The first layer of weights matrices A0 maps the d-dimensional input to a k-dimensional hidden variable h0. Then we apply ℓ layers of building block T with weight matrices Aj, Bj ∈ Rk×k.\n",
      "\n",
      "Finally, we apply another layer to map the hidden variable hℓ to the label ˆy in Rk. Mathematically, we have h0 = A0x , hj = hj−1 + TAj,Bj,bj(hj−1),\n",
      "∀j ∈ {1, . . . , ℓ}\n",
      "\n",
      "ˆy = hℓ + TAℓ+1,Bℓ+1,sℓ+1(hℓ) . We note that here $A_{\\ell+1}\\in\\mathbb{R}^{k\\times r}$ and $B_{\\ell+1}\\in\\mathbb{R}^{r\\times r}$ so that the dimension is compatible. We assume the number of labels $r$ and the input dimension $d$ are both smaller than $n$, which is safety true in practical applications.[2] The hyperparameter $k$ will be chosen to be $O(\\log n)$ and the number of layers is chosen to be $\\ell=\\lceil n/k\\rceil$. Thus, the first layer has $dk$ parameters, and each of the middle $\\ell$ building blocks contains $2k^{2}$ parameters and the final building block has $kr+r^{2}$ parameters. Hence, the total number of parameters is $O(kd+\\ell k^{2}+rk+r^{2})=O(n\\log n+r^{2})$.\n",
      "\n",
      "Towards constructing the network N of the form above that fits the data, we first take a random matrix A0 ∈ Rk×d that maps all the data points x(i) to vectors h(i)\n",
      "0\n",
      ":= A0x(i). Here we will use h(i)\n",
      "j to denote the j-th layer of hidden variable of the i-th example. By Johnson-Lindenstrauss Theorem\n",
      " (Johnson & Lindenstrauss (1984), or see Wikipedia (2016)), with good probability, the resulting vectors h(i)\n",
      "0 's remain to satisfy Assumption 3.1 (with slightly different scaling and larger constant ρ), that is, any two vectors h(i)\n",
      "0\n",
      " and h(j)\n",
      "0\n",
      " are not very correlated.\n",
      "\n",
      "Then we construct ℓ middle layers that maps h(i)\n",
      "0\n",
      "to h(i)\n",
      "ℓ\n",
      "for every i ∈ {1*, . . . , n*}. These vectors h(i)\n",
      "ℓ\n",
      " will clustered into r groups according to the labels, though they are in the Rk instead of in Rr as desired. Concretely, we design this cluster centers by picking r random unit vectors q1, . . . , qr in Rk. We view them as the surrogate label vectors in dimension k (note that k is potentially much smaller than r). In high dimensions (technically, if *k >* 4 log r) random unit vectors q1*, . . . , q*r are pair-wise uncorrelated with inner product less than < 0.5. We associate the i-th example with the target surrogate label vector v(i) defined as follows,\n",
      "\n",
      "if $y^{(i)}=e_{j}$, then $v^{(i)}=q_{j}$.\n",
      "\n",
      "Then we will construct the matrices (A1, B1), . . . , (Aℓ, Bℓ) such that the first ℓ layers of the network maps vector h(i)\n",
      "0\n",
      "to the surrogate label vector v(i). Mathematically, we will construct (A1, B1), . . . , (Aℓ, Bℓ) such that\n",
      "\n",
      "$\\forall i\\in\\{1,\\ldots,n\\},h^{(i)}_{\\ell}=v^{(i)}\\,.$\n",
      "\n",
      "Finally we will construct the last layer TAℓ+1,Bℓ+1,bℓ+1 so that it maps the vectors q1, . . . , qr ∈ Rk\n",
      " to e1, . . . , er ∈ Rr,\n",
      "    ∀j ∈ {1, . . . , r}, qj + TAℓ+1,Bℓ+1,bℓ+1(qj) = ej .\n",
      "                                                                              (3.4)\n",
      "Putting these together, we have that by the definition (3.2) and equation (3.3), for every i, if the label is y(i) is ej, then h(i)\n",
      "              ℓ\n",
      "        will be qj.\n",
      "                                        Then by equation (3.4), we have that ˆy(i) = qj + TAℓ+1,Bℓ+1,bℓ+1(qj) = ej. Hence we obtain that ˆy(i) = y(i).\n",
      "\n",
      "The key part of this plan is the construction of the middle ℓ layers of weight matrices so that h(i)\n",
      "                                                                                                ℓ\n",
      "                                                                                                    =\n",
      " v(i). We encapsulate this into the following informal lemma. The formal statement and the full proof is deferred to Section B.\n",
      "\n",
      "Lemma 3.3 (Informal version of Lemma B.2). In the setting above, for (almost) arbitrary vectors h(1)\n",
      "0 *, . . . , h*(n)\n",
      "0\n",
      " and v(1)*, . . . , v*(n)\n",
      "∈ {q1, . . . , qr}, there exists weights matrices (A1, B1), . . . , (Aℓ, Bℓ), such that,\n",
      "\n",
      "$$\\forall i\\in\\{1,\\ldots,n\\},\\quad h_{\\ell}^{(i)}=v^{(i)}\\;.$$\n",
      "We briefly sketch the proof of the Lemma to provide intuitions, and defer the full proof to Section B. The operation that each residual block applies to the hidden variable can be abstractly written as,\n",
      "\n",
      "ˆh → h + TU,V,s(h) . (3.5)\n",
      " where h corresponds to the hidden variable before the block and ˆh corresponds to that after. We claim that for an (almost) arbitrary sequence of vectors h(1)*, . . . , h*(n), there exist T*U,V,s*(·) such that operation (3.5) transforms k vectors of h(i)'s to an arbitrary set of other k vectors that we can freely choose, and maintain the value of the rest of n − k vectors. Concretely, for any subset S of size k, and any desired vector v(i)(i ∈ S), there exist *U, V, s* such that\n",
      "\n",
      "v(i) = h(i) + TU,V,s(h(i)) ∀i ∈ S h(i) = h(i) + TU,V,s(h(i)) ∀i ̸∈ S (3.6)\n",
      "This claim is formalized in Lemma B.1. We can use it repeatedly to construct ℓ layers of building blocks, each of which transforms a subset of k vectors in {h(1)\n",
      "0 *, . . . , h*(n)\n",
      "0 } to the corresponding vectors in {v(1)*, . . . , v*(n)}, and maintains the values of the others. Recall that we have ℓ = ⌈n/k⌉\n",
      " layers and therefore after ℓ layers, all the vectors h(i)\n",
      "0 's are transformed to v(i)'s, which complete the proof sketch.\n",
      "\n",
      "## 4 Power Of All-Convolutional Residual Networks\n",
      "\n",
      "Inspired by our theory, we experimented with all-convolutional residual networks on standard image classification benchmarks.\n",
      "\n",
      "## 4.1 Cifar10 And Cifar100\n",
      "\n",
      "Our architectures for CIFAR10 and CIFAR100 are identical except for the final dimension corresponding to the number of classes 10 and 100, respectively. In Table 1, we outline our architecture.\n",
      "\n",
      "Each *residual block* has the form x + C2(ReLU(C1x)), where C1, C2 are convolutions of the specified dimension (kernel width, kernel height, number of input channels, number of output channels). The second convolution in each block always has stride 1, while the first may have stride 2 where indicated. In cases where transformation is not dimensionality-preserving, the original input x is adjusted using averaging pooling and padding as is standard in residual layers. We trained our models with the Tensorflow framework, using a momentum optimizer with momentum 0.9, and batch size is 128. All convolutional weights are trained with weight decay 0.0001.\n",
      "\n",
      "The initial learning rate is 0.05, which drops by a factor 10 and 30000 and 50000 steps. The model reaches peak performance at around 50k steps, which takes about 24h on a single NVIDIA Tesla K40 GPU. Our code can be easily derived from an open source implementation3 by removing batch normalization, adjusting the residual components and model architecture. An important departure from the code is that we initialize a residual convolutional layer of kernel size k × k and c output channels using a random normal initializer of standard deviation σ = 1/k2c, rather than 1/k√c used for standard convolutional layers. This substantially smaller weight initialization helped training, while not affecting representation. A notable difference from standard models is that the last layer is not trained, but simply a fixed random projection. On the one hand, this slightly improved test error (perhaps due to a regularizing effect). On the other hand, it means that the only trainable weights in our model are those of the convolutions, making our architecture \"all-convolutional\".\n",
      "\n",
      "3https://github.com/tensorflow/models/tree/master/resnet\n",
      "\n",
      "| variable dimensions   | initial stride                  |\n",
      "|-----------------------|---------------------------------|\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 16                    | 1                               |\n",
      "| 1 standard conv       |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 16                    |                                 |\n",
      "| ×                     |                                 |\n",
      "| 64                    | 1                               |\n",
      "| 9 residual blocks     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 64                    |                                 |\n",
      "| ×                     |                                 |\n",
      "| 128                   | 2                               |\n",
      "| 9 residual blocks     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 3                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 128                   |                                 |\n",
      "| ×                     |                                 |\n",
      "| 256                   | 2                               |\n",
      "| 9 residual blocks     |                                 |\n",
      "| -                     | -                               |\n",
      "| 8                     |                                 |\n",
      "| ×                     |                                 |\n",
      "| 8                     |                                 |\n",
      "| global average pool   |                                 |\n",
      "| 256                   |                                 |\n",
      "| ×                     |                                 |\n",
      "| num classes           |                                 |\n",
      "| -                     | random projection (not trained) |\n",
      "\n",
      "An interesting aspect of our model is that despite its massive size of 13.59 million trainable parameters, the model does not seem to overfit too quickly even though the data set size is 50000. In contrast, we found it difficult to train a model with batch normalization of this size without significant overfitting on CIFAR10. Table 2 summarizes the top-1 classification error of our models compared with a non-exhaustive list of previous works, restricted to the best previous all-convolutional result by Springenberg et al. (2014), the first residual results He et al. (2015), and state-of-the-art results on CIFAR by Huang et al. (2016). All results are with standard data augmentation.\n",
      "\n",
      "| Method                                            | CIFAR10   |   CIFAR100 |   ImageNet | remarks   |\n",
      "|---------------------------------------------------|-----------|------------|------------|-----------|\n",
      "| All-CNN                                           |           |            |            |           |\n",
      "| 7                                                 | .         |         25 |         32 | .         |\n",
      "| all-convolutional, dropout, extra data processing |           |            |            |           |\n",
      "| Ours                                              |           |            |            |           |\n",
      "| 6                                                 | .         |         38 |         24 | .         |\n",
      "| all-convolutional                                 |           |            |            |           |\n",
      "| ResNet                                            |           |            |            |           |\n",
      "| 6                                                 | .         |         43 |         25 | .         |\n",
      "| DenseNet                                          |           |            |            |           |\n",
      "| 3                                                 | .         |         74 |         19 | .         |\n",
      "| N/A                                               |           |            |            |           |\n",
      "\n",
      "## 4.2 Imagenet\n",
      "\n",
      "The ImageNet ILSVRC 2012 data set has 1, 281, 167 data points with 1000 classes. Each image is resized to 224 × 224 pixels with 3 channels. We experimented with an all-convolutional variant of the 34-layer network in He et al. (2015). The original model achieved 25.03% classification error. Our derived model has 35.7M trainable parameters. We trained the model with a momentum optimizer (with momentum 0.9) and a learning rate schedule that decays by a factor of 0.94 every two epochs, starting from the initial learning rate 0.1. Training was distributed across 6 machines updating asynchronously. Each machine was equipped with 8 GPUs (NVIDIA Tesla K40) and used batch size 256 split across the 8 GPUs so that each GPU updated with batches of size 32.\n",
      "\n",
      "In contrast to the situation with CIFAR10 and CIFAR100, on ImageNet our all-convolutional model performed significantly worse than its original counterpart. Specifically, we experienced a significant amount of *underfitting* suggesting that a larger model would likely perform better.\n",
      "\n",
      "Despite this issue, our model still reached 35.29% top-1 classification error on the test set (50000 data points), and 14.17% top-5 test error after 700, 000 steps (about one week of training). While no longer state-of-the-art, this performance is significantly better than the 40.7% reported by Krizhevsky et al. (2012), as well as the best all-convolutional architecture by Springenberg et al. (2014). We believe it is quite likely that a better learning rate schedule and hyperparameter settings of our model could substantially improve on the preliminary performance reported here.\n",
      "\n",
      "## 5 Conclusion\n",
      "\n",
      "Our theory underlines the importance of identity parameterizations when training deep artificial neural networks. An outstanding open problem is to extend our optimization result to the non-linear case where each residual has a single ReLU activiation as in our expressivity result. We conjecture that a result analogous to Theorem 2.2 is true for the general non-linear case. Unlike with the standard parameterization, we see no fundamental obstacle for such a result. We hope our theory and experiments together help simplify the state of deep learning by aiming to explain its success with a few fundamental principles, rather than a multitude of tricks that need to be delicately combined. We believe that much of the advances in image recognition can be achieved with residual convolutional layers and ReLU activations alone. This could lead to extremely simple (albeit deep) architectures that match the state-of-the-art on all image classification benchmarks.\n",
      "\n",
      "## References\n",
      "\n",
      "Antonio Auffinger, G´erard Ben Arous, and Jiˇr´ı ˇCern`y. Random matrices and complexity of spin glasses. *Communications on Pure and Applied Mathematics*, 66(2):165–201, 2013.\n",
      "P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. *Neural Netw.*, 2(1):53–58, January 1989. ISSN 0893-6080. doi: 10. 1016/0893-6080(89)90014-2.\n",
      "URL http://dx.doi.org/10.1016/0893-6080(89) 90014-2.\n",
      "Anna Choromanska, Mikael Henaff, Michael Mathieu, G´erard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In *AISTATS*, 2015.\n",
      " Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In *Advances in neural information processing systems*, pp. 2933–2941, 2014.\n",
      "I. J. Goodfellow, O. Vinyals, and A. M. Saxe. Qualitatively characterizing neural network optimization problems. *ArXiv e-prints*, December 2014.\n",
      " Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *arXiv prepring arXiv:1506.01497*, 2015.\n",
      " Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, pp. 630–645, 2016. doi: 10.1007/ 978-3-319-46493-0 38. URL http://dx.doi.org/10.1007/978-3-319-46493-0_ 38. Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016. URL http://arxiv.org/abs/1608.06993.\n",
      " Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 448–456, 2015. URL http://jmlr. org/proceedings/papers/v37/ioffe15.html.\n",
      " William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.\n",
      " H. Karimi, J. Nutini, and M. Schmidt. Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\\L{}ojasiewicz Condition. *ArXiv e-prints*, August 2016.\n",
      " K. Kawaguchi. Deep Learning without Poor Local Minima. *ArXiv e-prints*, May 2016. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In *Advances in neural information processing systems*, pp. 1097–1105, 2012.\n",
      " D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. *ArXiv e-prints*, May 2016.\n",
      " J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for Simplicity: The All Convolutional Net. *ArXiv e-prints*, December 2014.\n",
      " Eric W. Weisstein. Normal matrix, from mathworld–a wolfram web resource., 2016. URL http: //mathworld.wolfram.com/NormalMatrix.html.\n",
      "\n",
      "Wikipedia.\n",
      "               Johnsonlindenstrauss\n",
      "                                    lemma\n",
      "                                            —\n",
      "                                                wikipedia,\n",
      "                                                          the\n",
      "                                                                free\n",
      "                                                                     encyclopedia,\n",
      "                                                                                   2016.\n",
      "  URL\n",
      "            https://en.wikipedia.org/w/index.php?title=Johnson%E2%80%\n",
      " 93Lindenstrauss_lemma&oldid=743553642.\n",
      "\n",
      "## A Missing Proofs In Section 2\n",
      "\n",
      "In this section, we give the complete proofs for Theorem 2.1 and Lemma 2.4, which are omitted in Section 2.\n",
      "\n",
      "## A.1 Proof Of Theorem 2.1\n",
      "\n",
      "It turns out the proof will be significantly easier if R is assumed to be a symmetric positive semidefinite (PSD) matrix, or if we allow the variables to be complex matrices. Here we first give a proof sketch for the first special case. The readers can skip it and jumps to the full proof below. We will also prove stronger results, namely, |||A⋆||| ≤ 3γ/ℓ, for the special case.\n",
      "\n",
      "When R is PSD, it can be diagonalized by orthonormal matrix U in the sense that R = *UZU* ⊤, where Z = diag(z1*, . . . , z*d) is a diagonal matrix with non-negative diagonal entries z1*, . . . , z*d. Let A⋆\n",
      "1 = · · · = A⋆\n",
      "ℓ = U diag(z1/ℓ\n",
      "i\n",
      ")U ⊤ − Id, then we have\n",
      "\n",
      "(Id + A⋆\n",
      "        ℓ) · · · (Id + A⋆\n",
      "                      1) = (U diag(z1/ℓ\n",
      "                                      i\n",
      "                                         )U ⊤)ℓ = U diag(z1/ℓ\n",
      "                                                             i\n",
      "                                                                )ℓU\n",
      "                                                                            (since U ⊤U = Id)\n",
      "\n",
      "= UZU ⊤ = R .\n",
      "\n",
      "We see that the network defined by A⋆ reconstruct the transformation R, and therefore it's a global minimum of the population risk (formally see Claim 2.3 below). Next, we verify that each of the A⋆\n",
      "                                                                                              j\n",
      "has small spectral norm:\n",
      "\n",
      "$$\\|A_{j}^{*}\\|=\\|{\\rm Id}-U\\,{\\rm diag}(z_{i}^{1/\\ell})U^{\\top})\\|=\\|U({\\rm Id}-{\\rm diag}(z_{i})^{1/\\ell})U^{\\top}\\|=\\|{\\rm Id}-{\\rm diag}(z_{i})^{1/\\ell}\\|$$ (since $$U$$ is orthonormal) $$=\\max_{i}|z_{i}^{1/\\ell}-1|\\,.$$ (A.1)\n",
      "Since σmin(R) ≤ zi ≤ σmax(R), we have ℓ ≥ 3γ *≥ |* log zi|. It follows that\n",
      "\n",
      "|z1/ℓ\n",
      "  i\n",
      "       − 1| = |e(log zi)/ℓ − 1| ≤ 3|(log zi)/ℓ| ≤ 3γ/ℓ .\n",
      "                                                                                   (since |ex − 1| ≤ 3|x| for all |x| ≤ 1)\n",
      "\n",
      "Then using equation (A.1) and the equation above, we have that |||A*||| ≤* maxj∥A⋆\n",
      "j∥ ≤ 3γ/ℓ, which completes the proof for the special case. Next we give the formal full proof of Theorem 2.1.\n",
      "\n",
      "Proof of Theorem 2.1. We assume the dimension d is an even number. The odd case has very similar proof and is left to the readers. Let R = *UKV* ⊤ be its singular value decomposition, where U,V are two orthonormal matrices and K is a diagonal matrix. Since U is a normal matrix (that is, U satisfies that UU ⊤ = U ⊤U), by Claim C.1, we have that U can be block-diagnolaized by orthonormal matrix S into U = *SDS*−1, where D = diag(D1*, . . . , D*d/2) is a real block diagonal matrix with each block Di being of size 2 × 2.\n",
      "\n",
      "Since U is orthonormal, U has all its eigenvalues lying on the unit circle (in complex plane). Since D\n",
      " and U are unitarily similar to each other, D also has eigenvalues lying on the unit circle, and so does each of the block Di. This means that each Di is a 2 × 2 dimensional rotation matrix. Each rotation matrix can be written as T(θ) =\n",
      "�\n",
      "cos θ\n",
      "− sin θ\n",
      "sin θ\n",
      "cos θ\n",
      "�\n",
      ". Suppose Di = T(θi) where θi ∈ [−*π, π*]. Then we have that Di = T(θi/q)q for any integer q (that is chosen later). Let W = diag(T(θi/q)).\n",
      "\n",
      "Therefore, it follows that D = diag(Di) = W q. Moreover, we have U = *SDS*−1 = (SWS−1)q.\n",
      "\n",
      "Therefore, let B1 = B2 = · · · = Bq = Id − *SWS*−1, then we have U = (Id + Bq) *. . .* (Id + B1).\n",
      "\n",
      "We verify the spectral norm of these matrices are indeed small,\n",
      "\n",
      "$$\\|B_{j}\\|=\\left\\|\\mathrm{Id}-SWS^{-1}\\right\\|=\\left\\|S(\\mathrm{Id}-W)S^{-1}\\right\\|$$ $$=\\|\\mathrm{Id}-W\\|$$ (since $$S$$ is unitary) $$=\\max_{i\\in[d/2]}\\|T(0)-T(\\theta_{i}/q)\\|$$ (since $$W=\\mathrm{diag}(T(\\theta_{i}/q))$$ is block diagonal) $$=\\max|\\sin(\\theta_{i}/q)|\\leq\\pi/q\\,.$$\n",
      "\n",
      "Similarly, we can choose B′\n",
      "                          1, . . . , B′\n",
      "                                  q with ∥Cj∥ ≤ π/q so that V ⊤ = (Id + B′\n",
      "                                                                           q) . . . (Id + B′\n",
      "                                                                                        1).\n",
      "\n",
      "Last, we deal with the diagonal matrix K.\n",
      "                                            Let K\n",
      "                                                    =\n",
      "                                                        diag(ki).\n",
      "                                                                   We have min ki\n",
      "                                                                                    =\n",
      " σmin(R), max ki = σmax(R). Then, we can write K = (K′)p where K′ = diag(k1/p\n",
      "                                                                            i\n",
      "                                                                               ) and p\n",
      " is an integer to be chosen later. We have that ∥K′ − Id∥ ≤ max |k1/p\n",
      "                                                         i\n",
      "                                                             − 1| ≤ max |elog ki·1/p − 1|.\n",
      "When p ≥ γ = max{log max ki, − log min ki} = max{log σmax(R), − log σmin(R)}, we have\n",
      "that\n",
      "\n",
      "$\\|K^{\\prime}-\\operatorname{Id}\\|\\leq\\max|e^{\\log k_{i}\\cdot1/p}-1|\\leq3\\max|\\log k_{i}\\cdot1/p|=3\\gamma/p$.\n",
      "\n",
      "(since $|e^{x}-1|\\leq3|x|$ for $|x|\\leq1$)\n",
      "\n",
      "Let B′′\n",
      "     1 = · · · = B′′\n",
      "                  p = K′ − Id and then we have K = (Id + B′′\n",
      " p) · · · (Id + B′′\n",
      " 1 ). Finally, we\n",
      "choose p = ℓ√3γ\n",
      "\n",
      " 2(√π+√3γ) and q =\n",
      " ℓ√π\n",
      " √π+√3γ , 4 and let A2p+q = Bq, · · · = Ap+q+1 = B1, Ap+q = B′′\n",
      " p, . . . , Aq+1 = B′′\n",
      " 1 , Aq = B′\n",
      " q, . . . , A1 = B′\n",
      " 1. We have that 2q + ℓ = 1 and\n",
      "\n",
      "$$R=U K V^{\\top}=(\\mathrm{Id}+A_{\\ell})\\dots(\\mathrm{Id}+A_{1})\\,.$$\n",
      "\n",
      "Moreover, we have |||A||| ≤ max{∥Bj∥, ∥B′\n",
      "  j∥.∥B′′\n",
      "  j ∥} ≤ π/q + 3γ/p ≤ 2(√π + √3γ)2/ℓ, as desired.\n",
      "\n",
      "## A.2 Proof Of Lemma 2.4\n",
      "\n",
      "We compute the partial gradients by definition. Let ∆j ∈ Rd×d be an infinitesimal change to Aj.\n",
      "\n",
      "Using Claim 2.3, consider the Taylor expansion of f(A1*, . . . , A*ℓ + ∆j*, . . . , A*ℓ)\n",
      "f(A1*, . . . , A*ℓ + ∆j*, . . . , A*ℓ)\n",
      "\n",
      "F = ���((Id + Aℓ) *· · ·* (Id + Aj + ∆j) *. . .* (Id + A1) − R)Σ1/2��� 2 F = ���((Id + Aℓ) *· · ·* (Id + A1) − R)Σ1/2 + (Id + Aℓ) · · · ∆j *. . .* (Id + A1)Σ1/2��� 2 F + 2⟨((Id + Aℓ) · · · (Id + A1) − R)Σ1/2, (Id + Aℓ) · · · ∆j *. . .* (Id + A1)Σ1/2⟩ + O(∥∆j∥2 F ) = ���(Id + Aℓ) *· · ·* (Id + A1) − R)Σ1/2��� 2 = f(A) + 2⟨(Id + A⊤ ℓ ) . . . (Id + A⊤ j+1)EΣ(Id + A⊤ j−1) . . . (Id + A⊤ 1 ), ∆j⟩ + O(∥∆j∥2 F ) .\n",
      "\n",
      "By definition, this means that the\n",
      "                                   ∂f\n",
      "                                   ∂Aj = 2(Id + A⊤\n",
      "                                                    ℓ ) . . . (Id + A⊤\n",
      "                                                                   j+1)EΣ(Id + A⊤\n",
      "                                                                                   j−1) . . . (Id +\n",
      "A⊤\n",
      " 1 ).\n",
      "\n",
      "## B Missing Proofs In Section 3\n",
      "\n",
      "In this section, we provide the full proof of Theorem 3.2. We start with the following Lemma that constructs a building block T that transform k vectors of an arbitrary sequence of n vectors to any arbitrary set of vectors, and main the value of the others. For better abstraction we use α(i),β(i) to denote the sequence of vectors.\n",
      "\n",
      "Lemma B.1. Let S ⊂ [n] be of size k*. Suppose* α(1), . . . , α(n) is a sequences of n vectors satisfying a) for every 1 ≤ i ≤ n, we have 1−ρ′ ≤ ∥αi∥2 ≤ 1+ρ′, and b) if i ̸= j and S contains at least one of i, j, then ∥α(i) −β(j)∥ ≥ 3ρ′*. Let* β(1), . . . , β(n) be an arbitrary sequence of vectors. Then, there exists U, V ∈ Rk×k, s such that for every i ∈ S, we have T*U,V,s*(α(i)) = β(i) − α(i), and moreover, for every i ∈ [n]\\S we have T*U,V,s*(α(i)) = 0.\n",
      "\n",
      "We can see that the conclusion implies\n",
      "\n",
      "$$\\beta^{(i)}=\\alpha^{(i)}+\\mathcal{T}_{U,V,s}(\\alpha^{(i)})\\ \\ \\forall i\\in S$$ $$\\alpha^{(i)}=\\alpha^{(i)}+\\mathcal{T}_{U,V,s}(\\alpha^{(i)})\\ \\ \\forall i\\not\\in S$$\n",
      "\n",
      "which is a different way of writing equation (3.6).\n",
      "\n",
      "Proof of Lemma B.1. Without loss of generality, suppose S = {1*, . . . , k*}. We construct *U, V, s* as follows. Let the i-th row of U be α(i) for i ∈ [k], and let s = −(1 − 2ρ′) · 1 where 1 denotes the all\n",
      "1's vector. Let the i-column of V be\n",
      "1\n",
      "∥α(i)∥2−(1−2ρ′)(β(i) − α(i)) for i ∈ [k].\n",
      "\n",
      "Next we verify that the correctness of the construction. We first consider 1 ≤ i ≤ k. We have that Uα(i) is a a vector with i-th coordinate equal to ∥α(i)∥2 ≥ 1 − ρ′. The j-th coordinate of Uα(i) is equal to ⟨α(j), α(i)⟩, which can be upperbounded using the assumption of the Lemma by\n",
      "\n",
      "2 ⟨α(j), α(i)⟩ = 1 � ∥α(i)∥2 + ∥α(j)∥2� − ∥α(i) − α(j)∥2 ≤ 1 + ρ′ − 3ρ′ ≤ 1 − 2ρ′ . (B.1)\n",
      "\n",
      "Therefore, this means Uα(i) − (1 − 2ρ′) · 1 contains a single positive entry (with value at least ∥α(i)∥2 − (1 − 2ρ′) ≥ ρ′), and all other entries being non-positive.\n",
      " This means that ReLu(Uα(i) +b) = �\n",
      " ∥α(i)∥2 − (1 − 2ρ′)\n",
      " �\n",
      " ei where ei is the i-th natural basis vector. It follows that V ReLu(Uα(i) + b) = (∥α(i)∥2 − (1 − 2ρ′)) V ei = β(i) − α(i).\n",
      "\n",
      "Finally, consider n ≥ i > k. Then similarly to the computation in equation (B.1), Uα(i) is a vector with all coordinates less than 1 − 2ρ′. Therefore Uα(i) + b is a vector with negative entries. Hence we have ReLu(Uα(i) + b) = 0, which implies V ReLu(Uα(i) + b) = 0.\n",
      "\n",
      "Now we are ready to state the formal version of Lemma 3.3.\n",
      "\n",
      "**Lemma B.2**.: _Suppose a sequence of $n$ vectors $z^{(1)},\\ldots,z^{(n)}$ satisfies a relaxed version of Assumption 3.1: a for every $i,1=-\\rho^{\\prime}\\leq\\|z^{(i)}\\|^{2}\\leq1+\\rho^{\\prime}\\,b)$ for every $i\\neq j$, we have $\\|z^{(i)}-z^{(j)}\\|^{2}\\geq\\rho^{\\prime}$; Let $v^{(1)},\\ldots,v^{(n)}$ be defined above. Then there exists weigh matrices $(A_{1},B_{1}),\\ldots,(A_{\\ell},B_{\\ell})$, such that given $\\forall i,h_{0}^{(i)}=z^{(i)}$, we have_\n",
      "\n",
      "$$\\forall i\\in\\{1,\\ldots,n\\},\\ \\ h_{\\ell}^{(i)}=v^{(i)}\\,.$$\n",
      "We will use Lemma B.1 repeatedly to construct building blocks TAj,Bk,sj(·), and thus prove Lemma B.2. Each building block TAj,Bk,sj(·) takes a subset of k vectors among {z(1), . . . , z(n)}\n",
      " and convert them to v(i)'s, while maintaining all other vectors as fixed. Since they are totally n/k layers, we finally maps all the z(i)'s to the target vectors v(i)'s.\n",
      "\n",
      "Proof of Lemma B.2. We use Lemma B.1 repeatedly. Let S1 = [1*, . . . , k*]. Then using Lemma B.1\n",
      " with α(i) = z(i) and β(i) = v(i) for i ∈ [n], we obtain that there exists A1, B1, b1 such that for i ≤ k, it holds that h(i)\n",
      "1\n",
      "= z(i) + TA1,B1,b1(z(i)) = v(i), and for i ≥ k, it holds that h(i)\n",
      "1\n",
      "=\n",
      " z(i) + TA1,B1,b1(z(i)) = z(i).\n",
      "\n",
      "Now we construct the other layers inductively. We will construct the layers such that the hidden variable at layer j satisfies h(i)\n",
      "j\n",
      "= v(i) for every 1 ≤ i ≤ jk, and h(i)\n",
      "j\n",
      "= z(i) for every n ≥ i >\n",
      " jk. Assume that we have constructed the first j layer and next we use Lemma B.1 to construct the j + 1 layer. Then we argue that the choice of α(1) = v(1)*, . . . , α*(jk) = v(jk), α(jk+1) =\n",
      " z(jk+1)*, . . . , α*(n) = z(n), and S = {jk + 1*, . . . ,* (j + 1)k} satisfies the assumption of Lemma B.1.\n",
      "\n",
      "Indeed, because qi's are chosen uniformly randomly, we have w.h.p for every s and i, ⟨qs, z(i)⟩ ≤\n",
      " 1 − ρ′. Thus, since v(i) ∈ {q1*, . . . , q*r}, we have that v(i) also doesn't correlate with any of the z(i).\n",
      "\n",
      "Then we apply Lemma B.1 and conclude that there exists Aj+1 = U, Bj+1 = *V, b*j+1 = s such that TAj+1,bj+1,bj+1(v(i)) = 0 for i ≤ jk, TAj+1,bj+1,bj+1(z(i)) = v(i) − z(i) for *jk < i* ≤ (j + 1)k, and TAj+1,bj+1,bj+1(z(i)) = 0 for n ≥ *i >* (j + 1)k. These imply that\n",
      "\n",
      "$$h^{(i)}_{j+1}=h^{(i)}_{j}+\\mathcal{T}_{A_{j+1},b_{j+1},b_{j+1}}(v^{(i)})=v^{(i)}\\quad\\forall1\\leq i\\leq jk$$ $$h^{(i)}_{j+1}=h^{(i)}_{j}+\\mathcal{T}_{A_{j+1},b_{j+1},b_{j+1}}(z^{(i)})=v^{(i)}\\quad\\forall jk+1\\leq i\\leq(j+1)k$$ $$h^{(i)}_{j+1}=h^{(i)}_{j}+\\mathcal{T}_{A_{j+1},b_{j+1},b_{j+1}}(z^{(i)})=z^{(i)}\\quad\\forall(j+1)k<i\\leq n$$\n",
      "\n",
      "Therefore we constructed the $j+1$ layers that meets the inductive hypothesis for layer $j+1$. Therefore, by induction we get all the layers, and the last layer satisfies that $h^{(i)}_{t}=v^{(i)}$ for every example $i$.\n",
      "\n",
      "Now we ready to prove Theorem 3.2, following the general plan sketched in Section 3. Proof of Theorem 3.2. We use formalize the intuition discussed below Theorem 3.2. First, take k = c(log n)/ρ2 for sufficiently large absolute constant c (for example, c = 10 works), by Johnson- Lindenstrauss Theorem (Johnson & Lindenstrauss (1984), or see Wikipedia (2016)) we have that when A0 is a random matrix with standard normal entires, with high probability, all the pairwise distance between the the set of vectors {0, x(1)*, . . . , x*(n)} are preserved up to 1 ± ρ/3 factor. That is, we have that for every i, 1−ρ/3 ≤ ∥A0x(i)∥ ≤ 1+ρ/3, and for every i ̸= j, ∥A0x(i)−A0x(j)∥ ≥ ρ(1 − ρ/3) ≥ 2ρ/3. Let z(i) = A0x(i) and ρ′ = ρ/3. Then we have z(i)'s satisfy the condition of Lemam B.2. We pick r random vectors q1*, . . . , q*r in Rk. Let v(1)*, . . . , v*(n) be defined as in equation (3.2). Then by Lemma B.2, we can construct matrices (A1, B1), . . . , (Aℓ, Bℓ) such that\n",
      "\n",
      "$h_{\\ell}^{(i)}=v^{(i)}$.\n",
      "\n",
      "Note that v(i) ∈ {q1*, . . . , q*r}, and qi's are random unit vector. Therefore, the choice of α(1) = q1*, . . . , α*(r) = qr, β(1) = e1*, . . . , β*(r) = er, and satisfies the condition of Lemma B.1, and using Lemma B.1 we conclude that there exists Aℓ+1, Bℓ+1, sℓ+1 such that\n",
      "\n",
      "$e_{j}=v_{j}+\\mathcal{T}_{A_{\\ell+1},B_{\\ell+1},b_{\\ell+1}}(v_{j}),\\ \\text{for every}\\ j\\in\\{1,\\ldots,r\\}\\,.$ (B.3)\n",
      "\n",
      "By the definition of v(i) in equation (3.2) and equation (B.2), we conclude that ˆy(i) = h(i)\n",
      "                                                                                           ℓ\n",
      "                                                                                              +\n",
      "TAℓ+1,Bℓ+1,bℓ+1(h(i)\n",
      "                 ℓ ) = y(i)., which complete the proof.\n",
      "\n",
      "## C Toolbox\n",
      "\n",
      "In this section, we state two folklore linear algebra statements. The following Claim should be known, but we can't find it in the literature. We provide the proof here for completeness.\n",
      "\n",
      "Claim C.1. Let U ∈ Rd×d be a real normal matrix (that is, it satisfies UU ⊤ = U ⊤U). Then, there exists an orthonormal matrix S ∈ Rd×d such that U = SDS⊤ , where D is a real block diagonal matrix that consists of blocks with size at most 2 × 2. Moreover, if d is even, then D consists of blocks with size exactly 2 × 2.\n",
      "\n",
      "Proof. Since U is a normal matrix, it is unitarily diagonalizable (see\n",
      "                                                 Weisstein (2016) for backgrounds).\n",
      " Therefore, there exists unitary matrix V in Cd×d and diagonal matrix in Cd×d such that U has eigen-decomposition U=\n",
      " V ΛV ∗.\n",
      " Since U itself is a real ma- trix, we have that the eigenvalues (the diagonal entries of Λ) come as conjugate pairs, and so do the eigenvectors (which are the columns of V ).\n",
      " That is, we can group the columns of V into pairs (v1, ¯v1), . . . , (vs, ¯vs), vs+1, . . . , vt, and let the corresponding eigenval- ues be λ1, ¯λ1, . . . , λλs, ¯λs, λs+1, . . . , λt.\n",
      " Here λs+1, . . . , λt ∈ R.\n",
      "Then we get that U\n",
      "                                                                          =\n",
      "�s\n",
      "  i=1 2ℜ(viλiv∗\n",
      "           i ) + �t\n",
      "         i=s+1 viλiv⊤\n",
      "  i . Let Qi = ℜ(viλiv∗\n",
      "           i ), then we have that Qi is a real matrix of rank-2. Let Si ∈ Rd×2 be a orthonormal basis of the column span of Qi and then we have that Qi can be written as Qi = SiDiS⊤\n",
      " i where Di is a 2 × 2 matrix. Finally, let S = [S1, . . . , Ss, vs+1, . . . , vt], and D = diag(D1, . . . , Ds, λs+1, . . . , λt) we complete the proof.\n",
      "\n",
      "The following Claim is used in the proof of Theorem 2.2. We provide a proof here for completeness.\n",
      "\n",
      "Claim C.2 (folklore). For any two matrices A, B ∈ Rd×d, we have that\n",
      "\n",
      "∥AB∥F ≥ σmin(A)∥B∥F .\n",
      "\n",
      "Proof. Since σmin(A)2 is the smallest eigenvalue of A⊤A, we have that\n",
      "\n",
      "$$B^{\\top}A^{\\top}A B\\succeq B^{\\top}\\cdot\\sigma_{\\operatorname*{min}}(A)^{2}\\mathbf{Id}\\cdot B\\,.$$\n",
      "Therefore, it follows that\n",
      "\n",
      "$$\\|AB\\|_{F}^{2}=\\text{tr}(B^{\\top}A^{\\top}AB)\\geq\\text{tr}(B^{\\top}\\cdot\\sigma_{\\min}(A)^{2}\\text{Id}\\cdot B)$$ $$=\\sigma_{\\min}(A)^{2}\\text{tr}(B^{\\top}B)=\\sigma_{\\min}(A)^{2}\\|B\\|_{F}^{2}\\,.$$\n",
      "\n",
      "Taking square root of both sides completes the proof.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\", example_test[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97c0c026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      " Characterizing and Measuring the Similarity of Neural Networks with Persistent Homology .\n",
      "\n",
      "Abstract:\n",
      " Characterizing the structural properties of neural networks is crucial yet poorly understood, and there are no well-established similarity measures between networks. In this work, we observe that neural networks can be represented as abstract simplicial complex and analyzed using their topological 'fingerprints' via Persistent Homology (PH). We then describe a PH-based representation proposed for characterizing and measuring similarity of neural networks. We empirically show the effectiveness of this representation as a descriptor of different architectures in several datasets. This approach based on Topological Data Analysis is a step towards better understanding neural networks and a useful similarity measure.\n",
      "\n",
      "Main Text:\n",
      " Introduction\n",
      "\n",
      "Machine learning practitioners can train different neural networks for the same task. Even for the same neural architecture, there are many hyperparameters, such as the number of neurons per layer or the number of layers. Moreover, the final weights for the same architecture and hyperparameters can vary depending on the initialization and the optimization process itself, which is stochastic. Thus, there is no direct way of comparing neural networks accounting for the fact that neur ...\n"
     ]
    }
   ],
   "source": [
    "sections = parse_input_sections(example_train[\"input\"])\n",
    "print(\"Title:\\n\", sections[\"title\"])\n",
    "print(\"\\nAbstract:\\n\", sections[\"abstract\"])\n",
    "print(\"\\nMain Text:\\n\", sections[\"main_text\"][:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff232b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: The paper begins by presenting a simple analysis for deep linear networks. This is more to demonstrate the intuitions behind their derivations, and does not have practical relevance. They then extend to non-linear resnets with ReLU units and demonstrate that they have finite sample expressivity. They formally establish these results. Inspired by their theory, they perform experiments using simpler architectures without any batch norm, dropout or other regularizations and fix the last layer and still attain competitive results. Indeed, they admit that data augmentation is a form of regularization and can replace other regularization schemes. \n",
      " I think the paper meets the threshold to be accepted. Final decision: Accept (Poster)\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\", example_test[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d757b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n"
     ]
    }
   ],
   "source": [
    "print(\"System:\", example_test[\"system\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7e27ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: [[\"Role: Reviewer 1. Style: Strict. Task: Provide a critical review based on the paper provided, including a summary, strengths, weaknesses, and any questions you have. Please provide your review of 'Identity Matters in Deep Learning'\", \"Summary: Paper Summary:\\n\\nAuthors investigate identity re-parametrization in the linear and the non linear case. \\n\\nDetailed comments:\\n\\n— Linear Residual Network:\\n\\nThe paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. \\n\\n — Non linear Residual Network:\\n\\nAuthors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. \\n\\n1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify \\n\\n2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?\\nIn the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  \\n\\n3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? \\nA simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).\\n\\n4- What does the construction tell us about the number of layers? \\n\\n5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block.\\nI don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction\\ndoes not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. \\n\\n— Experiments : \\n\\n- last layer is not trained means the layer before the linear layer preceding the softmax?\\n\\nMinor comments:\\n\\nAbstract:  how  the identity mapping motivated batch normalization?\\n Question: Can you please clarify or discuss those  points?\\n\\n- Would the results in Theo 2.1 and 2.2 hold when input and output (x and y) have different dimensions? \\n\\n- Theo 3.2 ensures overfitting the training set under the assumption that all points can not be too close. can you say anything about f(x+delta) how stable is  your construction, or any generalization or stability statement?\\n\\n- did you do any experimentation with your construction on real or synthetic data under your data assumption?\\n would be interesting to see some empirical results using your construction and not back propagation.\\nwould be interesting to see on those data when model underfits or overfits.\\n\\n\\n\"], [\"Role: Author. Task: Respond to Reviewer 1's comments by clarifying the mentioned weaknesses and answering the posed questions. Summarize the Reviewer 1's comment above and provide a response accordingly.\", \"Author does't have any rebuttal.\"], [\"Role: Reviewer 1. Style: Strict. Task: Based on the author's response, provide a final score from 1 to 10 and a confidence from 1 to 5.\", \"Reviewer1 does't have any response to the Author. Score: 5: Marginally below acceptance threshold Confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct\"], [\"Role: Reviewer 1. Style: Toletant. Task: Provide a critical review based on the paper provided, including a summary, strengths, weaknesses, and any questions you have. Please provide your review of 'Identity Matters in Deep Learning'\", \"Summary: This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper:\\n\\n-Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks.\\n\\n-Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this.\\n\\n-Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer.\\n\\nMinor comments:\\n\\n1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea.\\n\\n2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1\\n\\n  Question: How different is the result when you learn the top layer instead of random projection? Could it be the main source of under-fitting for ImageNet simply because you have many more classes? Also, have you tried optimizing without weight decay?\\n\"], [\"Role: Author. Task: Respond to Reviewer 1's comments by clarifying the mentioned weaknesses and answering the posed questions. Summarize the Reviewer 1's comment above and provide a response accordingly.\", \"Author does't have any rebuttal.\"], [\"Role: Reviewer 1. Style: Toletant. Task: Based on the author's response, provide a final score from 1 to 10 and a confidence from 1 to 5.\", \"Reviewer1 does't have any response to the Author. Score: 6: Marginally above acceptance threshold Confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature\"], [\"Role: Reviewer 1. Style: Toletant. Task: Provide a critical review based on the paper provided, including a summary, strengths, weaknesses, and any questions you have. Please provide your review of 'Identity Matters in Deep Learning'\", 'Summary: This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.\\n\\nOne of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. \\n\\nMinors: one line before Eq. (3.1), U \\\\in R ? \\\\times k\\n\\n Question: '], [\"Role: Author. Task: Respond to Reviewer 1's comments by clarifying the mentioned weaknesses and answering the posed questions. Summarize the Reviewer 1's comment above and provide a response accordingly.\", \"Author does't have any rebuttal.\"], [\"Role: Reviewer 1. Style: Toletant. Task: Based on the author's response, provide a final score from 1 to 10 and a confidence from 1 to 5.\", \"Reviewer1 does't have any response to the Author. Score: 8: Top 50% of accepted papers, clear accept Confidence: 3: The reviewer is fairly confident that the evaluation is correct\"]]\n"
     ]
    }
   ],
   "source": [
    "print(\"History:\", example_test[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0e30873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(example_test[\"history\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f66fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Reviewer 1. Style: Strict. Task: Provide a critical review based on the paper provided, including a summary, strengths, weaknesses, and any questions you have. Please provide your review of 'Identity Matters in Deep Learning'\", \"Summary: Paper Summary:\\n\\nAuthors investigate identity re-parametrization in the linear and the non linear case. \\n\\nDetailed comments:\\n\\n— Linear Residual Network:\\n\\nThe paper shows that for a linear residual network any critical point is a global optimum. This problem is non convex it is interesting that this simple re-parametrization leads to such a result. \\n\\n — Non linear Residual Network:\\n\\nAuthors propose a construction that maps the points to their labels via a resnet , using an initial random projection, followed by a residual block that clusters the data based on their label, and a last layer that maps the clusters to the label. \\n\\n1- In Eq 3.4  seems the dimensions are not matching q_j in R^k and e_j in R^r. please clarify \\n\\n2- The construction seems fine, but what is special about the resnet here in this construction? One can do a similar construction if we did not have the identity? can you discuss this point?\\nIn the linear case it is clear from a spectral point of view how the identity is helping the optimization. Please provide some intuition.  \\n\\n3-   Existence of a network in the residual  class that overfits does it give us any intuition on why residual network outperform other architectures? What does an existence result of such a network tell us about its representation power ? \\nA simple linear model under the assumption that points can not be too close can overfit the data, and get fast convergence rate (see for instance tsybakov noise condition).\\n\\n4- What does the construction tell us about the number of layers? \\n\\n5- clustering the activation independently from the label, is an old way to pretrain the network. One could use those centroids as weights for the next layer (this is also related to Nystrom approximation see for instance https://www.cse.ust.hk/~twinsen/nystrom.pdf ). Your clustering is very strongly connected to the label at each residual block.\\nI don't think this is appealing or useful since no feature extraction is happening. Moreover the number of layers in this construction\\ndoes not matter. Can you weaken the clustering to be independent to the label at least in the early layers? then one could you use your construction as an initialization in the training. \\n\\n— Experiments : \\n\\n- last layer is not trained means the layer before the linear layer preceding the softmax?\\n\\nMinor comments:\\n\\nAbstract:  how  the identity mapping motivated batch normalization?\\n Question: Can you please clarify or discuss those  points?\\n\\n- Would the results in Theo 2.1 and 2.2 hold when input and output (x and y) have different dimensions? \\n\\n- Theo 3.2 ensures overfitting the training set under the assumption that all points can not be too close. can you say anything about f(x+delta) how stable is  your construction, or any generalization or stability statement?\\n\\n- did you do any experimentation with your construction on real or synthetic data under your data assumption?\\n would be interesting to see some empirical results using your construction and not back propagation.\\nwould be interesting to see on those data when model underfits or overfits.\\n\\n\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad351484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The paper combines the idea from the classical hidden dirichlet processe and CGMM to automate the process of selecting the hyperparameters of CGMM.\n",
      "Interesting idea but not very impressive experimental results. Strengths and Weaknesses: The authors propose an interesting idea of utilizing the HDP framework to obtain the number of latent variables at each layer. HOwever the gain in the accuracy seems either non-existent or marginal.\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99ad9e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " The paper combines the idea from the classical hidden dirichlet processe and CGMM to automate the process of selecting the hyperparameters of CGMM.\n",
      "Interesting idea but not very impressive experimental results.\n",
      "\n",
      "Strengths and Weaknesses:\n",
      " The authors propose an interesting idea of utilizing the HDP framework to obtain the number of latent variables at each layer. HOwever the gain in the accuracy seems either non-existent or marginal.\n",
      "\n",
      "Questions: Not provided\n"
     ]
    }
   ],
   "source": [
    "review_text = example_test[\"history\"][0][1]\n",
    "sections = parse_review_sections(review_text)\n",
    "\n",
    "print(\"Summary:\\n\", sections[\"summary\"])\n",
    "print(\"\\nStrengths and Weaknesses:\\n\", sections[\"strengths_and_weaknesses\"])\n",
    "if sections[\"questions\"]:\n",
    "    print(\"\\nQuestions:\\n\", sections[\"questions\"])\n",
    "else:\n",
    "    print(\"\\nQuestions: Not provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a41866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Author. Task: Respond to Reviewer 1's comments by clarifying the mentioned weaknesses and answering the posed questions. Summarize the Reviewer 1's comment above and provide a response accordingly.\", \"Author does't have any rebuttal.\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5009609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Reviewer 1. Style: Strict. Task: Based on the author's response, provide a final score from 1 to 10 and a confidence from 1 to 5.\", \"Reviewer1 does't have any response to the Author. Score: 5: Marginally below acceptance threshold Confidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7083513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Reviewer 1. Style: Toletant. Task: Provide a critical review based on the paper provided, including a summary, strengths, weaknesses, and any questions you have. Please provide your review of 'Identity Matters in Deep Learning'\", \"Summary: This paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h(x)+x instead of h(x). This has been shown to perform well in practice (eg. ResNet). The discussions and experiments in the paper are interesting. Here's a few comments on the paper:\\n\\n-Section 2: Studying the linear networks is interesting by itself. However, it is not clear that how this could translate to any insight about non-linear networks. For example, you have proved that every critical point is global minimum. I think it is helpful to add some discussion about the relationship between linear and non-linear networks.\\n\\n-Section 3: The construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and I don't see why we need a different proof given all the results on finite sample expressivity of feedforward networks. I appreciate if you clarify this.\\n\\n-Section 4: I like the experiments. The choice of random projection on the top layer is brilliant. However, since you have combined this choice with all-convolutional residual networks, it is hard for the reader to separate the affect of each of them. Therefore, I suggest reporting the numbers for all-convolutional residual networks with learned top layer and also ResNet with random projection on the top layer.\\n\\nMinor comments:\\n\\n1- I don't agree that Batch Normalization can be reduced to identity transformation and I don't know if bringing that in the abstract without proper discussion is a good idea.\\n\\n2- Page 5 above assumption 3.1 : x^(i)=1 ==> ||x^(i)||_2=1\\n\\n  Question: How different is the result when you learn the top layer instead of random projection? Could it be the main source of under-fitting for ImageNet simply because you have many more classes? Also, have you tried optimizing without weight decay?\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c49ee1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Author. Task: Respond to Reviewer 1's comments by clarifying the mentioned weaknesses and answering the posed questions. Summarize the Reviewer 1's comment above and provide a response accordingly.\", \"Author does't have any rebuttal.\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abca6b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Reviewer 1. Style: Toletant. Task: Based on the author's response, provide a final score from 1 to 10 and a confidence from 1 to 5.\", \"Reviewer1 does't have any response to the Author. Score: 6: Marginally above acceptance threshold Confidence: 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fe34693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Reviewer 1. Style: Toletant. Task: Provide a critical review based on the paper provided, including a summary, strengths, weaknesses, and any questions you have. Please provide your review of 'Identity Matters in Deep Learning'\", 'Summary: This paper provides some theoretical guarantees for the identity parameterization by showing that 1) arbitrarily deep linear residual networks have no spurious local optima; and 2) residual networks with ReLu activations have universal finite-sample expressivity. This paper is well written and studied a fundamental problem in deep neural network. I am very positive on this paper overall and feel that this result is quite significant by essentially showing the stability of auto-encoder, given the fact that it is hard to provide concrete theoretical guarantees for deep neural networks.\\n\\nOne of key questions is how to extent the result in this paper to the more general nonlinear actuation function case. \\n\\nMinors: one line before Eq. (3.1), U \\\\in R ? \\\\times k\\n\\n Question: ']\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "780874a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Author. Task: Respond to Reviewer 1's comments by clarifying the mentioned weaknesses and answering the posed questions. Summarize the Reviewer 1's comment above and provide a response accordingly.\", \"Author does't have any rebuttal.\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f83f36b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Role: Reviewer 1. Style: Toletant. Task: Based on the author's response, provide a final score from 1 to 10 and a confidence from 1 to 5.\", \"Reviewer1 does't have any response to the Author. Score: 8: Top 50% of accepted papers, clear accept Confidence: 3: The reviewer is fairly confident that the evaluation is correct\"]\n"
     ]
    }
   ],
   "source": [
    "print(example_test[\"history\"][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653edb0",
   "metadata": {},
   "source": [
    "## Baseline LLM only\n",
    "\n",
    "Gives you a running system in <100 lines, a sanity‑check for your metric code, and a performance floor you’ll later try to beat.\n",
    "\n",
    "• Draft a prompt template that mirrors real reviews (summary, strengths, weaknesses, score). <br>\n",
    "• CLI script: generate_review.py --model gpt-4o --paper_id X → JSON review. <br>\n",
    "• Serialize outputs for 50–100 papers. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12ee47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "717ce7d0",
   "metadata": {},
   "source": [
    "## Evaluation harness\n",
    "\n",
    "You want tight feedback loops. Implementing metrics now lets you plug in future variants with one line of code.\n",
    "\n",
    "\n",
    "• Automatic overlap metrics (BERTScore, ROUGE). <br>\n",
    "• Placeholder hooks for factual‑consistency checks (e.g., using GPT‑4 to judge). <br>\n",
    "• Skeleton for a human‑rating spreadsheet or web form. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d379b",
   "metadata": {},
   "source": [
    "## RAG plus Vector DB\n",
    "\n",
    "Only once data is fixed can you build a stable RAG back‑end.\n",
    "\n",
    "• Decide on chunk size & overlap; embed with an off‑the‑shelf model (e.g., Instructor or OpenAI Ada‑002). <br>\n",
    "• Build a vector index (FAISS/Qdrant) and a lightweight search API. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a5525",
   "metadata": {},
   "source": [
    "## Retrival powered generation\n",
    "\n",
    "Now you can focus on retrieval + fusion strategies and immediately compare against the baseline using the harness you already trust.\n",
    "\n",
    "• Retrieve top‑k chunks, concatenate with the prompt, generate review. <br>\n",
    "• Experiment with reranking or citation‑insertion heuristics. <br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
